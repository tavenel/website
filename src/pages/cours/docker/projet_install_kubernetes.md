---
title: üìå Projet Installation d'un Cluster Kubernetes et d√©ploiement d'une application
date: 2024 / 2025
---

## üéØ Objectifs

- Comprendre les concepts fondamentaux de Kubernetes  
- Installer et configurer un cluster Kubernetes  
- Administrer un cluster et g√©rer son cycle de vie  
- D√©ployer et superviser une application dans Kubernetes  

## üõ†Ô∏è √âtapes du Projet

### Phase 1 : Installation du Cluster Kubernetes

Le but de cette partie est d'installer un cluster Kubernetes. Le cluster devra √™tre compos√© d'au moins 2 noeuds : 2 worker, le `ControlPlane` peut √©ventuellement √™tre install√© sur le m√™me noeud qu'un `Worker` pour limiter les ressources n√©cessaires.

Note 1 : il n'est pas demand√© de g√©rer pr√©cis√©ment la s√©curit√© du cluster, ni la supervision et le logging, ces parties seront abord√©es durant le second module Kubernetes.

Note 2 : On demande √† installer une "vraie" distribution Kubernetes pouvant √™tre d√©ploy√©e en production : `k8s` via `kubeadm`, `rke`, `k3s`, ‚Ä¶ On √©vitera donc les versions orient√©es d√©veloppeur √† d√©ployer uniquement sur sa machine personnelle : ~`Docker Desktop`, `Minikube`, ‚Ä¶~

#### Pr√©-requis

Sur chaque `Node` :

- D√©sactiver le swap : `swapoff -a` et `fstab`
- Synchroniser les horloges
- Charger les modules noyau (avec `modprobe`) suivants : `overlay` et `br_netfilter`
- `sysctl -w net.netfilter.nf_conntrack_max=1000000`
- `echo "net.netfilter.nf_conntrack_max=1000000" >> /etc/sysctl.conf`
- Un runtime de conteneurs : `containerd`, `CRI-O`, `Docker`, ‚Ä¶
- `Kubeadm`, `Kubelet` et `Kubectl`

Configuration r√©seau :

- Tous les _Node_ :
  - `kubelet` : 10250
  - Si utilisation de `BGP` : 179
- _Control Plane_ uniquement : 
  - `kube-apiserver` : 6443
	- `scheduler` : 10251
	- `controller-manager` : 10252
- Les _Node_ doivent pouvoir communiquer entre eux.

:::tip
On pourra tester la compatibilit√© d'un noeud du cluster avec le _Node Conformance Test_. Remplacer `$CONFIG_DIR` par le chemin du manifeste du kubelet :  `/etc/kubernetes/kubelet`,  `/etc/default/kubelet`,  `/etc/systemd/system/kubelet.service`, ‚Ä¶ `$LOG_DIR` est le chemin o√π stocker les r√©sultats du test.

Pour plus d'information : <https://kubernetes.io/docs/setup/best-practices/node-conformance/>

```sh
sudo docker run -it --rm \
  --privileged \
  --net=host \
  -v /:/rootfs \
  -v $CONFIG_DIR:$CONFIG_DIR \
  -v $LOG_DIR:/var/result \
  registry.k8s.io/node-test:0.2
```
:::

#### Proc√©dure

Les installations de clusters Kubernetes sont assez h√©t√©rog√®nes en fonction de l'environnement cible, du _CNI_ utilis√© et de la distribution choisie (sp√©cificit√©s `k3s`, ‚Ä¶). On retrouve cependant un sch√©ma assez standard :

- Les noeuds _worker_ (noeuds qui ex√©cutent les applications) n'ont besoin que d'un `kubelet` et d'un `kube-proxy` (le `kube-proxy` est optionnel dans de rares cas, par exemple si l'on utilise `Cilium` ou un `kube-router`). Les `kubelet` n'ont finalement besoin que d'un enregistrement aupr√®s de l'`API Server`. La configuration du `kube-proxy` est en g√©n√©ral assez simple car le _CNI_ s'occupe des couches basses (√† v√©rifier aupr√®s du _CNI_).
- Le(s) _control plane_ sont les noeuds d'administration. Ils servent donc √† d√©ployer un `API Server` qui permet de communiquer avec le client `kubectl` qui permet de g√©rer le cluster une fois d√©ploy√©. Celui-ci a besoin de stocker de l'√©tat : c'est le but du composant `etcd`. Il faut ensuite lui ajouter un `Control Manager` qui ex√©cute l'intelligence du cluster par un ensemble de _controllers_ et un `Scheduler` qui permet de choisir le _Node_ qui ex√©cutera un _Pod_. On ajoute normalement un `Kubelet` afin de g√©rer les _Pods_ du _control plane_ (il est possible de ne pas d√©ployer les services du _control plane_ dans des _Pods_ mais d'utiliser par exemple `systemd`. Les clusters g√©r√©s par des fournisseurs de Cloud utilisent en g√©n√©ral ce genre de d√©ploiement, qui est assez rare _on-premise_). Le `Control Manager`, le `Scheduler` et le `Kubelet` n'ont besoin que d'√™tre raccord√©s √† l'`API Server`.
- Seul l'`etcd` (ou √©ventuellement la BDD dans le cas de `k3s`) g√®re l'**√©tat** du cluster : c'est donc le composant critique, celui responsable majoritairement de la r√©silience du cluster : attention √† ses pr√©-requis !

On effectue donc en principe l'odre de d√©ploiement suivant :

- Cr√©ation d'un 1er noeud _control plane_ (aussi appel√© _master_) :
  - d√©ploiement `etcd` (ou BDD).
  - d√©ploiement `api-server` utilisant l'`etcd`.
	- cr√©ation de la configuration du `kubelet`, d√©ploiement et enregistrement aupr√®s de l'`api-server`.
	- d√©ploiement `control-manager` et `scheduler` et enregistrement aupr√®s de l'`api-server`
- Cr√©ation des autres _control plane_ :
  - d√©ploiement des composants (similaire au 1er noeud) mais enregistrement des composants HA aupr√®s du 1er _control plane_, notamment `api-server` et `etcd`.
  - choisir une solution de load balancing adapt√©e : `kube-vip`, `MetalLB`, ‚Ä¶
  - mettre en place une strat√©gie de sauvegarde r√©guli√®re de la base etcd et pr√©voir des m√©canismes de restauration en cas de d√©faillance.
- Cr√©ation des workers :
	- cr√©ation de la configuration du `kubelet`, d√©ploiement et enregistrement aupr√®s de l'`api-server`.

##### Exemple de proc√©dure d'installation sans haute disponibilit√©

```sh
# Init du control plane
kubeadm init --control-plane-endpoint "<IP_Bastion>:6443" --pod-network-cidr=192.168.0.0/16 --upload-certs --cri-socket unix:///run/containerd/containerd.sock
# Ajout des Node Worker en r√©cup√©rant la sortie de la commande pr√©c√©dente
kubeadm join <IP_bastion>:6443 --token <TOKEN> --discovery-token-ca-cert-hash sha256:<HASH>
# Puis retour au control plane

# Config kubectl
sudo cat /etc/kubernetes/admin.conf > ~/.kube/config

# Le Node doit √™tre NotReady
kubectl get nodes -A
NAME      STATUS     ROLES           AGE   VERSION
cplane1   NotReady   control-plane   51s   v1.31.6

# Installation du CNI (par exemple, Calico)
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# Passage du Node en Ready
kubectl get nodes -A
cplane1   Ready    control-plane   113s   v1.31.6
```

Ou par fichier de configuration :

```yaml
# kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta4
kubernetesVersion: v1.21.0
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
[‚Ä¶]
```

```sh
$ kubeadm init --config kubeadm-config.yaml
```


##### Avec H/A

```bash
# pr√©-requis : Bastion => Installation d'un HAProxy entre les OS des Control Plane

# Init du 1e master
kubeadm init --control-plane-endpoint "<IP_bastion>:6443" --pod-network-cidr=192.168.0.0/16 --upload-cert
# Join des autres control-plane
kubeadm join "<IP_bastion>:6443" --token "<TOKEN>" --discovery-token-ca-cert-hash "sha256:<HASH>" --control-plane --certificate-key "<HASH_CERT>"
# Join des workers
kubeadm join "<IP_bastion>:6443" --token "<TOKEN>" --discovery-token-ca-cert-hash "sha256:<HASH>"
# calico, ‚Ä¶
```

#### API Server H/A

Chaque _Node_ : `kubelet`, `kube-proxy`, `kube-router`, ‚Ä¶ doivent se lier √† l'`API Server` **dont l'acc√®s doit √™tre H/A**, avec plusieurs possibilit√©s :

1. **Load Balancer externe** devant les _API Server_
  - _Kubelet_ pointe sur ce _load balancer_
  - si infra Cloud, souvent g√©r√© par le Cloud Provider
2. **Load Balancer local** : `Nginx`, `HAProxy`, ‚Ä¶ sur chaque _Node_ pour atteindre les _API Server_
  - _Kubelet_ pointe sur `localhost`
3. **round-robin DNS** pour tous les _API Server_ (officiellement non support√©)
4. H/A API endpoint dans un cluster manag√©, virtual IP, tunnel _Node_ <-> _API Server_ (`k3s`), ‚Ä¶

#### Pods statiques

:::tip
D√©ployer le _Control Plane_ dans des _Pods_ permet de les g√©rer avec toute la puissance de Kubernetes‚Ä¶ mais recquiert un cluster op√©rationnel !
Pour contourner cette limite, Kubernetes permet de d√©ployer des _Pods_ directement dans le _Kubelet_ (sans passer par l'_API Server_) en utilisant des _manifest_. Le _Kubelet_ r√©concilie les _Pods_ en cas de changement(s) dans le _manifest_. Voir [ces slides](https://2021-05-enix.container.training/5.yml.html#227) pour plus d'information.
:::

#### Sizing

:::tip
Dimensionner un cluster Kubernetes est tr√®s compliqu√©. Il est possible de redimensionner dynamiquement un cluster pendant son cycle de vie : voir ces slides sur le [Sizing de Cluster et le ClusterAutoscaler de Nodes](https://2021-05-enix.container.training/4.yml.html#248) et [Scaling with custom metrics](https://2021-05-enix.container.training/4.yml.html#334).
:::

#### Supervision

Voir [ces slides](https://2021-05-enix.container.training/5.yml.html#217) pour plus d'information sur les APIs internes et de monitoring

On pourra notamment monitorer a minima ces endpoints (`HTTP/tcp`, non authentifi√©) :

- `etcd` :
  - 2381 `/health` et `/metrics`
- `kubelet` :
  - 10248 `/healthz` retourne "ok"
- `kube-proxy` :
  - 10249 `/healthz` retourne "ok", `/configz`, `/metrics`
  - 10256 `/healthz` avec timestamp
- `kube-controller` & `kube-scheduler` :
  - 10257 (`kube-controller`) et 10259 (`kube-scheduler`) : `/healthz` (et `/configz` & `/metrics` en `HTTPS` avec authentification)

#### Contraintes

:::warn
Attention, on demande bien d'installer un cluster **production-ready** ! Celui-ci devra donc √™tre en haute disponibilit√© (Load balancer devant l'API Server, ‚Ä¶) et on r√©fl√©chira aux proc√©dures d'administration, de sauvegarde, ‚Ä¶ On pourra cependant s'affranchir d'utiliser HTTPS, notamment pour la communication entre les diff√©rents composants (ce qui est bien s√ªr une obligation dans une "vraie" production).
:::

:::link
- Voir aussi : <https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/installation/>
- Documentation de r√©f√©rence : <https://kubernetes.io/docs/setup/production-environment/>
- Pour tester la s√©curit√© du cluster, on pourra utiliser <https://github.com/aquasecurity/kube-bench> pour passer le benchmark CIS. 
:::

#### Exercice

:::exo
R√©aliser l'installation du cluster Kubernetes en H/A :

1. **Choix de l‚Äôenvironnement**
   - D√©ploiement sur des machines virtuelles (VirtualBox, VMware).
   - D√©ploiement sur le cloud (GCP, AWS, Azure, ‚Ä¶). Oracle OCI propose une offre gratuite suffisante pour ce projet.
2. **Installation des pr√©requis**
   - Machine Linux
   - Container runtime (`Docker` ou `containerd` ou autre)
   - Outils Kubernetes : `kubeadm`, `kubelet`, `kubectl`, ‚Ä¶
3. **Mise en place du cluster**
   - Initialisation du cluster avec `kubeadm` ou autre
   - Ajout de noeuds workers
   - Configuration du r√©seau avec un CNI (`Flannel`, `Calico`, ‚Ä¶)
4. On **testera** la partie H/A du `control-plane` :
   - D√©connecter ou simuler la d√©faillance d'un `control-plane` pour observer la capacit√© du cluster √† basculer automatiquement.
   - Lancer un test de restauration de `etcd` √† partir d'une sauvegarde et v√©rifier la coh√©rence du cluster.
:::

#### Administration

:::warn
Administrer un cluster Kubernetes ne se limite pas √† son installation : il faut g√©rer les mises √† jour, les maintenances, la s√©curit√©, l'observabilit√© du cluster, ‚Ä¶ Voir aussi : <https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/administration/>
:::

##### Upgrade

```bash
apt-mark unhold kubeadm && apt-get update && apt-get install -y kubeadm='1.31.x-y.y' && apt-mark hold kubeadm
# Upgrade 1e control plane
kubeadm upgrade plan
kubeadm upgrade apply vX.Y.Z
systemctl restart kubelet
# Upgrade autres control plane
kubeadm upgrade apply
# Upgrade workers
kubectl drain "<node-name>" --ignore-daemonsets # retire tous les Pods
apt-get update && apt-get install -y kubelet kubectl && apt-mark hold kubelet kubectl
systemctl restart kubelet
kubectl uncordon "<node-name>" # fin du drainage
```

##### Renouvellement des certificats

```bash
kubectl get csr
kubeadm alpha certs renew all
```



### Phase 2 : D√©ploiement d‚Äôune Application

Le but de cette partie est de d√©ployer dans le cluster un projet personnel existant qui se compose de plusieurs composants (par exemple, une application web front-end, une API back-end, une base de donn√©es, etc.). On recommande l'utilisation de fichiers de manifeste `yml` pour cr√©er les ressources Kubernetes.

1. **Cr√©ation des manifestes Kubernetes**
   - D√©ploiement (`Deployment`)
   - Service (`Service`)
   - Bonus : Ingress Controller
2. **Gestion des configurations et secrets**
   - `ConfigMaps` et `Secrets`
3. **Scalabilit√© et tol√©rance aux pannes**
   - Autoscaling avec `Horizontal Pod Autoscaler`
   - Rolling updates et rollback

:::exo
On testera la partie H/A du d√©ploiement applicatif :

- Tester la suppression d'un conteneur / Pod
- Tester la d√©connexion d'un _worker node_
- V√©rifier la r√©conciliation des ressources
- V√©rifier le _scaling_ automatique en cas de pic de charge
:::

## üìú Livrables

- üìÇ **Rapport d√©taill√©** avec √©tapes et configurations
- üìú **Code source** des manifestes Kubernetes
- üé§ **Pr√©sentation** des r√©sultats et d√©monstration

## Legal

- ¬© 2025 Tom Avenel under CC  BY-SA 4.0
- Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries. Docker, Inc. and other parties may also have trademark rights in other terms used herein.
- K8s¬Æ is a registered trademark of The Linux Foundation in the United States and/or other countries.

