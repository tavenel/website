---
title: üìå Administration de cluster Kubernetes
date: 2024 / 2025
---

## Objectifs

Administrer un cluster Kubernetes ne se limite pas √† son installation : il faut g√©rer les mises √† jour, les maintenances, la s√©curit√©, l'observabilit√© du cluster, ‚Ä¶

:::link
Voir aussi :

- <https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/administration/>.
- <https://kubernetes.io/docs/tasks/debug/debug-cluster/> et les sous-sections
:::

## Backup de clusters

- Les sauvegardes peuvent avoir plusieurs objectifs¬†:
  - Reprise apr√®s sinistre (serveurs ou stockage d√©truits ou inaccessibles)
  - Reprise apr√®s incident (donn√©es alt√©r√©es ou corrompues par un humain ou un processus)
  - Clonage d'environnements (pour tests, validation‚Ä¶)
- Kubernetes :
  - facilite la reprise apr√®s sinistre (primitives de r√©plication)
  - facilite la r√©plication d'environnements (toutes les ressources peuvent √™tre d√©crites avec des manifests)
  - n'offre **aucune aide sur la reprise apr√®s incident** : continuez les sauvegardes de donn√©es (BDD, storage, disques), voir m√™me backup complet du _Control Plane_
- **Sauvegardez les informations TLS** (minimum : cl√© et certificat du CA + de l'APIServer)
  - si install√© par `kubeadm`, dans `/etc/kubernetes/pki`
- **Sauvegardez les `PersistentVolume`** :
  - par le syst√®me de stockage (SAN, Cloud, ‚Ä¶)
  - par des outils d√©di√©s : voir [outils Kubernetes](/tools#-kubernetes-specific)
  - par [l'API Kubernetes](https://kubernetes.io/docs/concepts/storage/volume-snapshots/)

:::tip
Automatiser le (re)d√©ploiement d'un cluster (GitOps) peut √™tre une proc√©dure de recovery efficace mais risqu√©e (ne rien oublier !) : continuez les backup d'`etcd`.
:::

### Backup etcd

- Contient tous les objets du cluster (metadata : taille faible)
- **ne sauvegarde pas** les PV ni les donn√©es stock√©es localement dans les _Node_

#### Proc√©dure pour clusters `kubeadm`

- Choisir un _control plane_
- Trouver l'image _etcd_
- `etcdctl snapshot` dans un _debug container_ car :
  - le syst√®me de fichiers h√¥te est mont√© dans `/host`,
  - le _Pod_ de debug utilise le r√©seau de l'h√¥te
- Transf√©rer le _snapshot_ gr√¢ce √† un autre debug container_ en _base64_ pour √©viter toute corruption.
- Source et cr√©dits : [Backup Cluster - J√©r√¥me Petazzoni](https://github.com/jpetazzo/container.training/blob/main/slides/k8s/cluster-backup.md)

```sh
# Obtenir le nom d'un _Node_ du _Controle Plane_ :
NODE=$(kubectl get nodes \
--selector=node-role.kubernetes.io/control-plane \
-o jsonpath={.items[0].metadata.name})

# Obtenir l'image etcd¬†:
IMAGE=$(kubectl get pods --namespace kube-system etcd-$NODE \
-o jsonpath={..containers[].image})

# Ex√©cuter `etcdctl` dans un _Pod_ de debug :
kubectl debug --attach --profile=general \
node/$NODE --image $IMAGE -- \
etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/host/etc/kubernetes/pki/etcd/ca.crt \
--cert=/host/etc/kubernetes/pki/etcd/healthcheck-client.crt \
--key=/host/etc/kubernetes/pki/etcd/healthcheck-client.key \
snapshot save /host/tmp/snapshot

# Transf√©rer le snapshot :
kubectl debug --attach --profile=general --quiet \
node/$NODE --image $IMAGE -- \
base64 /host/tmp/snapshot | base64 -d > snapshot
```

### Restauration etcd

:::warn
Pas de restauration d'etcd si des `APIServer` sont en cours d'ex√©cution !

1. Arr√™ter toutes les instances d'`APIServer`
2. Restaurer l'√©tat de toutes les instances `etcd`
3. Red√©marrer toutes les instances d'`APIServer`

Il est √©galement recommand√© de red√©marrer les composants Kubernetes (`kube-scheduler`, `kube-controller-manager`, `kubelet`) afin de s'assurer qu'ils ne reposent pas sur des donn√©es obsol√®tes (normalement automatique car la restauration prend du temps et les composants perdent leur lien vers l'etcd donc red√©marrent).
:::

:::warn
`etcdutl restore` cr√©e un nouveau r√©pertoire de donn√©es √† partir du snapshot mais ne met pas √† jour le serveur `etcd` en cours d'ex√©cution.
:::

:::link
- Voir aussi la [documentation etcd snapshot and restore](https://coreos.com/etcd/docs/latest/op-guide/recovery.html#snapshotting-the-keyspace)
- Un [environnement de test pour etcd](https://kodekloud.com/playgrounds/playground-ha-etcd-cluster)
- Source et cr√©dits : [Backup Cluster - J√©r√¥me Petazzoni](https://github.com/jpetazzo/container.training/blob/main/slides/k8s/cluster-backup.md)
:::

#### Proc√©dure kubeadm

```bash
# 1. Cr√©er un nouveau r√©pertoire de donn√©es √† partir du snapshot¬†:
sudo rm -rf /var/lib/etcd
docker run --rm -v /var/lib:/var/lib -v $PWD:/vol $IMAGE \
etcdutl snapshot restore /vol/snapshot --data-dir=/var/lib/etcd

# 2. Provisionner le _Control Plane_ √† l'aide de ce r√©pertoire :
sudo kubeadm init \
--ignore-preflight-errors=DirAvailable--var-lib-etcd

# 3. Rejoindre les autres n≈ìuds
```

### Exercice

:::exo
- Assurer la sauvegarde et la r√©silience : Mettre en place une strat√©gie de sauvegarde r√©guli√®re de la base etcd et pr√©voir des m√©canismes de restauration en cas de d√©faillance.
- Lancer un test de restauration de `etcd` √† partir d'une sauvegarde et v√©rifier la coh√©rence du cluster.
:::

## Upgrade

### Versions et upgrade

- Kubernetes suit un versionning _s√©mantique_ vMAJOR.MINOR.PATCH (ex 1.28.9).
- Il est _recommand√©_ (pas obligatoire) d'ex√©cuter des versions homog√®nes sur l'ensemble du cluster mais :
- Les `APIServer` (en H/A) peuvent avoir diff√©rentes versions
- Diff√©rents _Node_ peuvent ex√©cuter diff√©rentes versions de `Kubelet`
- Diff√©rents _Node_ peuvent ex√©cuter diff√©rentes versions du noyau
- Diff√©rents _Node_ peuvent ex√©cuter diff√©rents engines de conteneurs
- Les composants peuvent √™tre mis √† niveau un par un sans probl√®me.
- Il est toujours possible de combiner diff√©rentes versions de _PATCH_ (par exemple, 1.28.9 et 1.28.13 sont compatibles) mais il est recommand√© de toujours mettre √† jour vers la derni√®re version de _PATCH_
- **L'`APIServer` doit √™tre plus r√©cent** que ses clients (`Kubelet` et _Control Plane_), donc **√™tre mis √† jour en premier**
- Tous les composants supportent (au moins) une **diff√©rence d'une version _MINEURE_** => upgrade √† chaud possible
- Voir [la documentation sur les versions non homog√®nes](https://kubernetes.io/releases/version-skew-policy/)
- Mettre √† jour avec **le m√™me outil qui a servi √† l'installation du composant** : gestionnaire de package, `kubeadm`, Pod, conteneur, ‚Ä¶
- En moyenne, **une mise √† jour tous les 3 mois**

### Proc√©dure

:::link
L'upgrade d'un cluster suit [la proc√©dure officielle de la documentation](https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/)
:::

:::warn
- Lors de l'upgrade d'un `Kubelet`, il est recommand√© de le _boucler_ (_cordon_) par la commande `kubectl drain` pour retirer tous les Pods ex√©cut√©s sur celui-ci au pr√©alable.
- Le d√©placement de pods _stateful_ (BDD, ‚Ä¶) peut entra√Æner des interruptions de service ! Utiliser la r√©plication de BDD (switch de l'instance _primaire_ sur le _Node_ non impact√©) - certains `Operator` (ex [CNPG](https://cloudnative-pg.io/)) effectuent ce basculement automatiquement lorsqu'ils d√©tectent qu'un _Node_ devient _cordon_.
:::

Exemple simplifi√© de proc√©dure d'upgrade avec `kubeadm` (pour plus d'information, suivre [la documentation officielle](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)). Voir aussi [la formation de J√©r√¥me Petazzoni](https://github.com/jpetazzo/container.training/blob/main/slides/k8s/cluster-upgrade.md)

```bash
# Upgrade kubeadm
apt-mark unhold kubeadm && apt-get update && apt-get install -y kubeadm='1.31.x-y.y' && apt-mark hold kubeadm
# Upgrade 1e control plane
kubeadm upgrade plan
kubeadm upgrade apply vX.Y.Z
systemctl restart kubelet
# Upgrade autres control plane
kubeadm upgrade apply
# Upgrade workers
kubectl drain "<node-name>" --ignore-daemonsets # retire tous les Pods
apt-get update && apt-get install -y kubelet kubectl && apt-mark hold kubelet kubectl
systemctl restart kubelet
kubectl uncordon "<node-name>" # fin du drainage
```

### Exercice

:::exo
- D√©crire une proc√©dure de mise √† jour pour votre cluster.
- Tester la proc√©dure en mettant √† jour votre cluster.
:::

## Supervision

Voir [ces slides](https://2021-05-enix.container.training/5.yml.html#217) pour plus d'information sur les APIs internes et de monitoring

On pourra notamment monitorer a minima ces endpoints (`HTTP/tcp`, non authentifi√©) :

- `etcd` :
  - 2381 `/health` et `/metrics`
- `kubelet` :
  - 10248 `/healthz` retourne "ok"
- `kube-proxy` :
  - 10249 `/healthz` retourne "ok", `/configz`, `/metrics`
  - 10256 `/healthz` avec timestamp
- `kube-controller` & `kube-scheduler` :
  - 10257 (`kube-controller`) et 10259 (`kube-scheduler`) : `/healthz` (et `/configz` & `/metrics` en `HTTPS` avec authentification)

### Exercice

:::exo
- Mettre en place la supervision du cluster en installant une stack _Prometheus_ / _Grafana_ : voir [le TP pour l'installation de Prometheus & Grafana via Helm](/cours/docker/tp_prometheus_grafana_k8s).
:::

## S√©curit√©

### S√©curisation du cluster

De nombreux composants acceptent les connexions (et les requ√™tes) d'autres composants : `api-server`, `etcd`, `Kubelet`
Nous devons s√©curiser ces connexions pour refuser les requ√™tes non autoris√©es et pour emp√™cher l'interception de secrets, de _tokens_ et d'autres informations sensibles
S√©curiser les control-plane en mettant en place une communication s√©curis√©e entre composants (voir cours).

### Renouvellement des certificats

Exemple de proc√©dure pour renouveller les certificats du cluster en utilisant `kubeadm` :

```bash
kubectl get csr
kubeadm alpha certs renew all
```

### Role-Based Access Control (RBAC)

Mettre en place un syst√®me de role pour :

- S√©curiser votre application
- Cr√©er un compte de support ayant le droit d'afficher des informations sur le cluster mais pas de le modifier.

### NetworkPolicies

Par d√©faut, un Pod peut communiquer avec tout autre Pod/Service de tout Namespace, ce qui n'est pas id√©al ! Les `NetworkPolicies` permettent de segmenter le traffic r√©seau (voir cours).

:::tip
Certains CNI ne supportent pas (totalement) les `NetworkPolicies` : la ressource est appliqu√©e mais sans effet ! C'est le cas par exemple de _Flannel_ (m√™me s'il est possible de lui ajouter une partie de _Calico_ pour fixer ce probl√®me : _Canal_ ou d'utiliser [Kube Network Policies](https://github.com/flannel-io/flannel/blob/master/Documentation/netpol.md) ).

Dans notre cas, on ne demande pas de r√©installer un nouveau CNI mais de d√©finir des NetworkPolicies pour le projet (par exemple, dans le plan de changer un jour de CNI).
:::

:::warn
Attention √† ne pas bloquer les communications attendues par Kubernetes ! Par exemple, on ne demande pas de d√©finir de NetworkPolicy dans le Namespace: `kube-system`.
:::

### Audit

Une fois le cluster s√©curis√©, nous allons auditer les r√¥les disponibles sur celui-ci.

Cet audit peut se r√©aliser par des plugins `kubectl` (installables par `krew`) :

- `kubectl who-can` / [kubectl-who-can](https://github.com/aquasecurity/kubectl-who-can) by Aqua Security
- `kubectl access-matrix` / [Rakkess (Review Access)](https://github.com/corneliusweig/rakkess) by Cornelius Weig
- `kubectl rbac-lookup` / [RBAC Lookup](https://github.com/FairwindsOps/rbac-lookup) by FairwindsOps
- `kubectl rbac-tool` / [RBAC Tool](https://github.com/alcideio/rbac-tool) by insightCloudSec

Il est bien entendu √©galement n√©cessaire d'auditer les actions r√©ellement r√©alis√©es sur le cluster, √† int√©grer √† la supervision.

### Sealed Secret

:::warn
Par d√©faut, les secrets Kubernetes ne sont pas chiffr√©s mais seulement encod√©s en base 64 !
:::

Des outils externes permettent d'ajouter du chiffrement, on pourra par exemple utiliser _Kubeseal_ pour que seul le cluster soit capable de d√©chiffrer un secret.

### Ingress SSL

Pour pouvoir acc√©der √† l'application en HTTPS, on utilisera un Ingress SSL. Pour cela, il faut :

- Un contr√¥leur Ingress install√© (ex : `NGINX Ingress Controller`)
- Option 1 : g√©n√©rer un certificat manuellement => il faudra g√©rer manuellement son cycle de vie !
- Option 2 : utiliser `cert-manager` pour automatiser la g√©n√©ration du certificat :
  - par _Let's Encrypt_ si le nom de domaine pointe vers l'IP publique du contr√¥leur Ingress
  - en test, on pourra [simuler la g√©n√©ration de certificats HTTPS avec Pebble](https://blog.manabie.io/2021/11/simulate-https-certificates-acme-k8s/)

:::tip
Pour g√©n√©rer un certificat SSL manuellement :

1. G√©n√©rer un certificat SSL

```sh
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -out tls.crt -keyout tls.key \
  -subj "/CN=example.com/O=example"
```

2. Cr√©er un Secret Kubernetes TLS

```sh
kubectl create secret tls example-tls \
  --cert=tls.crt \
  --key=tls.key \
  -n default
```
:::

:::link
Voir aussi :

- La [cheatsheet Kubernetes](/cours/docker/kubernetes-cheatsheet/#tls--clusterissuer-lets-encrypt)
- [NGINX Ingress Controller](https://kubernetes.github.io/ingress-nginx/user-guide/tls/)
- [Documentation Cert-Manager](https://cert-manager.io/docs/)
:::

### Exercice

:::exo
- S√©curiser les control-plane en mettant en place une communication s√©curis√©e entre composants.
- Mettre en place et tester une proc√©dure de renouvellement des certificats.
- Mettre en place et tester des r√¥les de s√©curit√© (RBAC) pour :
  - s√©curiser votre application ;
	- cr√©er un compte de support pouvant lister les ressources principales du cluster sans pouvoir y apporter de modification.
- Mettre en place des NetworkPolicies pour votre application.
- Mettre en place un audit des droits sur le cluster.
- Chiffrer les secrets avec _Kubeseal_.
- (Bonus) Configurer un Ingress en HTTPS pour acc√©der √† votre application.
:::

