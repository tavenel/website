---
title: üìå Administration de cluster Kubernetes
date: 2024 / 2025
---

## Objectifs

Administrer un cluster Kubernetes ne se limite pas √† son installation : il faut g√©rer les mises √† jour, les maintenances, la s√©curit√©, l'observabilit√© du cluster, ‚Ä¶

Voir aussi : <https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/administration/>.

## Supervision

Voir [ces slides](https://2021-05-enix.container.training/5.yml.html#217) pour plus d'information sur les APIs internes et de monitoring

On pourra notamment monitorer a minima ces endpoints (`HTTP/tcp`, non authentifi√©) :

- `etcd` :
  - 2381 `/health` et `/metrics`
- `kubelet` :
  - 10248 `/healthz` retourne "ok"
- `kube-proxy` :
  - 10249 `/healthz` retourne "ok", `/configz`, `/metrics`
  - 10256 `/healthz` avec timestamp
- `kube-controller` & `kube-scheduler` :
  - 10257 (`kube-controller`) et 10259 (`kube-scheduler`) : `/healthz` (et `/configz` & `/metrics` en `HTTPS` avec authentification)

## Renouvellement des certificats

Exemple de proc√©dure pour renouveller les certificats du cluster en utilisant `kubeadm` :

```bash
kubectl get csr
kubeadm alpha certs renew all
```

## Upgrade

### Versions et upgrade

- Kubernetes suit un versionning _s√©mantique_ vMAJOR.MINOR.PATCH (ex 1.28.9).
- Il est _recommand√©_ (pas obligatoire) d'ex√©cuter des versions homog√®nes sur l'ensemble du cluster mais :
- Les `APIServer` (en H/A) peuvent avoir diff√©rentes versions
- Diff√©rents _Node_ peuvent ex√©cuter diff√©rentes versions de `Kubelet`
- Diff√©rents _Node_ peuvent ex√©cuter diff√©rentes versions du noyau
- Diff√©rents _Node_ peuvent ex√©cuter diff√©rents engines de conteneurs
- Les composants peuvent √™tre mis √† niveau un par un sans probl√®me.
- Il est toujours possible de combiner diff√©rentes versions de _PATCH_ (par exemple, 1.28.9 et 1.28.13 sont compatibles) mais il est recommand√© de toujours mettre √† jour vers la derni√®re version de _PATCH_
- **L'`APIServer` doit √™tre plus r√©cent** que ses clients (`Kubelet` et _Control Plane_), donc **√™tre mis √† jour en premier**
- Tous les composants supportent (au moins) une **diff√©rence d'une version _MINEURE_** => upgrade √† chaud possible
- Voir [la documentation sur les versions non homog√®nes](https://kubernetes.io/releases/version-skew-policy/)
- Mettre √† jour avec **le m√™me outil qui a servi √† l'installation du composant** : gestionnaire de package, `kubeadm`, Pod, conteneur, ‚Ä¶
- En moyenne, **une mise √† jour tous les 3 mois**

### Proc√©dure

:::link
L'upgrade d'un cluster suit [la proc√©dure officielle de la documentation](https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/)
:::

:::warn
- Lors de l'upgrade d'un `Kubelet`, il est recommand√© de le _boucler_ (_cordon_) par la commande `kubectl drain` pour retirer tous les Pods ex√©cut√©s sur celui-ci au pr√©alable.
- Le d√©placement de pods _stateful_ (BDD, ‚Ä¶) peut entra√Æner des interruptions de service ! Utiliser la r√©plication de BDD (switch de l'instance _primaire_ sur le _Node_ non impact√©) - certains `Operator` (ex [CNPG](https://cloudnative-pg.io/)) effectuent ce basculement automatiquement lorsqu'ils d√©tectent qu'un _Node_ devient _cordon_.
:::

Exemple simplifi√© de proc√©dure d'upgrade avec `kubeadm` (pour plus d'information, suivre [la documentation officielle](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)). Voir aussi [la formation de J√©r√¥me Petazzo](https://github.com/jpetazzo/container.training/blob/main/slides/k8s/cluster-upgrade.md)

```bash
# Upgrade kubeadm
apt-mark unhold kubeadm && apt-get update && apt-get install -y kubeadm='1.31.x-y.y' && apt-mark hold kubeadm
# Upgrade 1e control plane
kubeadm upgrade plan
kubeadm upgrade apply vX.Y.Z
systemctl restart kubelet
# Upgrade autres control plane
kubeadm upgrade apply
# Upgrade workers
kubectl drain "<node-name>" --ignore-daemonsets # retire tous les Pods
apt-get update && apt-get install -y kubelet kubectl && apt-mark hold kubelet kubectl
systemctl restart kubelet
kubectl uncordon "<node-name>" # fin du drainage
```

## Backup de clusters

- Les sauvegardes peuvent avoir plusieurs objectifs¬†:
  - Reprise apr√®s sinistre (serveurs ou stockage d√©truits ou inaccessibles)
  - Reprise apr√®s incident (donn√©es alt√©r√©es ou corrompues par un humain ou un processus)
  - Clonage d'environnements (pour tests, validation‚Ä¶)
- Kubernetes :
  - facilite la reprise apr√®s sinistre (primitives de r√©plication)
  - facilite la r√©plication d'environnements (toutes les ressources peuvent √™tre d√©crites avec des manifests)
  - n'offre **aucune aide sur la reprise apr√®s incident** : continuez les sauvegardes de donn√©es (BDD, storage, disques), voir m√™me backup complet du _Control Plane_
- **Sauvegardez les informations TLS** (minimum : cl√© et certificat du CA + de l'APIServer)
  - si install√© par `kubeadm`, dans `/etc/kubernetes/pki`
- **Sauvegardez les `PersistentVolume`** :
  - par le syst√®me de stockage (SAN, Cloud, ‚Ä¶)
  - par des outils d√©di√©s : voir [outils Kubernetes](/tools#-kubernetes-specific)
  - par [l'API Kubernetes](https://kubernetes.io/docs/concepts/storage/volume-snapshots/)

:::tip
Automatiser le (re)d√©ploiement d'un cluster (GitOps) peut √™tre une proc√©dure de recovery efficace mais risqu√©e (ne rien oublier !) : continuez les backup d'`etcd`.
:::

### Backup etcd

- Contient tous les objets du cluster (metadata : taille faible)
- **ne sauvegarde pas** les PV ni les donn√©es stock√©es localement dans les _Node_

#### Proc√©dure pour clusters `kubeadm`

- Choisir un _control plane_
- Trouver l'image _etcd_
- `etcdctl snapshot` dans un _debug container_ car :
  - le syst√®me de fichiers h√¥te est mont√© dans `/host`,
  - le _Pod_ de debug utilise le r√©seau de l'h√¥te
- Transf√©rer le _snapshot_ gr√¢ce √† un autre debug container_ en _base64_ pour √©viter toute corruption.
- Source et cr√©dits : [Backup Cluster - J√©r√¥me Petazzo](https://github.com/jpetazzo/container.training/blob/main/slides/k8s/cluster-backup.md)

```sh
# Obtenir le nom d'un _Node_ du _Controle Plane_ :
NODE=$(kubectl get nodes \
--selector=node-role.kubernetes.io/control-plane \
-o jsonpath={.items[0].metadata.name})

# Obtenir l'image etcd¬†:
IMAGE=$(kubectl get pods --namespace kube-system etcd-$NODE \
-o jsonpath={..containers[].image})

# Ex√©cuter `etcdctl` dans un _Pod_ de debug :
kubectl debug --attach --profile=general \
node/$NODE --image $IMAGE -- \
etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/host/etc/kubernetes/pki/etcd/ca.crt \
--cert=/host/etc/kubernetes/pki/etcd/healthcheck-client.crt \
--key=/host/etc/kubernetes/pki/etcd/healthcheck-client.key \
snapshot save /host/tmp/snapshot

# Transf√©rer le snapshot :
kubectl debug --attach --profile=general --quiet \
node/$NODE --image $IMAGE -- \
base64 /host/tmp/snapshot | base64 -d > snapshot
```

### Restauration etcd

:::warn
Pas de restauration d'etcd si des `APIServer` sont en cours d'ex√©cution !

1. Arr√™ter toutes les instances d'`APIServer`
2. Restaurer l'√©tat de toutes les instances `etcd`
3. Red√©marrer toutes les instances d'`APIServer`

Il est √©galement recommand√© de red√©marrer les composants Kubernetes (`kube-scheduler`, `kube-controller-manager`, `kubelet`) afin de s'assurer qu'ils ne reposent pas sur des donn√©es obsol√®tes (normalement automatique car la restauration prend du temps et les composants perdent leur lien vers l'etcd donc red√©marrent).
:::

:::warn
`etcdutl restore` cr√©e un nouveau r√©pertoire de donn√©es √† partir du snapshot mais ne met pas √† jour le serveur `etcd` en cours d'ex√©cution.
:::

- Voir aussi la [documentation etcd snapshot and restore](https://coreos.com/etcd/docs/latest/op-guide/recovery.html#snapshotting-the-keyspace)
- Source et cr√©dits : [Backup Cluster - J√©r√¥me Petazzo](https://github.com/jpetazzo/container.training/blob/main/slides/k8s/cluster-backup.md)

#### Proc√©dure kubeadm

```bash
# 1. Cr√©er un nouveau r√©pertoire de donn√©es √† partir du snapshot¬†:
sudo rm -rf /var/lib/etcd
docker run --rm -v /var/lib:/var/lib -v $PWD:/vol $IMAGE \
etcdutl snapshot restore /vol/snapshot --data-dir=/var/lib/etcd

# 2. Provisionner le _Control Plane_ √† l'aide de ce r√©pertoire :
sudo kubeadm init \
--ignore-preflight-errors=DirAvailable--var-lib-etcd

# 3. Rejoindre les autres n≈ìuds
```


