---
title: TP Clusters de Haute Disponibilit√© (HA) et Corosync
date: 2025 / 2026
---

**Niveau :** Avanc√© / Master Administration Syst√®me

## üéØ Objectif

Monter un cluster 3 n≈ìuds :

- Installer Corosync + Pacemaker sur 3 VMs.
- Configurer corosync.conf (multicast ou udpu selon l'environnement).
- Cr√©er une ressource VirtualIP + service nginx et tester le basculement en stoppant le service sur le n≈ìud actif.

## üéØ Objectifs p√©dagogiques

- Comprendre les concepts et les m√©canismes des clusters HA.
- Mettre en place un cluster haute disponibilit√© complet avec **Corosync + Pacemaker**.
- Exp√©rimenter le basculement (failover) de services entre deux n≈ìuds.
- Tester la r√©silience du cluster en cas de panne.

## Th√©orie (rappel synth√©tique)

*(R√©sum√© extrait du cours complet : voir version d√©taill√©e du module)*

### Qu'est-ce qu'un cluster de haute disponibilit√© ?

Un **cluster HA** vise √† assurer la **continuit√© de service** malgr√© des pannes mat√©rielles ou logicielles. Il s'appuie sur :

- **Corosync** : communication fiable entre les n≈ìuds, quorum, membership.
- **Pacemaker** : orchestration et gestion des ressources (services, IPs, volumes, etc.).
- **Fencing/STONITH** : m√©canisme d'exclusion d'un n≈ìud d√©faillant.

### Types d'architecture

- **Active/Passive** : un n≈ìud actif, l'autre en secours.
- **Active/Active** : services r√©partis entre plusieurs n≈ìuds (plus complexe).

### R√®gles de base

- Toujours avoir un **quorum majoritaire**.
- Toujours **activer STONITH**.
- **Tester les sc√©narios de panne** r√©guli√®rement.

## TP Pratique : Cluster HA avec Corosync + Pacemaker

:::tip
Les commandes exactes varient l√©g√®rement selon la distribution (Debian/Ubuntu vs RHEL/CentOS). Ci-dessous des exemples g√©n√©riques.
:::

### Installation

**Debian / Ubuntu**

```bash
sudo apt update
sudo apt install corosync pacemaker crmsh -y
```

**RHEL / CentOS / Alma / Rocky**

```bash
sudo dnf install corosync pacemaker pcs -y
```

### G√©n√©ration et distribution de la cl√© d'authentification

Sur `node1` :

```bash
sudo corosync-keygen
# copie du fichier /etc/corosync/authkey vers les autres n≈ìuds (scp) en veillant aux permissions (0400)
sudo scp /etc/corosync/authkey node2:/etc/corosync/
sudo chmod 400 /etc/corosync/authkey
```

### Configuration du cluster

Fichier `/etc/corosync/corosync.conf` identique sur les deux n≈ìuds :

```json
# /etc/corosync/corosync.conf
totem {
  version: 2
  secauth: on
  cluster_name: ha-cluster
  transport: udpu
  interface {
    ringnumber: 0
    bindnetaddr: 192.168.100.0
    mcastport: 5405
  }
}

nodelist {
  node {
    ring0_addr: 192.168.100.10
    nodeid: 1
  }
  node {
    ring0_addr: 192.168.100.20
    nodeid: 2
  }
}

quorum {
  provider: corosync_votequorum
  two_node: 1
}

logging {
  to_syslog: yes
}

service {
  name: pacemaker
}
```

Red√©marrez Corosync sur les deux n≈ìuds :

```bash
sudo systemctl restart corosync pacemaker
```

V√©rifiez :

```bash
# √©tat Corosync
sudo corosync-cfgtool -s

# √©tat quorum
sudo corosync-quorumtool -l

# logs
journalctl -u corosync -b --no-pager
```

### Configuration Pacemaker

V√©rifiez que les deux n≈ìuds sont visibles :

```bash
sudo crm_mon -1
# ou
pcs status --full
```

Les deux n≈ìuds doivent appara√Ætre en ligne (`Online: [ node1 node2 ]`).

### Cr√©ation d'une ressource IP virtuelle (VIP)

```bash
sudo pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.100.100 cidr_netmask=24 op monitor interval=30s
```

V√©rifiez :

```bash
sudo pcs status resources
```

Pinguez la VIP :

```bash
ping 192.168.100.100
```

### Ajout d'une ressource Apache (exemple)

Sur **chaque n≈ìud**, installez Apache :

```bash
sudo apt install apache2 -y
```

Cr√©ez la ressource :

```bash
sudo pcs resource create WebServer ocf:heartbeat:apache configfile=/etc/apache2/apache2.conf op monitor interval=30s
```

Ajoutez une contrainte :

```bash
sudo pcs constraint colocation add WebServer ClusterIP INFINITY
sudo pcs constraint order ClusterIP then WebServer
```

V√©rifiez :

```bash
sudo pcs status --full
```

### Test du basculement

1. Identifiez le n≈ìud actif :

   ```bash
   sudo crm_mon -1
   ```

2. Stoppez le service Pacemaker sur le n≈ìud actif :

   ```bash
   sudo systemctl stop pacemaker
   ```

3. Observez le basculement automatique sur le second n≈ìud.

4. Red√©marrez le service :

   ```bash
   sudo systemctl start pacemaker
   ```

### Test de panne r√©seau (simulation)

Sur un n≈ìud :

```bash
sudo ip link set eth1 down
```

Sur l'autre : observez la reprise du service.

Restaurez :

```bash
sudo ip link set eth1 up
```

### Nettoyage du cluster

```bash
sudo pcs cluster stop --all
sudo pcs cluster destroy --all
```

## Pour aller plus loin

- Ajouter un **n≈ìud t√©moin (qdevice)** pour quorum externe.
- Tester un **fencing virtuel** (ex. `fence_virt` sur VirtualBox).
- Utiliser un LUN iSCSI (ou DRBD) pour le stockage partag√© des ressources.
- Int√©grer une ressource **DRBD** pour la r√©plication de stockage.
- Observer le comportement via `crm_mon -Af` pour suivi en direct.
- Tester perte d'un anneau r√©seau (d√©connecter une interface).
- Tester ralentissement r√©seau (tc/netem) et observer les param√®tres totem.

## Annexes

### Cheatsheet commandes

- `corosync-cfgtool -s` : statut Corosync.
- `corosync-quorumtool -l` : statut quorum.
- `corosync-keygen` : g√©n√©rer authkey.
- `systemctl status corosync pacemaker` : √©tat des services.
- `crm_mon -1` / `pcs status --full` : √©tat des ressources.

### Commandes utiles de diagnostic

```bash
corosync-cfgtool -s         # affiche l'√©tat des anneaux / token
corosync-quorumtool -l      # √©tat du quorum et votes
journalctl -u corosync -b   # logs du service corosync
journalctl -u pacemaker -b  # logs du gestionnaire de ressources
crm_mon -1, crm_resource    # vue du cluster et ressources (Pacemaker)
pcs status --full           # √©quivalent pcs
corosync-cmapctl --help     # inspecter cmap (si disponible)
corosync-objctl             # op√©rations d'administration avanc√©e (selon versions).
```

### Fichier `corosync.conf` : structure et exemples

Le fichier principal de configuration est **/etc/corosync/corosync.conf**. Il contient :

- `totem { ... }` - param√®tres du protocole et interfaces r√©seau.
- `nodelist { node { ... } }` - adresse/identifiant des n≈ìuds.
- `quorum { ... }` - configuration du quorum.
- `logging { ... }` - options de journalisation.
- `service { name: pacemaker }` - d√©claration des services utilisateur.

#### Exemple 1 - cluster 3 n≈ìuds (multicast, configuration minimale)

```yaml
totem {
  version: 2
  secauth: on
  cluster_name: mycluster
  token: 3000
  token_retransmits_before_loss_const: 10
  join: 60
  consensus: 3600
  max_messages: 20
  interface {
    ringnumber: 0
    bindnetaddr: 192.168.100.0
    mcastaddr: 226.94.1.1
    mcastport: 5405
  }
}
nodelist {
  node {
    ring0_addr: node1.example.local
    nodeid: 1
  }
  node {
    ring0_addr: node2.example.local
    nodeid: 2
  }
  node {
    ring0_addr: node3.example.local
    nodeid: 3
  }
}
quorum {
  provider: corosync_votequorum
  two_node: 0
}
logging {
  to_syslog: yes
}
service {
  name: pacemaker
}
```

#### Exemple 2 - cluster 2 n≈ìuds (UDPU, binding explicite)

```yaml
totem {
  version: 2
  secauth: on
  cluster_name: mycluster-two-node
  interface {
    ringnumber: 0
    bindnetaddr: 192.168.50.0
    transport: udpu
  }
}
nodelist {
  node { ring0_addr: 192.168.50.10 nodeid: 1 }
  node { ring0_addr: 192.168.50.20 nodeid: 2 }
}
quorum {
  provider: corosync_votequorum
  two_node: 1
}
logging { to_syslog: yes }
service { name: pacemaker }
```

#### Exemple 3 - configuration multi-ring (redondance r√©seau)

Ajouter plusieurs blocs `interface { ringnumber: 0 }` et `interface { ringnumber: 1 }` pointant vers deux r√©seaux physiques distincts (ex. `192.168.100.0` et `192.168.101.0`) : ainsi si une interface tombe, l'autre maintient la communication.

