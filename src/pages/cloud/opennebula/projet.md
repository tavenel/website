---
title: Projet Montage d'un Virtual Data Center
date: 2025 / 2026
---

## Pr√©sentation du projet

### R√©sum√©

Ce projet vise √† faire concevoir, d√©ployer et exploiter un **Virtual Data Center** complet sur OpenNebula, automatis√© et observable. Les √©tudiants fourniront une infrastructure documentaire, du code Infrastructure-as-Code (Terraform), des playbooks Ansible, une solution de supervision (Prometheus/Grafana) et de centralisation des logs (Loki) ainse qu'un petit cluster Kubernetes dans OpenNebula et d√©montreront l'observabilit√© applicative.

**Technologies** : OpenNebula, Prometheus, Grafana, Loki, Terraform, Ansible, Kubernetes

### Objectifs p√©dagogiques

√Ä l'issue du projet, les √©tudiants devront √™tre capables de :

1. Concevoir l'architecture d'un cloud priv√© simple bas√© sur OpenNebula.
2. D√©ployer et configurer OpenNebula dans un environnement virtualis√©.
3. √âcrire des modules Terraform pour provisionner des VMs dans OpenNebula.
4. Automatiser la configuration et le d√©ploiement des services avec Ansible.
5. Mettre en place une cha√Æne d'observabilit√© : Prometheus (m√©triques), Grafana (dashboards), Loki (logs) et Alertmanager.
6. Documenter l'infrastructure et r√©diger un rapport technique professionnel.
7. D√©ployer et monitorer un cluster Kubernetes provisionn√© sur OpenNebula.

### Pr√©requis

- Connaissances de base Linux (CLI, r√©seau, services)
- Virtualisation (KVM/QEMU) et notions de stockage r√©seau
- Notions de r√©seau (VLAN, NAT, routage de base)
- Acc√®s √† un hyperviseur (ou VMs) pour le lab : au moins 6 VMs, ou acc√®s √† un environnement cloud pour provisionner des VMs

### Topologie recommand√©e (lab)

- **ordinateur personnel ou bastion** : 2 vCPU, 4GB RAM. Outils : git, terraform, ansible
- **opennebula-master** : 4 vCPU, 8GB RAM, 60 Go disque. Frontend OpenNebula, database, sunstone, stockage local. Ubuntu ou AlmaLinux.
- **opennebula-compute-1** : 4 vCPU, 8GB RAM, 80 Go disque, virtualisation activ√©e. Ubuntu ou AlmaLinux.
- **monitoring** : 2 vCPU, 4‚Äì8GB RAM. Prometheus + Loki + Grafana (peut √™tre s√©par√© en 2 VMs si besoin)
- **k8s-node(s)** : 1‚Äì3 VMs pour k8s (taille selon tests)

> Remarque : pour des TP, on peut r√©duire la m√©moire et le CPU si l'infra h√¥te est limit√©e.

### Livrables finaux

1. **D√©p√¥t Git** (structure propre) contenant :

   - `terraform/` : modules + exemples d'usage
   - `ansible/` : roles, playbooks, inventories
   - `grafana/` : dashboards JSON
   - `prometheus/` : config, rules
   - `loki/` : pipeline config
   - `docs/` : architecture, guide d'installation, proc√©dure de reprise, rapport final
   - `k8s/` : ressources Kubernetes
2. **Rapport technique** : architecture, choix techniques, difficult√© rencontr√©es, r√©sultats
3. **Screencast / D√©mo / Slides** pour la soutenance : 20 minutes montrant provisioning, monitoring et logs

### Conseils & pi√®ges courants

- **It√©rer rapidement** : encouragez de petites boucles (terraform apply + ansible) plut√¥t que des changements massifs.
- **Gestion des secrets** : ne stocker aucun secret en clair dans le repo (utiliser Ansible Vault / HashiCorp Vault / variables CI chiffr√©es).
- **State Terraform** : pour un TP, un backend local est acceptable ; pour les groupes avanc√©s proposer un "remote state" (consul/s3).
- **Timeboxing** : respecter les jalons ; fournir des rendus interm√©diaires.

### √âtapes optionnelles pour aller plus loin (pour groupes avanc√©s)

- Mettre en place un catalogue d'images automatis√© (Packer)
- D√©ployer Grafana Alertmanager avec notification (email / slack)

## Conception & pr√©paration

- Formation des √©quipes, r√©partition des r√¥les
- Atelier conception : diagramme d'architecture du Virtual DC
- Pr√©paration des machines et images (cr√©ation d'une image Linux de r√©f√©rence, cloud-init/ssh)
- Mise en place du d√©p√¥t Git (structure initiale)

**Livrable** : diagramme d'archi + repo git initial (README)

## Installation d'OpenNebula

### Objectifs

- Comprendre les composants d'OpenNebula
- Installer et configurer une plateforme OpenNebula minimaliste (Front-end Sunstone + KVM Node)
- V√©rifier la communication et configurer les r√©seaux de base
- Configuration base de donn√©es, utilisateurs, authentification (ssh, bonus LDAP)

### √âtapes

#### Installation du Front-End OpenNebula

```bash
sudo apt update && sudo apt -y install gnupg
wget -q -O- https://downloads.opennebula.io/repo/repo.key | sudo apt-key add -
echo "deb https://downloads.opennebula.io/repo/6.8/Ubuntu/22.04 stable opennebula" | sudo tee /etc/apt/sources.list.d/opennebula.list
sudo apt update
sudo apt install opennebula opennebula-sunstone opennebula-gate opennebula-flow -y
```

Activer et d√©marrer les services :

```bash
sudo systemctl enable --now opennebula opennebula-sunstone
```

#### Configuration du n≈ìud KVM

```bash
sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils -y
sudo systemctl enable --now libvirtd
```

Ajouter l'utilisateur `oneadmin` sur le n≈ìud :

```bash
sudo useradd -m oneadmin
sudo passwd oneadmin
sudo usermod -aG libvirt oneadmin
```

Copier la cl√© SSH du front-end vers le n≈ìud :

```bash
sudo -u oneadmin ssh-copy-id oneadmin@opennebula-compute-1
```

#### Ajouter le n≈ìud √† OpenNebula

Depuis le front-end :

```bash
sudo -u oneadmin onehost create kvm-opennebula-compute-1 --im kvm --vm kvm
```

V√©rifier :

```bash
sudo -u oneadmin onehost list
```

#### Configuration r√©seau de base

Cr√©er un r√©seau virtuel :

```bash
cat <<EOF > network.yml
name: public-net
bridge: br0
vn_mad: dummy
phydev: br0
ip_ranges:
  - start: 192.168.100.100
    end: 192.168.100.200
EOF
onevnet create network.yml
```

### Livrables

- Capture d'√©cran de Sunstone op√©rationnel
- `onehost list`
- `onevnet list`

## Cr√©ation d'images & VM Templates

### Objectifs

- Cr√©er une image _cloud-init_
- Importer dans l'_Image Datastore_
- Construire un template complet pr√™t au d√©ploiement automatis√©
- Ajout du datastore/images et tests de cr√©ation VM manuelle via Sunstone

### √âtapes

#### Importer une image cloud officielle

```bash
sudo -u oneadmin oneimage create \
--name "ubuntu2204-cloud" \
--path https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \
--datastore default
```

#### Pr√©paration du cloud-init context

Cr√©er un fichier `cloud-init.tpl` :

```yaml
#cloud-config
hostname: vm-$(NAME)
manage_etc_hosts: true
users:
  - name: ubuntu
    groups: sudo
    shell: /bin/bash
    ssh_authorized_keys:
      - $(SSH_KEY)
```

#### Cr√©ation du template

```bash
cat <<EOF > template.yml
NAME="ubuntu22-base"
CPU="1"
MEMORY="2048"
DISK=[ IMAGE="ubuntu2204-cloud" ]
NIC=[ NETWORK="public-net" ]
CONTEXT=[
  NETWORK="YES",
  SSH_PUBLIC_KEY="\$USER[SSH_PUBLIC_KEY]",
]
EOF
one template create template.yml
```

#### D√©ploiement d'une VM

```bash
onevm create template.yml
onevm list
```

### Livrables

- Template final (.yml)
- Capture √©cran de la VM active

## Construction d'images Cloud avec HashiCorp Packer

### Objectifs

- Comprendre l'utilit√© du _baking d'images_ dans un Virtual Data Center.
- Concevoir une image personnalis√©e pour OpenNebula via Packer.
- Automatiser la g√©n√©ration d'images compatibles KVM/Firecracker.
- Int√©grer Packer dans une pipeline IaC (Terraform + OpenNebula).

### Installation de Packer

- Installation sur le bastion :

  ```bash
  curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
  sudo apt-add-repository "deb https://apt.releases.hashicorp.com $(lsb_release -cs) main"
  sudo apt-get update && sudo apt-get install packer
  ```

### Cr√©ation d'un template Packer OpenNebula

```
packer/
 ‚îú‚îÄ‚îÄ http/
 ‚îÇ    ‚îî‚îÄ‚îÄ preseed.cfg
 ‚îú‚îÄ‚îÄ scripts/
 ‚îÇ    ‚îú‚îÄ‚îÄ harden.sh
 ‚îÇ    ‚îî‚îÄ‚îÄ install-tools.sh
 ‚îî‚îÄ‚îÄ template.json
```

**Builder OpenNebula KVM**

Fichier : `template.json`

```json
{
  "builders": [
    {
      "type": "qemu",
      "iso_url": "https://releases.ubuntu.com/22.04/ubuntu-22.04-live-server-amd64.iso",
      "iso_checksum": "sha256:xxxxxxxx",
      "output_directory": "output-ubuntu",
      "vm_name": "ubuntu-2204-base",
      "disk_size": 8192,
      "format": "qcow2",
      "headless": true,
      "ssh_username": "packer",
      "ssh_password": "packer",
      "ssh_timeout": "20m",
      "boot_command": [
        "<esc><wait>",
        "auto url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/preseed.cfg ",
        "<enter>"
      ],
      "http_directory": "http"
    }
  ],
  "provisioners": [
    {
      "type": "shell",
      "script": "scripts/install-tools.sh"
    },
    {
      "type": "shell",
      "script": "scripts/harden.sh"
    }
  ],
  "post-processors": [
    {
      "type": "shell-local",
      "inline": [
        "mv output-ubuntu/ubuntu-2204-base qcow2-image/ubuntu-2204.qcow2"
      ]
    }
  ]
}
```

### Provisioning de l'image

```bash
apt update
apt install -y qemu-guest-agent curl vim

# hardening
sed -i 's/^#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config
systemctl restart ssh
```

### Production de l'image

```bash
packer init .
packer validate template.json
packer build template.json
```

R√©sultat attendu :

- Une image QCOW2 optimis√©e pour OpenNebula
- Donn√©es de provisioning appliqu√©es
- SSH s√©curis√©
- Agent QEMU pr√©sent (affichage IP dans Sunstone)

### Import de l'image dans OpenNebula

Via Sunstone :
**Storage ‚Üí Images ‚Üí + Create ‚Üí OS ‚Üí QCOW2 ‚Üí Upload**

ou en CLI :

```bash
oneimage create --name ubuntu-2204-base \
  --path qcow2-image/ubuntu-2204.qcow2 \
  --datastore default \
  --type OS
```

### Int√©gration Packer + Terraform

- Utiliser Packer pour cr√©er une _source_image_id_ dans un module Terraform :

```hcl
resource "opennebula_image" "base" {
  name = "ubuntu-base"
  path = "qcow2-image/ubuntu-2204.qcow2"
  datastore_id = 1
}
```

### üí° Livrables attendus

- Projet Packer complet (template.json + scripts + preseed).
- Image QCOW2 fonctionnelle dans OpenNebula.

## Terraform pour OpenNebula

### Objectifs

- Installer Terraform
- Initialiser un provider OpenNebula
- D√©ployer une VM via Terraform

### √âtapes

#### Installer Terraform

```bash
sudo apt install unzip -y
wget https://releases.hashicorp.com/terraform/1.9.0/terraform_1.9.0_linux_amd64.zip
sudo unzip terraform_* -d /usr/local/bin/
```

#### Provider OpenNebula

Cr√©er `main.tf` :

```hcl
provider "opennebula" {
  endpoint = "http://opennebula-master:2633/RPC2"
  username = "oneadmin"
  password = "votre_password"
}

resource "opennebula_virtual_machine" "vm1" {
  name = "terraform-vm"
  template_id = 10
}
```

#### D√©ploiement

```bash
terraform init
terraform plan
terraform apply
```

### Livrables

- R√©pertoire Terraform
- `terraform plan` (capture)

## Automatisation Ansible pour OpenNebula

### Objectifs

- Installer Ansible
- Cr√©er un playbook d√©ployant un service sur une VM OpenNebula
- Int√©gration Terraform ‚Üí Ansible (provisionner avec Terraform, configurer avec Ansible via inventory dynamique)

### √âtapes

#### Inventaire dynamique bas√© sur Terraform Output

Ajouter dans `outputs.tf` :

```hcl
output "vm_ip" {
  value = opennebula_virtual_machine.vm1.ip
}
```

Export :

```bash
terraform output -json > inventory.json
```

#### Playbook d'installation NGINX

Cr√©er `site.yml` :

```yaml
---
- hosts: all
  become: yes
  tasks:
    - name: Install nginx
      apt:
        name: nginx
        state: present
    - name: Enable and start nginx
      service:
        name: nginx
        state: started
```

Lancer :

```bash
ansible-playbook -i inventory.json site.yml
```

### Livrables

- Playbook Ansible complet test√© sur VMs provisionn√©es

## Monitoring avec Prometheus

### Objectifs

- Installer Prometheus
- Collecter les m√©triques OpenNebula (exporters)

### √âtapes

#### Installer Prometheus

```bash
sudo useradd --system --no-create-home prometheus
sudo mkdir -p /etc/prometheus /var/lib/prometheus
sudo apt install prometheus -y
```

#### Ajouter node_exporter

```bash
sudo apt install prometheus-node-exporter -y
```

#### Config Prometheus

`/etc/prometheus/prometheus.yml` :

```yaml
scrape_configs:
  - job_name: node_exporter
    static_configs:
      - targets: ["opennebula-master:9100", "opennebula-compute-1:9100"]
```

Restart :

```bash
sudo systemctl restart prometheus
```

### Livrables

- Dashboard des m√©triques _node_exporter_

## Observabilit√© avec Grafana & Loki

### Objectifs

- Installation de Grafana
- Installation de Loki + Promtail
- Dashboard logs + m√©triques combin√©s

### √âtapes

#### Installation de Grafana

```bash
sudo apt install -y apt-transport-https
sudo apt install grafana -y
sudo systemctl enable --now grafana-server
```

#### Installer Loki

Cr√©er `/etc/loki/local-config.yaml` puis :

```bash
sudo systemctl enable --now loki
```

#### Installer Promtail

```bash
sudo systemctl enable --now promtail
```

#### Configurer Grafana

- Ajouter source Prometheus
- Ajouter source Loki
- Importer les dashboard `1860` et `15141`

### Livrables

- Screenshots du dashboard int√©grant logs et m√©triques

## Kubernetes dans OpenNebula

### Objectifs

- D√©ployer un petit cluster Kubernetes (1 master, 2 workers) provisionn√© dans OpenNebula via Terraform
- Automatiser l'installation et la configuration du cluster avec Ansible (`kubeadm`)
- Int√©grer la supervision Kubernetes : `kube-state-metrics`, `node-exporter`, `metrics-server`, `Prometheus/Grafana`
- D√©ployer une application de test (nginx) et d√©montrer HPA (Horizontal Pod Autoscaler)

### Provisionnement Terraform (exemple)

- **Fichier `terraform/k8s/main.tf` (extrait)**

```hcl
# terraform/k8s/main.tf

provider "opennebula" {
  endpoint = "http://opennebula-master:2633/RPC2"
  username = var.one_user
  password = var.one_pass
}

module "k8s_vms" {
  source = "../modules/vdc-vm"
  count = 3
  name  = "k8s-${count.index+1}"
  cpu   = 2
  memory = 4096
  image = "ubuntu2204-cloud"
  network = "public-net"
}

output "k8s_ips" {
  value = module.k8s_vms.*.ip
}
```

- R√©seau : `public-net` configur√© dans OpenNebula
- `terraform apply` doit produire 3 adresses IP (master + workers)
- Exporter les IPs :

```bash
terraform output -json k8s_ips > k8s_inventory.json
```

### Inventaire Ansible g√©n√©r√© depuis Terraform

Transformer `k8s_inventory.json` en inventory Ansible (script simple ou template). Exemple minimal `inventory.ini` :

```
[k8s-master]
<IP_MASTER>

[k8s-workers]
<IP_WORKER1>
<IP_WORKER2>
```

### Playbooks / r√¥les Ansible

Proposer 3 r√¥les : `k8s-prereqs`, `kubeadm-init` (master), `kubeadm-join` (workers).

**R√¥le `k8s-prereqs` (t√¢ches principales)**

- D√©sactiver swap
- Installer containerd
- Installer kubeadm, kubelet, kubectl
- Activer et d√©marrer services

Extrait `roles/k8s-prereqs/tasks/main.yml` :

```yaml
# roles/k8s-prereqs/tasks/main.yml

- name: Disable swap
  ansible.builtin.command: swapoff -a
  become: true

- name: Ensure containerd installed
  ansible.builtin.package:
    name: containerd
    state: present
  become: true

- name: Configure sysctl for Kubernetes
  ansible.builtin.sysctl:
    name: net.ipv4.ip_forward
    value: '1'
    state: present
    reload: yes
  become: true

- name: Add Kubernetes apt repo
  ansible.builtin.apt_key:
    url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
    state: present
  become: true

- name: Install kubeadm kubelet kubectl
  ansible.builtin.apt:
    name: ["kubelet","kubeadm","kubectl"]
    state: present
    update_cache: yes
  become: true
```

**R√¥le `kubeadm-init` (sur master)**

- Ex√©cuter `kubeadm init --pod-network-cidr=10.244.0.0/16 --upload-certs`
- Copier le `kubeconfig` vers `/home/ubuntu/.kube/config`
- G√©n√©rer token de `join` & certificat (via `kubeadm token create --print-join-command --ttl 24h` ou stash des outputs)

Extrait `roles/kubeadm-init/tasks/main.yml` :

```yaml
# roles/kubeadm-init/tasks/main.yml

- name: Initialize Kubernetes master
  become: true
  command: >-
    kubeadm init --pod-network-cidr=10.244.0.0/16 --upload-certs
  args:
    creates: /etc/kubernetes/admin.conf

- name: Save kubeconfig for ubuntu user
  become: true
  copy:
    src: /etc/kubernetes/admin.conf
    dest: /home/ubuntu/.kube/config
    remote_src: yes
    owner: ubuntu
    mode: '0644'
```

Apr√®s l'init, on r√©cup√®re la commande `kubeadm join` (avec token) pour l'appliquer sur les workers.

**R√¥le `kubeadm-join` (sur workers)**

- Ex√©cuter la commande `kubeadm join <MASTER:PORT> --token ... --discovery-token-ca-cert-hash ...`

Extrait `roles/kubeadm-join/tasks/main.yml` :

```yaml
# roles/kubeadm-join/tasks/main.yml

- name: Join worker to cluster
  become: true
  command: "{{ join_command }}"
  args:
    warn: false
```

> Remarque : La playbook contr√¥leur devra r√©cup√©rer la commande de `join` depuis le master (module `slurp` ou `shell` pour `kubeadm token create --print-join-command`).

### Installer un CNI : Calico

Sur le master (apr√®s `kubeadm init`) :

```bash
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

V√©rifier `kubectl get pods -n kube-system` jusqu'√† ce que Calico soit `Running`.

### Installer components de monitoring pour Kubernetes

**Metrics Server** (n√©cessaire pour HPA) :

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

**kube-state-metrics** :

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/kube-state-metrics/master/examples/standard/deployment.yaml
```

**node-exporter** :

- D√©ployer node-exporter en DaemonSet (exemple officiel Prometheus)

**Prometheus** :

- Option 1 : installer `kube-prometheus-stack` via Helm (si Helm dispo)
- Option 2 : d√©ployer un Prometheus Standalone et ajouter scrapes pour `kube-state-metrics`, `node-exporter`, et `kubelet` endpoints

**Exemple de job scrape (prometheus.yml)**

```yaml
- job_name: 'kubernetes'
  kubernetes_sd_configs:
    - role: pod
  relabel_configs:
    - source_labels: [__meta_kubernetes_pod_label_app]
      action: keep
      regex: node-exporter|kube-state-metrics
```

### D√©ployer une application de test + HPA

**D√©ployer un d√©ploiement nginx**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-sample
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-sample
  template:
    metadata:
      labels:
        app: nginx-sample
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
```

**Exposer le service**

```bash
kubectl expose deployment/nginx-sample --type=NodePort --port=80
```

**Activer HPA** (exemple bas√© sur CPU)

```bash
kubectl autoscale deployment nginx-sample --cpu-percent=50 --min=1 --max=5
```

Tester le HPA en g√©n√©rant une charge (par ex. `hey`, `ab`, ou un pod stress) et observer `kubectl get hpa`.

### Supervision & Dashboards

- Importer dashboards Grafana pour Kubernetes (node, kube-state-metrics, kubelet, HPA)
- Cr√©er un dashboard d'√©tat global (nodes, pods, cpu/memory, HPA)
- Configurer alertes (pod restarts hauts, node NotReady, HPA scaling failures)

### Livrables

- Playbooks Ansible et roles (pr√©requis, init, join)
- Modules Terraform et inventory g√©n√©r√©
- D√©mo : `kubectl get nodes`, `kubectl get pods -A`, dashboard Grafana montrant HPA
