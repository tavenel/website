---
title: Introduction au Data Mining
date: 2023 / 2024
extra:
- math
---

# Introduction

## Qu'est-ce que le data mining ?

Une science √† la fronti√®re des math√©matiques et de l'informatique (et intelligence artificielle).

> _L'analyse des donn√©es est un outil pour d√©gager de la gangue des donn√©es le pur diamant de la v√©ridique nature. (J.P.Benz√©cri 1973)_

Exemple : extraire de millions de s√©quen√ßages g√©n√©tiques des patterns permettant de pr√©dire une maladie.

Un nouveau champ situ√© au croisement de la statistique et des technologies de l'information (bases de donn√©es, intelligence artificielle, apprentissage etc.) dont le but est de d√©couvrir des structures dans de vastes ensembles de donn√©es.

> _Data Mining is the nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data. (U.M.Fayyad, G.Piatetski-Shapiro)_

> _I shall define Data Mining as the discovery of interesting, unexpected, or valuable structures in large data sets. (D.J.Hand)_

- Le Data Mining analyse des donn√©es recueillies √† d'autres fins: c'est une analyse secondaire de bases de donn√©es, souvent con√ßues pour la gestion de donn√©es individuelles (Kardaun, T.Alanko,1998).
- Le Data Mining ne se pr√©occupe donc pas de collecter des donn√©es de mani√®re efficace (sondages, plans d'exp√©riences) (Hand, 2000).

Le Data Mining est n√© de :

- L'√©volution des SGBD vers l'informatique d√©cisionnelle avec les entrep√¥ts de donn√©es (Data Warehouse).
- La constitution d'√©normes bases de donn√©es : transactions de cartes de cr√©dit, appels t√©l√©phoniques, factures de supermarch√©s: terabytes de donn√©es recueillies automatiquement.
- D√©veloppement de la Gestion de la Relation Client (CRM)
  + Marketing client au lieu de marketing produit
  + Attrition, satisfaction, etc.
- Recherches en Intelligence artificielle, apprentissage, extraction de connaissances

:::tip
Qu'est-ce qu'un bon algorithme d'apprentissage ?

- Interpr√©table : la r√®gle de classification est "compr√©hensible" ;
- Critique : fournit un score en classification, un intervalle en r√©gression ;
- Consistance : convergence vers l'erreur bay√©sienne : quand $n$ tend vers l'infini, $f_n$ tend vers la [r√®gle de Bayes][wiki-bayes] ;
- Minimax : cette convergence est la plus rapide possible ;
- Non asymptotique : garanties de performance pour $n$ donn√© ;
- Sans param√®tre : Param√©trage automatique ;
- Vitesse : complexit√© lin√©aire, possibilit√© de parall√©liser ;
- En ligne : mise √† jour s√©quentielle. 
:::

Il n'y a pas de meilleure m√©thode !

- Chacune est plus ou moins adapt√©e au probl√®me consid√©r√©, √† la nature des donn√©es, aux propri√©t√©s de la relation entre descripteurs et variable expliqu√©e...
- Il faut apprendre les qualit√©s et les d√©fauts de chaque m√©thode
- Il faut apprendre √† exp√©rimenter pour trouver la plus pertinentes
- L'estimation de la qualit√© des m√©thodes est donc centrale (mais pas toujours √©vidente).

> _Tous les mod√®les sont faux, certains sont utiles (Box, G.E.P. and Draper, N.R.: Empirical Model-Building and Response Surfaces, p. 424, Wiley, 1987)_

Quelques paradoxes :

- Un bon mod√®le statistique ne donne pas n√©cessairement des pr√©dictions pr√©cises au niveau individuel.
  + facteurs de risque en √©pid√©miologie
- On peut pr√©voir sans comprendre:
  + pas besoin d'une th√©orie du consommateur pour faire du ciblage
  + un mod√®le n'est qu'un algorithme
- Questions d'√©thique et traitements de donn√©es personnelles


# Introduction √† la statistique

## Vocabulaire

:::strong
- _Donn√©e statistique_ : observation ou mesure qui sert de base √† un raisonnement. C'est le point de d√©part pour une recherche.
- _Individu_ ou _unit√© statistique_ : √âl√©ment observ√© ou mesur√© : par exemple une personne dans un sondage, un objet dans un comparatif.
- _Population_ : Ensemble des individus observ√©s ou mesur√©s dans le cadre d'une √©tude. Peut √™tre infini. Par exemple : l'ensemble des entreprises fran√ßaises.
- _√âchantillon_ : partie de la population que l'on √©tudie. L'ensemble de $1000$ entreprises fran√ßaises tir√©es au hasard constitue un √©chantillon de la population pr√©c√©dente.
- _Caract√®re_ (ou _variable_) : ce qui est observ√© ou mesur√© sur un individu.
- _S√©rie statistique_ ou _distribution_ : liste de valeurs recueillies sur un caract√®re observ√© ou mesur√© sur un ensemble d'individus.
- _Variable observ√©e_ : variable concr√®tement observable chez un individu (ex: salaire).
- _Variable latente_ : variable non observ√©e chez un individu (ex: √¢ge + salaire).
:::

### √âchantillon vs population

:::warn
Attention √† ne pas confondre √©chantillon et population !

- L'√©tude d'un √©chantillon est une science : la statistique descriptive.
- L'estimation des caract√©ristiques d'une population depuis un √©chantillon repr√©sentatif est une autre science : la statistique inf√©rentielle.
- Pour faire des statistiques, il faut donc avoir des **connaissances dans le domaine** de l'√©tude.
:::

### Exercice

:::exo
On souhaite r√©aliser un sondage √† Paris pour conna√Ætre le temps de trajet moyen des utilisateurs des transports en commun pour aller travailler.

Pour cela, pour chaque personne sond√©e, on recueille‚ÄØ:

- Son √¢ge et sa cat√©gorie socio-professionnelle (CSP)
- Son temps moyen pass√© dans les transports en commun

Quels sont :

- Les individus de l'√©tude ?
- La population de l'√©tude ?
- L'√©chantillon de l'√©tude ?
- Les variables √©tudi√©es ?
:::

:::correction

- Les individus sont les personnes sond√©es.
- La population est l'ensemble des personnes utilisant les transports en commun.
- L'√©chantillon est l'ensemble des personnes ayant r√©pondu favorablement au sondage.
- On a trois variables : l'√¢ge, la CSP et le temps moyen.

:::

## Types de variables

:::strong
2 types de variables :

- Les variables _quantitatives_ : variables √† valeurs num√©riques, pour lesquelles les op√©rations arithm√©tiques ont un sens. Par exemple, un √¢ge, une distance, un volume, etc :
  + _discr√®tes_ : dans un ensemble d√©nombrable (on peut compter toutes les valeurs possibles) ;
  + _continues_ : valeurs possibles non connues √† l'avance et travail dans les r√©els ;
- Les variables _qualitatives_ : les valeurs possibles sont cod√©es par des _modalit√©s_ (ou _cat√©gories_). Par exemple, la couleur des yeux, le d√©partement, ou tout autre codage o√π les op√©rations arithm√©tiques ne sont pas correctement d√©finies :
  + _ordinale_ : pouvant √™tre ordonn√© (niveau d'appr√©ciation d'un produit, mention dans un dipl√¥me) ;
  + _nominale_ : aucun ordre (couleur des yeux, ville de naissance).
:::

:::exo
Quel est le type des variables pr√©c√©dentes ?
:::

:::correction

- √¢ge : variable quantitative discr√®te
- CSP : variable qualitative ordinale
- temps moyen: variable quantitative continue
:::

# Statistique descriptive √† une dimension

:::link
Pour plus d'information, voir le [cours zeste de savoir de statistique descriptive √† une dimension][zds-stat-desc-1d].
:::

## Moyenne (_mean_)

- Famille d'indicateurs statistiques les plus courants car simples.
- R√©sume une liste de valeurs num√©riques en un seul nombre r√©el.
- Donne une information sur la distribution, aucune sur les individus (r√©partition, ...)
  + Peu robuste aux valeurs extr√™mes.
  + Exemple d'un unique salaire √©lev√©.

:::link
Voir [l'article Wikipedia sur la moyenne][wiki-moyenne].
:::

### Moyenne arithm√©tique

C'est de tr√®s loin la moyenne la plus utilis√©e : somme des valeurs divis√© par nombre de valeurs.

:::strong
$$\mu = \frac{\sum_{i=1}^n{m_i \cdot x_i}}{\sum_{i=1}^n {m_i}}$$
:::

### Autres moyennes

- Il existe d'autres moyennes, par exemple :
  + [Moyenne de H√∂lder][wiki-moyenne-holder] (ou d'ordre p) : $$\left( \frac{1}{n} \sum_{i=1}^n x_i^p \right)^{\frac{1}{p}}$$
  + Permet de donner plus d'importance aux petites valeurs (p petit) ou aux grandes valeurs (p grand).
- La moyenne num√©rique est g√©n√©ralisable : barycentre en g√©om√©trie, esp√©rance en probabilit√©s, ...
- En absence de pr√©cision, le terme seul de _moyenne_ d√©crit la _moyenne arithm√©tique_.

:::link
Lien vers [la Moyenne de H√∂lder][wiki-moyenne-holder].
:::

### Exercice

:::exo
Soit une promotion de 20 √©tudiants :

- 19 d'entre-eux sont embauch√©s avec un salaire situ√© entre $20000$ et $30000$ euros par an
- Un √©tudiant a un salaire d'un million d'euros par an.

Quelle est la moyenne arithm√©tique ? Que remarquez-vous ?

Calculer la moyenne d'ordre 2 et la moyenne d'ordre 50. Que remarquez-vous ?
:::

:::correction
**Moyenne arithm√©tique**

$$m=\frac{19 \times 25 000 + 1 \times 1 000 000}{20}=73750$$

La moyenne arithm√©tique est tr√®s √©lev√©e √† cause d'une valeur extr√™me : la moyenne ne repr√©sente pas bien l'√©chantillon.

**Moyenne de H√∂lder**

Calculons la moyenne de H√∂lder d'ordre 2 et d'ordre 50, par exemple en Python :
```python
m = ( (1/20)*(19*25000**p + 1000000**p) )**(1/p)
m_2 => 224930
m_50 => 941844
```

La moyenne de H√∂lder est toujours tr√®s influenc√©e par la valeur extr√™me. On remarque bien que pour $p=50 > 2$ on fait plus ressortir la plus grande valeur que pour $p=2$.

:::

## M√©diane (_median_)

La [m√©diane][wiki-mediane] indique le centre d'une s√©rie statistique : c'est la valeur qui partage la moiti√© inf√©rieure de la moiti√© sup√©rieure de cette s√©rie.

- Donne peu d'information sur la s√©rie elle-m√™me.
- Utile pour nuancer d'autres mesures de tendance, comme la moyenne :
  + Assez robuste aux valeurs extr√™mes.
  + Reprise de l'exemple pr√©c√©dent.
- Plus complexe que la moyenne car n√©cessite un tri : co√ªteux, g√©n√©ralement $\mathcal{O}(n\log{n})$.

### Formule de la m√©diane

La m√©diane se d√©finit formellement par :

:::strong
$$Med=\displaystyle \text{argmin}_{\alpha \in \mathbb{R}} \sum_{i=1}^n |\alpha-x_i|$$
:::

o√π $argmin$ d√©finit la variable pour laquelle une fonction atteint son minimum :

$$f(x^*)=\min_{x \in \mathbb{R}} f(x) \Leftrightarrow x^* = \text{argmin}_{x \in \mathbb{R}} f(x)$$

### Quartiles

Soit une serie statistique numerique de m√©diane $Med$.

- Premier quartile $Q_1$ : m√©diane de la s√©rie des observations strictement inf√©rieures √† $Med$.
- Deuxi√®me quartile $Q_2$ : m√©diane de la s√©rie compl√®te.
- Troisi√®me quartile $Q_3$ : m√©diane de la s√©rie des observations strictement sup√©rieures √† $Med$.
- L'√©cart inter quartile est d√©fini par : $I_q = Q_3 - Q_1$.

G√©n√©ralisation : d√©ciles, centiles, ...

### Calcul de la m√©diane

Contrairement √† la moyenne, la m√©diane ne se calcule pas alg√©briquement : il faut la trouver on ordonnant les valeurs de la s√©rie.

Pour une s√©rie de $n$ √©l√©ments :

1. Ordonner la s√©rie
- Si $n$ est impair : l'√©l√©ment √† la position $\frac{n+1}{2}$ est la m√©diane.
- Si $n$ est pair :
  + Toute valeur entre l'√©l√©ment √† la position $\frac{n}{2}$ et l'√©l√©ment √† la position $\frac{n}{2}+1$ est une m√©diane.
  + En pratique, on utilise la moyenne arithm√©tique de ces deux √©l√©ments.

:::link
Lien vers [la page Wikipedia sur la m√©diane][wiki-mediane].
:::

### Exercice

:::exo
Reprenons l'exercice sur la moyenne des salaires d'une promotion :

Soit une promotion de 20 √©tudiants :

- 19 d'entre-eux sont embauch√©s avec un salaire situ√© entre $20000$ et $30000$ euros par an
- Un √©tudiant a un salaire d'un million d'euros par an.

Quelle est le salaire m√©dian ? Que remarquez-vous ?
:::

:::correction

La m√©diane est entre les classes $20000$ euros et $30000$ euros donc $Med=25000$ euros.

Le salaire m√©dian est beaucoup plus repr√©sentatif de l'√©chantillon que le salaire moyen.

:::

## √âcart-type (_deviation_) et variance

L'[√©cart-type][wiki-deviation] $\sigma$ (et donc la variance $V=\sigma^2$) repr√©sente la dispersion des valeurs dans la distribution.

La variance se calcule comme la moyenne des carr√©s des √©carts √† la moyenne :

- L'√©cart-type est donc la moyenne des √©carts.
- On peut le voir comme la "distance" des valeurs √† la moyenne.
- Donne une indication sur l'homog√©n√©it√© de la distribution.

:::strong
$$\sigma=\sqrt{V}$$

$$V = \frac{1}{n}\sum_{i=1}^n \left(x_i - \overline{x}\right)^2$$
:::

![Exemples avec m√™me moyenne mais ecart-type diff√©rent](@assets/data/Comparison_standard_deviations.svg)

<div class="caption">Deux exemples de distribution ayant la m√™me moyenne mais un √©cart-type diff√©rent. Source : https://upload.wikimedia.org/wikipedia/commons/f/f9/Comparison_standard_deviations.svg (domaine public).</div>

### En programmation

:::warn
Attention : l'√©cart-type calcul√© directement avec la formule math√©matique peut √™tre [incorrect en programmation][deviation-computing-error] √† cause d'arrondis, notamment pour des valeurs faibles (entre 0 et 1). 

Il peut √™tre utile d'utiliser [d'autres algorithmes √©prouv√©s][deviation-computing-solution] pour ce besoin.
:::

### üåü Avantages et inconv√©nients

:::tip
+  Mesures particuli√®rement repr√©sentatives de la r√©alit√© lorsque la distribution est normale.
+  Tient compte de toutes les valeurs de la distribution donc repr√©sentent bien sa dispersion.
+  Utiles pour comparer la dispersion d'une variable d'une m√™me population √† des temps diff√©rents ou de populations semblables.
- Mesures affect√©es par les valeurs extr√™mes
- Difficult√©s d'interpr√©tation li√©es au fait que la valeur de l'√©cart-type varie selon les valeur de la variable.
  * Un grand √©cart-type n'est pas synonyme de grande dispersion (d√©pend des valeurs).
:::

:::link
Voir [la page Wikipedia sur l'√©cart-type][wiki-deviation].
:::

### Exercice

:::exo
On donne ci-dessous les notes de trois groupes d'√©tudiants‚ÄØ:

$$G_1=\{10,12,11,13,14,12,7,15\} \qquad G_2=\{12,17,4,8,19,11,12,9\} \qquad G_3=\{8,7,12,10,14,10,8,3,8,7\}$$

Quelle est le groupe le plus h√©t√©rog√®ne‚ÄØ? Que remarquez-vous ?
:::

:::correction

On commence par calculer les moyenne de chaque groupe, que l'on notera $\bar{G1}, \bar{G2}$ et $\bar{G3}$‚ÄØ:

$$\bar{G1}=\frac{10+12+11+13+14+12+7+15}{8}=11,75 \qquad \bar{G2}=11,5 \qquad \bar{G3}=8,7$$

Calculons la variance de $G1$ :

$$v_1=\frac{(10-11,75)^2+(12-11,75)^2+(11-11,75)^2+(13-11,75)^2+(14-11,75)^2+(12-11,75)^2+(7-11,75)^2+(15-11,75)^2}{8}$$

$$v_1=\frac{1,75^2+0,25^2+0.75^2+2,25^2+0,25^2+4,75^2+3,25^2}{8} \approx 6.214 \qquad v_2=23.143 \qquad v_3=9.122$$

On remarque que $v_2 > v_3 > v_1$, le groupe le plus h√©t√©rog√®ne est donc le groupe $G2$.

On remarque que les groupes $G1$ et $G2$ ont des moyennes assez proches, et pourtant, leur variance est compl√®tement diff√©rente. La moyenne n'est donc pas suffisante : on effectuera quasi syst√©matiquement un calcul de variance ou d'√©cart-type.

:::

## Statistiques √† deux dimensions

### Centre de gravit√©

D√©finition : Le centre de gravit√© du nuage repr√©sentant les deux s√©ries $x_i$ et $y_i$ de moyennes $\bar{x}$ et $\bar{y}$ respectivement est le point $G = (\bar{x}, \bar{y})$ dont les coordonn√©es sont les moyennes des deux s√©ries.

On peut aussi relier l'√©tendue du nuage aux deux √©cart-type $\sigma(x)$ et $\sigma(y)$ des s√©ries $x$ et $y$. Le premier est li√© √† l'√©tendue horizontale (dans le sens de l'axe des $x$), et le second √† l'√©tendue verticale (dans le sens de l'axe des $y$).

### Covariance de deux s√©ries

Si tous les points du nuage √©taient pratiquement situ√©s sur une m√™me droite (nuage de forme allong√©e), le nuage pourrait √™tre tr√®s √©tendu, √† la fois en $y$ et en $y$ sans pour autant √™tre √©tendu en surface dans le plan.

Pour mesurer _l'√©tendue en surface_ du nuage, on calcule la _covariance_ des deux s√©ries, c'est-√†-dire la moyenne des produits des √©carts √† la moyenne :

:::strong
$$Cov(x, y) = \frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})$$
:::

√Ä noter que si $y = x, Cov(x, x) = Var(x)$.

Le signe de la covariance renseigne sur le fait que les deux s√©ries varient dans le m√™me sens ou en sens oppos√© : une covariance positive indique que l'une augmente quand l'autre augmente ou diminue quand l'autre diminue.

Par contre la valeur absolue de la covariance (le fait qu'elle ait une valeur petite ou grande) fournit peu d'informations pertinentes car elle d√©pend des unit√©s dans lesquelles ont √©t√© exprim√©es les deux s√©ries $x$ et $y$.

### Coefficient de corr√©lation lin√©aire

Pour obtenir un √©quivalent de la covariance qui soit ind√©pendant des unit√©s dans lesquelles ont √©t√© exprim√© les deux s√©ries de mesures il convient de calculer le coefficient de corr√©lation lin√©aire :

:::strong
$$\rho(x,y)=\frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}$$
:::

On peut montrer que sa valeur est toujours comprise entre $-1$ et $+1$.

On dit que les deux s√©ries sont faiblement corr√©l√©es lorsque $\rho(x, y)$ est proche de 0, et dans ce cas le nuage a une forme tr√®s dispers√©e. Au contraire si $\rho(x, y)$ est proche de $-1$ ou $+1$, les deux s√©ries sont dites fortement corr√©l√©es (positivement si $\rho(x, y) > 0$ et n√©gativement si $\rho(x, y) < 0$) et dans ce cas le nuage est regroup√© le long d'une droite.

:::tip
On utilise souvent plut√¥t le carr√© de ce terme pour augmenter sa dispersion vers 0 ou 1.
:::

Ce coefficient sera tr√®s utile lors des r√©gressions lin√©aires.

:::exo
| Nombre de caisses ouvertes (x) :       | 3  | 4  | 5  | 6    | 8    | 10   | 12   |
|----------------------------------------| ---| ---| ---| -----| -----| -----| -----|
| Temps moyen d'attente en minutes (y) : | 16 | 15 | 16 | 14.9 | 16.6 | 14.7 | 14.8 |

Dans cet exemple, y a-t-il une relation directe entre le nombre de caisses ouvertes et le temps moyen d'attente ?
:::

:::correction
1. Commen√ßons par calculer le centre de gravit√© du nuage de points $\{x;y\}$ : $G=(6.8 ; 15.4)$.
2. Calculons les variances de $X$ et $Y$ :
  + $Var(X) = 9.27$
  + $Var(Y) = 0.49$
3. Calculons la covariance de $X$ et $Y$ : $Cov(X,Y)$ = -0.97$
4. On en d√©duit la coefficient de corr√©lation : $\rho(X,Y)=-0.45$.

Le coefficient de corr√©lation lin√©aire est assez √©loign√© de 1 : $R^2=-0.20$. Il n'y a pas de lien direct entre le nombre de caisses ouvertes et le temps moyen d'attente dans cet exemple.

Exemple de calcul en `Python` :

```python
import numpy
x=[3,4,5,6,8,10,12]
y=[16, 15, 16, 14.9, 16.6, 14.7, 14.8]
R2=( numpy.cov(x,y)[0][1] / numpy.sqrt(numpy.var(x) * numpy.var(y)) )**2

import matplotlib.pyplot as plt
#plt.plot(x,y) # ligne
plt.scatter(x,y) # points
plt.show()
```

![Nuage de points](@assets/data/caisses.png)

<div class="caption">Le nuage de points</div>

:::

## Lois de probabilit√©

### Rappels et notations

:::tip
- $P(Y=1)$ est la probabilit√© a priori pour que $Y=1$. Pour simplifier, on note parfois : $p(1)$.
- $p(X|1)$ est la distribution conditionnelle des $X$ sachant la valeur prise par $Y$.
- $\sum_{i=1}^n p_i = 1$
- La probabilit√© a posteriori d'obtenir la modalit√© $1$ de $Y$ sachant la valeur prise par $X$ est not√©e $p(1|X)$.
- Si $X$ est une variable al√©atoire **r√©elle**, la fonction de r√©partition de $X$ est la fonction $F_X(t) = \mathbb P(X\leq t)$.
- Si l'√©v√©nement $A$ se produit $N_A$ fois sur $N$ observations, la fr√©quence de l'√©v√©nement $A$ est : $\frac{N_A}{N}$.
:::

### Esp√©rance

L'esp√©rance d'une variable al√©atoire $X$ correspond √† la moyenne des valeurs possibles de $X$ pond√©r√©es par les probabilit√©s associ√©es √† ces valeurs et est d√©finie par :

:::strong
$$\bar{X} = \sum_i P_i x_i$$
:::

ou pour une variable continue √† densit√© :

$$\mathbb{E}[X] = \int_{-\infty}^\infty x f(x) \mathrm{d}x$$

C'est intuitivement la valeur que l'on s'attend √† trouver (en moyenne...).

### Th√©or√®me de Bayes

:::strong
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
:::

:::correction
D√©monstration par $P(A\cap B) = P(A) P(B|A) = P(B) P(A|B)$
:::

:::link
Voir : [Th√©or√®me de Bayes][wiki-bayes].
:::

### Loi binomiale

Exemple :

- QCM de 3 questions et 4 r√©ponses dont une seule correcte par question.
- Population tr√®s nombreuse.
- Variable $X$ : nombre de r√©ponses correctes donn√©es par le sujet.
- Si les sujets r√©pondent au hasard, quelle est la fr√©quence de $X = 2$ ?

#### Combinaisons

Le nombre de mani√®res de choisir $k$ √©l√©ments parmi $n$ √©l√©ments est appel√© _nombre de combinaisons de $n$ √©l√©ments pris $k$ √† $k$_ et est d√©fini par le coefficient binomial :

$$\binom{n}{k} = C_n^k = \frac{n!}{k!(n-k)!}$$

Avec factorielle $n$ : $n! = 1 \cdot 2 \cdot ... \cdot n$.

#### Loi de Bernoulli

Une loi de Bernoulli est une variable statistique $X$ √† 2 modalit√©s : 1 ou 0 (ou succ√®s/√©chec).

Caract√©ristiques : $\mu = p, \sigma^2 = p(1-p)$ avec $p$ la fr√©quence de la modalit√© "succ√®s".

#### Loi binomiale

La variable $X$ : "nombre de succ√®s observ√©s lorsqu'on r√©p√®te $n$ (entier) fois, de fa√ßon ind√©pendante, une exp√©rience de Bernoulli de param√®tre $p$" suit une _loi binomiale de param√®tres $n$ et $p$_ :

- ses modalit√©s sont $0, 1, ..., n$.
- La fr√©quence de la modalit√© $k$ est donn√©e par : $f_k = \mathbb P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$.

Caract√©ristiques : $\mu = np, \sigma^2 = np(1-p)$

![Exemples de distributions binomiales](@assets/data/binomial-distribution.png)

<div class="caption">Exemples de distributions binomiales. (Cr√©dits et sources : https://commons.wikimedia.org/wiki/File:Binomial_Distribution.PNG)</div>

### Loi normale

Loi th√©orique d'une variable _continue_ approchant bien les distributions exp√©rimentales dans lesquelles la dispersion de la variable r√©sulte d'effets nombreux, additifs, ind√©pendants et de m√™me grandeur.

:::link
Voir la [page Wikipedia sur la loi normale][wiki-loi-normale].
:::

![Exemple de loi normale](@assets/data/normal-distribution.svg)

<div class="caption">Exemples de distributions normales. (Cr√©dits et sources : https://commons.wikimedia.org/wiki/File:Normal_Distribution_PDF.svg)</div>

#### Cas particulier

La loi normale centr√©e : $\mu = 0$ et r√©duite : $\sigma = 1$.

Sa fonction de densit√© est : $f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$.

#### Cas g√©n√©ral

Une variable statistique $X$ de moyenne $\mu$ et d'√©cart-type $\sigma$ suit la loi normale $\mathcal N(\mu,\sigma)$ si : $Z = \frac{X - \mu}{\sigma}$ suit une loi normale centr√©e r√©duite : $\mathcal N(0,1)$.

#### Th√©or√®me fondamental : convergence en loi normale

Cette loi est tr√®s utile car la loi d'une somme de variables al√©atoires ind√©pendantes et identiquement distribu√©es converge g√©n√©ralement vers une loi normale.

## Tests statistiques

### Hypoth√®se nulle

- En statistiques l'hypoth√®se nulle ($H_0$) est une hypoth√®se postulant l'√©galit√© entre des param√®tres statistiques (par exemple la moyenne ou la variance) de deux √©chantillons dont elle fait l'hypoth√®se qu'ils sont pris sur des populations √©quivalentes.
- Hypoth√®se vraie par d√©faut (car prouver une √©galit√© stricte a une probabilit√© de 0 donc n'est pas r√©aliste) : en traitement des donn√©es on l'utilisera pour d√©finir un risque au-del√† duquel on rejettera l'hypoth√®se faite sur le mod√®le (ind√©pendance, etc...).
- Vision statistique d'une d√©monstration par l'absurde

#### Exemple

Si l'on veut tester l'√©galit√© des tailles moyennes chez les femmes (groupe 1) et les hommes (groupe 2) :

$$ H_0 : \mu_1 = \mu_2 $$

O√π :

- $H_0$ : hypoth√®se nulle
- $\mu_1$ = moyenne population 1
- $\mu_2$ = moyenne population 2

### p-valeur

Dans un test statistique, la _p-valeur_ est la probabilit√© pour un mod√®le statistique donn√© sous l'hypoth√®se nulle d'obtenir une valeur au moins aussi extr√™me que celle observ√©e.

En termes statistiques, la p-valeur s'interpr√®te comme la probabilit√© d'un r√©sultat au moins aussi "extr√™me" que le r√©sultat observ√©, sachant l'hypoth√®se nulle:

:::strong
$$p = \mathbb P(x|H_0)$$
:::

Le r√©sultat d'une p-valeur "improbable" (√† d√©finir) implique que l'exp√©rience observ√©e ne suit pas l'hypoth√®se nulle. Par exemple, imaginons que l'on connaisse la loi r√©partissant le poids des individus d'une population en surpoids et qu'on teste un traitement amincissant sur un groupe de personnes. On √©value le poids moyen du groupe apr√®s le traitement et on v√©rifie avec la loi initiale si le r√©sultat est probable ou improbable. S'il est improbable, le traitement est efficace. 

On utilise g√©n√©ralement les seuils suivants :

- $p\leq 0.01$ : tr√®s forte pr√©somption contre l'hypoth√®se nulle ;
- $0.01 < p\leq 0.05$ : forte pr√©somption contre l'hypoth√®se nulle ;
- $0.05 < p\leq 0.1$ : faible pr√©somption contre l'hypoth√®se nulle ;
- $0.1 < p$ : pas de pr√©somption contre l'hypoth√®se nulle.

![Exemple graphique de p-valeur sur une loi normale](@assets/data/Valeur-p.jpg)

<div class="caption">Illustration de la p-valeur</div>

$X$ d√©signe la loi de probabilit√© de la statistique de test et $z$ la valeur calcul√©e de la statistique de test. [Source et cr√©dits : Wikimedia](https://commons.wikimedia.org/wiki/File:Valeur-p.jpg)_.

:::warn
La p-valeur interpr√®te $\mathbb P(x|H_0)$ or en faisant un test statistique, on cherche √† savoir quelle est la probabilit√© que $H_0$ soit vraie sachant les donn√©es donc $\mathbb P(H_0|x)$ ! D'apr√®s le th√©or√™me de Bayes : $\mathbb P(x|H_0) = \frac{\mathbb P(H_0 | x) \mathbb P(x)}{\mathbb P(H_0)}$
:::

### Test de normalit√©

- Ces tests permettent de v√©rifier si des donn√©es r√©elles suivent une [loi normale][wiki-loi-normale] (not√©e $\mathcal N$).
- Tr√®s utiles en statistiques : de nombreux tests supposent la normalit√© des distributions pour √™tre applicables.
- Exemple : application aux r√©sidus d'un mod√®le de r√©gression lin√©aire - ces r√©sidus ne peuvent pas √™tre utilis√©s dans des tests qui font intervenir des hypoth√®ses de normalit√© (test du $\chi^2$, ...). Si les r√©sidus ne sont pas normalement distribu√©s, cela signifie que la variable d√©pendante ou au moins une variable explicative pourrait avoir une fonction de r√©partition erron√©e.

![Exemples de distributions normale et non-normale](@assets/data/Normality_histogram.png)

<div class="caption">Exemples de distributions suivant une loi normale (√† gauche) et n'en suivant pas (√† droite). Source et cr√©dits : https://commons.wikimedia.org/wiki/File:Normality_histogram.png</div>

### Test du Khi-2

C'est un test statistique pour rejetter une hypoth√®se d'ind√©pendance entre variables.

$H_0$ : $X$ suit une [loi du $\chi^2$][wiki-loi-chi-2] √† $k$ degr√©s de libert√© si :

:::strong
$$X=\sum_{i=1}^k X_i^2$$
:::

o√π $X_1, ..., X_k$ sont $k$ variables al√©atoires ind√©pendantes qui suivent la loi normale de moyenne 0 et d'√©cart-type 1 : $\mathcal N(0,1)$.

#### Principe

- On r√©partie les donn√©es en $J$ classes ;
- On calcule la statistique de test $T$ (voir ci-dessous) ;
- On utilise une [table de valeurs des quantiles][wiki-chi-2-table] du $\chi^2$ par degr√© de libert√©.
- On rejette $H_0$ pour une _p-valeur_ choisie (appel√© niveau de risque $\alpha$), g√©n√©ralement : $p\leq 0.05$.

:::link
[Table de valeurs des quantiles][wiki-chi-2-table]
:::

#### Exemple d'une loi multinomiale

On d√©finit alors la statistique du $\chi_2$ par :

:::strong
$$T = \sum_{j=1}^J \frac{(N \hat{p}_j - N p_j)^2}{N p_j}$$
:::

o√π :

- $v_1, ..., v_J$ sont les $J$ valeurs possibles de $Y$ ;
- $y_1, ..., y_N$ sont les $N$ observations de $Y$ ;
- $p_j$ est la probabilit√© th√©orique que $Y$ prenne la valeur $v_j$ ;
- $\hat{p}_j$ est la probabilit√© empirique (mesur√©e) que $Y$ prenne la valeur $v_j$ ;
- $N p_j = \sum_{i=1}^N 1_{y_i = v_j}$ (idem pour $N \hat{p}_j$ avec la probabilit√© mesur√©e).

On d√©montre que sous $H_0$, $T$ converge vers une loi du $\chi^2$ √† $(J-1)$ degr√©s de libert√©.

Ainsi, pour un test de niveau $\alpha$, on en d√©duit le rejet de $H_0$ si $T$ est plus grande que le quantile $1 - \alpha$ de la loi √† $(J - 1)$ degr√©s de libert√©.

On g√©n√©ralise ce cas particulier √† une variable al√©atoire $Y$ prenant un nombre d√©nombrable de valeurs ou √† une variable al√©atoire continue, en d√©coupant l'ensemble des valeurs que peut prendre $Y$ en $J$ classes distinctes.

#### Exercice

:::exo
On souhaite tester l'hypoth√®se selon laquelle un d√© √† six faces n'est pas truqu√©, avec un risque $\alpha = 0.05$.

Pour cela, le d√© est lanc√© 600 fois de suite. S'il est √©quilibr√©, on s'attend que sur ces 600 jets, chaque chiffre tombe 100 fois.

Supposons que notre exp√©rience donne les r√©sultats suivants :

| num√©ro tir√© | 1  | 2   | 3   | 4  | 5   | 6  |
|-------------|----|-----|-----|----|-----|----|
| effectifs   | 88 | 109 | 107 | 94 | 105 | 97 |

c'est-√†-dire nous avons obtenu 88 fois le chiffre 1, 109 fois le chiffre 2, etc.


1. Quelle est l'hypoth√®se nulle $H_0$ ?
2. Combien y a-t-il de degr√©s de libert√© ?
3. En consid√©rant l'hypoth√®se nulle vraie, d√©finir la variable statistique du $\chi^2$ : $T$.
4. Peut-on consid√©rer que le d√© est truqu√© avec un risque de 5% ?
5. M√™me question pour le nouveau tirage suivant :

| num√©ro tir√© | 1  | 2   | 3  | 4  | 5   | 6  |
|-------------|----|-----|----|----|-----|----|
| effectifs   | 89 | 131 | 93 | 92 | 104 | 91 |

:::

:::correction

1. L'hypoth√®se que l'on souhaite rejeter (qu'on appelle hypoth√®se nulle et qu'on note $H_0$) est donc ici : "Le d√© est √©quilibr√©". 
2. Le nombre de degr√©s de libert√© est de $6 - 1 = 5$. En effet, $88 + 109 + 107 + 94 + 105 + 97 = 600$ et si l'on conna√Æt par exemple les nombres de fois o√π l'on obtient les chiffres 1 √† 5, on conna√Æt le nombre de fois o√π l'on obtient le chiffre 6 : $600 - (88 + 109 + 107 + 94 + 105) = 97$. 
3. En consid√©rant l'hypoth√®se nulle vraie, la variable T d√©finie pr√©c√©demment vaut : ${\frac {(88-100)^{2}}{100}}+{\frac {(109-100)^{2}}{100}}+{\frac {(107-100)^{2}}{100}}+{\frac {(94-100)^{2}}{100}}+{\frac {(105-100)^{2}}{100}}+{\frac {(97-100)^{2}}{100}}=3.44$
4. La statistique T suit la loi du $\chi^2$ √† cinq degr√©s de libert√©. Cette loi donne la valeur en de√ß√† de laquelle on consid√®re le tirage comme conforme avec un risque $\alpha = 0.05 : P(T < 11.07) = 0.95$. Puisque $3.44 < 11.07$, on ne peut pas rejeter l'hypoth√®se nulle : ces donn√©es statistiques ne permettent pas de consid√©rer que le d√© est truqu√©.
5. $T ={\frac {(89-100)^{2}}{100}}+{\frac {(131-100)^{2}}{100}}+{\frac {(93-100)^{2}}{100}}+{\frac {(92-100)^{2}}{100}}+{\frac {(104-100)^{2}}{100}}+{\frac {(91-100)^{2}}{100}}=12.92 > 11.07$ : on peut rejeter l'hypoth√®se nulle : le d√© est truqu√©. 

:::

## L'apprentissage automatique

### Pr√©sentation

- M√©thodes qui permettent √† une machine d'√©voluer gr√¢ce √† un processus d'apprentissage.
- Permet d'accomplir des t√¢ches qu'il est difficile ou impossible de remplir par des moyens algorithmiques plus classiques.
- Objectif : extraire et exploiter automatiquement l'information pr√©sente dans un jeu de donn√©es.

### Apprentissage supervis√© vs non-supervis√©

:::tip
Les algorithmes d'apprentissage peuvent se cat√©goriser selon le type d'apprentissage qu'ils emploient :

- L'apprentissage **supervis√©**
  + Un expert est employ√© pour √©tiqueter correctement des exemples.
  + La machine doit trouver ou approximer la fonction qui permet d'affecter la bonne √©tiquette aux donn√©es.
- L'apprentissage **non supervis√©**
  + Aucun expert n'est disponible.
  + La machine doit d√©couvrir par elle-m√™me la structure des donn√©es.
- L'apprentissage **par renforcement** (hors programme)
  + La machine apprend √† se rapprocher d'une strat√©gie comportementale optimale par des interactions r√©p√©titives avec l'environnement.
  + L'apprentissage se fait sans supervision.
  + Repose sur le principe d'essai/erreur.
  + Utile quand l'environnement √©volue.
:::

# Apprentissage supervis√©

√Ä partir d'un √©chantillon d'apprentissage $D_n = (x_1, y_1), ... ,(x_n, y_n)$ , inf√©rer la relation entre $x$ et $y$.

:::tip
Exemples d'utilisation :

- D√©tecteur de spam
- Risque de cr√©dit
- Pr√©diction des pics d'ozone
- Aide au diagnostic m√©dical (ex : cancer du sein)
- Aide au pilotage
- Moteurs de recommandation, apprentissage sur les graphes
:::

Cadre classique :

- Donn√©es : √©chantillon d'apprentissage $(x_k, y_k)_{1\leq k\leq n}$ constitu√© d'observations que l'on suppose repr√©sentatives et sans lien entre elles.
- Objectif : pr√©dire les valeurs de $y$ associ√©es √† chaque valeur possible de $x \in X$.
- _Classification_ : $Y$ discret (on peut d√©nombrer l'ensemble des valeurs: variables _qualitatives_). Pour chaque valeur de $x \in X$ on pr√©dit la classe la plus souvent associ√©e.
  + Classification _non param√©trique_ (_classification hi√©rarchique_, _m√©thode des centres mobiles_) : ne consid√®re qu'une seule hypoth√®se - plus deux individus sont proches, plus ils ont de chances de faire partie de la m√™me classe.
  + _Classification probabiliste_ : utilise une hypoth√®se sur la distribution des individus √† classifier (par exemple, suivre une _loi normale_). On d√©termine alors quels sont les param√®tres des lois (_moyenne_, _variance_) et √† quelle classe les individus ont le plus de chances d'appartenir.
- _R√©gression_ : M√©thodes d'analyse statistique permettant d'approcher une variable √† partir d'autres qui lui sont corr√©l√©es. G√©n√©ralement $Y$ est continue ou est une fonction (variables _quantitatives_).
- R√®gle de classification : √† partir de l'√©chantillon d'apprentissage, construire $f_n : X \rightarrow Y$ associant √† chaque entr√©e possible $x$ la classe $y$ pr√©dite pour elle.

```mermaid
---
title: Sch√©ma d'apprentissage supervis√©
---
flowchart TD
  subgraph Superviseur
    superviseur["superviseur"]
    sortie_desiree["sortie d√©sir√©e"]
    erreur["erreur"]

    superviseur --> sortie_desiree --> erreur
  end

  subgraph R√©seau
    r√©seau["r√©seau"]
    sortie_obtenue["sortie obtenue"]

    r√©seau --> sortie_obtenue --> erreur
  end

  entr√©es["entr√©es"]
  entr√©es --> superviseur
  entr√©es --> r√©seau
  erreur -.-> r√©seau

  %% Label as a note below
  classDef labelStyle fill:#fff,stroke:none,color:#666,font-style:italic,font-size:12px;
  label["Sch√©ma d'apprentissage supervis√©"]:::labelStyle

  class Superviseur blue
  class R√©seau green
```

## M√©thodes de r√©gression

On consid√®re une population d'individus (√™tres humains, animaux, pays, biens de consommation, ...) qui peuvent √™tre d√©crits selon plusieurs crit√®res appel√©s variables. Il peut s'agir de variables _quantitatives_ (grandeurs num√©riques telles que la taille, l'√¢ge, le prix, un pourcentage, ...) ou _qualitatives_ (sexe, cat√©gorie socio-professionnelle, saison, type de produit, ...)

Certaines variables peuvent √™tre plus difficiles √† mesurer que d'autres, pour des raisons techniques, des raisons d'acc√®s (donn√©es publiques contre donn√©es priv√©es), ou encore du fait d'un d√©lai important entre la mise en place d'une exp√©rience et son aboutissement. Il arrive donc que l'on souhaite estimer ces variables (dites _expliqu√©es_) √† partir des donn√©es plus faciles √† obtenir (dites _explicatives_).

La construction de la r√©gression repose d'une part sur une mod√©lisation des variables statistiques par des variables al√©atoires (r√©elles ou non), d'autre part sur un recueil de donn√©es crois√©es, c'est-√†-dire que pour un m√™me √©chantillon de population, on dispose d'observations des diff√©rentes variables mesur√©es avec une impr√©cision √©ventuelle.

La r√©gression consiste alors √† formuler un indicateur sur les valeurs de la variable expliqu√©e d√©pendant uniquement des valeurs des variables explicatives. Cet indicateur pourra ensuite √™tre utilis√© sur une population pour laquelle on ne connait que les valeurs des variables explicatives, afin d'estimer les valeurs de la variable expliqu√©e.

On distingue essentiellement deux cas selon la nature de la variable expliqu√©e, repr√©sent√©e ici par une variable al√©atoire $Y$. Les variables explicatives seront not√©es $X_1, ... , X_n$.

### Cas quantitatif

Pour une variable expliqu√©e quantitative, c'est-√†-dire lorsque $Y$ est une variable al√©atoire r√©elle, la fonction de r√©gression est d√©finie par un indicateur de la loi de $Y$ conditionnellement aux valeurs des autres variables. Il s'agit le plus souvent de l'esp√©rance conditionnelle $f(x_{1}, ... ,x_{n})=\mathbb {E} (Y|X_{i}=x_{i})$, mais on peut aussi consid√©rer d'autres indicateurs de distribution conditionnelle comme la m√©diane, la variance...

C'est donc une fonction num√©rique, dont les arguments sont des valeurs possibles des variables explicatives.

Cependant, la d√©finition probabiliste ne suffit pas en g√©n√©ral pour d√©crire la construction de l'indicateur √† partir d'un jeu de donn√©es statistiques. En effet, les observations ne fournissent pas toujours toutes les combinaisons de modalit√©s dans le cas de variables explicatives discr√®tes, et ne peuvent √™tre exhaustives dans le cas de variables explicatives continues. La r√©gression s'appuie alors sur un mod√®le de fonction avec un ou plusieurs param√®tres, par exemple une fonction affine dans le cas de la r√©gression lin√©aire ou multilin√©aire.

### Cas qualitatif

Pour une variable expliqu√©e qualitative, la r√©gression s'apparente aux probl√®mes de classification, au sens o√π l'on cherche √† d√©terminer une modalit√© √† partir des valeurs des autres variables.

La fonction de r√©gression correspond dans ce cas √† un partage de l'espace des valeurs des variables explicatives, par des m√©thodes g√©om√©triques ou par _r√©gression logistique_.

### Principaux mod√®les de r√©gression

- On parle de _mod√®le lin√©aire_ lorsque les param√®tres apparaissent comme les coefficients d'une combinaison lin√©aire de fonctions de r√©f√©rence, quitte √† passer par un changement de variable.
- Si la variable expliqu√©e est une variable al√©atoire binomiale (0 ou 1), il est courant d'utiliser une _r√©gression logistique_.

### R√©gression lin√©aire

Le but de la r√©gression lin√©aire simple (respectivement multiple) est d'expliquer une variable $Y$ √† l'aide d'une variable $X$ (respectivement plusieurs variables $X_1, ... , X_q$).

La variable $Y$ est appel√©e variable _√† expliquer_ ou variable _d√©pendante_ et les variables $X_j (j=1, ... ,q)$ sont appel√©es variables _explicatives_ ou variables _ind√©pendantes_. 

:::strong
Le but est donc de trouver une fonction $f$ telle que : $y_i \approx f(x_i)$.
:::

Exemple simple : ajustement affine : $y = ax + b$.

- L'existence d'une relation entre $X$ et $Y$ n'implique pas n√©cessairement une relation de causalit√© entre elles !
- Exemple : corr√©lation chocolat / prix Nobel : <https://www.bbc.com/news/magazine-20356613>.
- Autres exemples √©tranges : <https://www.tylervigen.com/spurious-correlations>.

#### Exemple

_√âtude de la relation entre la tension art√©rielle et l'√¢ge d'un individu_.

Les donn√©es sont extraites de _Bouyer et al. (1995) Epid√©miologie. Principes et m√©thodes quantitatives, Les √©ditions INSERM._

1. Objectif
  - On souhaite savoir si, de fa√ßon g√©n√©rale, l'√¢ge a une influence sur la tension art√©rielle et sous quelle forme cette influence peut √™tre exprim√©e.
  - Le but est d'expliquer au mieux comment la tension art√©rielle varie en fonction de l'√¢ge et √©ventuellement de pr√©dire la tension √† partir de l'√¢ge.
2. Population et variables √©tudi√©es 
  - Population g√©n√©rale d'individus sur laquelle on d√©finit 2 variables :
  - La variable $Y$ : variable tension - c'est la variable √† expliquer (variable √† r√©gresser)
  - La variable $X$ : variable √¢ge - c'est la variable explicative (variable r√©gresseur)
3. √âchantillon al√©atoire d'individus
  - Pour l'√©tude, on doit faire des mesures sur $n$ individus tir√©s au sort dans la population.
  - On observe deux √©chantillons appari√©s de $X$ et $Y$ de taille $n$ : $(x_1, y_1), ... , (x_i, y_i), ... , (x_n, y_n)$ o√π $x_i$ et $y_i$ sont les valeurs de $X$ et $Y$ observ√©es sur le $i^{eme}$ individu tir√© au sort.
4. Mod√®le exprimant la relation entre $Y$ et $X$
  - On cherche √† exprimer la relation entre la variable tension et la variable √¢ge √† l'aide d'une fonction math√©matique du type $y = f(x)$.
  - Graphiquement cela revient √† repr√©senter cette relation √† l'aide d'une courbe (graphe de la fonction).
5. Choix du mod√®le
  - Quelle fonction math√©matique utiliser ?
  - Pour choisir le mod√®le de relation, on doit faire des observations sur un √©chantillon d'individus.
  - Les donn√©es recueillies sur ces individus sont repr√©sent√©es graphiquement √† l'aide d'un nuage de points.
  - Si le nuage a une forme particuli√®re s'apparentant √† une courbe math√©matique, on choisira la fonction math√©matique correspondant √† cette courbe.

![Exemple de r√©gression lin√©aire](@assets/data/reg-lin-ex1.png)

<div class="caption">Source et cr√©dit : Ana Karina Fermin Rodriguez.</div>
 
La forme √©tir√©e et croissante du nuage sugg√®re une relation lin√©aire entre la tension et l'√¢ge. Le coefficient de corr√©lation lin√©aire observ√© sur l'√©chantillon est $r = 0, 7868$ assez proche de $1$ : on va mod√©liser cette relation √† l'aide d'une droite.

### √âquation g√©n√©rale du mod√®le de r√©gression lin√©aire simple

Si la relation √©tait parfaitement lin√©aire cela se traduirait par des points align√©s et l'on pourrait √©crire la relation entre $Y$ et $X$ sous la forme :

$$Y = aX + b$$

Connaissant l'√¢ge $x$ d'un individu, l'√©quation permettrait de d√©terminer exactement la tension art√©rielle $y$.

Or la relation observ√©e sur l'√©chantillon n'est pas exacte : le nuage est √©tir√© mais les points ne sont pas align√©s.
De plus, on voit que des personnes du m√™me √¢ge ont des tensions art√©rielles diff√©rentes.

Ces diff√©rences peuvent √™tre expliqu√©es par d'autres variables ayant une influence sur la variable tension et qui ne sont pas prises en compte dans le mod√®le, ou encore par des erreurs de mesures.

Pour rendre compte de cette situation, on √©crit la relation entre la tension et l'√¢ge sous la forme g√©n√©rale suivante : _droite + erreur_ :

:::strong
$$Y_i = aX_i + b + \epsilon_i, i=1..n$$
:::

Pour un √¢ge $x$ donn√©, la tension d'un individu est la somme de deux termes :

- $ax+b$ enti√®rement d√©termin√© par l'√¢ge ;
- le terme d'erreur $\epsilon$ (variable al√©atoire) qui varie de fa√ßon al√©atoire d'un individu √† l'autre :
  + Synth√©tise toutes les variables influant sur la tension et qui ne sont pas prises en compte.
  + Les erreurs doivent √™tre des variables ind√©pendantes, de m√™me loi, centr√©es et de m√™me variance.
  + $\epsilon_i$ est l'observation de cette variable : repr√©sente l'erreur commise (l'√©cart de valeur entre $Y$ observ√©e et le mod√®le).
  
La variable $Y$ est donc al√©atoire. La variable $X$ est suppos√©e non al√©atoire, on la mesure sans erreur sur chaque individu.

Le mod√®le de r√©gression lin√©aire est souvent estim√© par la m√©thode des moindres carr√©s mais il existe aussi de nombreuses autres m√©thodes pour estimer ce mod√®le (par exemple si les termes d'erreurs ne sont pas ind√©pendants).

On peut par exemple estimer le mod√®le par [maximum de vraisemblance][wiki-max-vraisemblance] ou encore par [inf√©rence bay√©sienne][wiki-inference-bayesienne].

### M√©thode des moindres carr√©s - Khi-deux

La m√©thode des moindres carr√©s permet de comparer des donn√©es exp√©rimentales (incluant des erreurs de mesure) √† un mod√®le math√©matique cens√© d√©crire ces donn√©es.

Le mod√®le th√©orique est une famille de fonctions $f(x;\theta)$ d'une ou plusieurs variables muettes $x$, index√©es par un ou plusieurs param√®tres $\theta$ inconnus.

La m√©thode des moindres carr√©s permet de s√©lectionner parmi ces fonctions celle qui reproduit le mieux les donn√©es exp√©rimentales.


![M√©thode des moindres carr√©s](@assets/data/Moindres_carres_introduction.png)

<div class="caption">Source et cr√©dits : Nicolas Regnault https://commons.wikimedia.org/wiki/File:Moindres_carres_introduction.png</div>



L'id√©e est de minimiser les √©carts quadratiques (i.e. au carr√©) entre l'observation et le mod√®le, ainsi pour $N$ observations on minimise :

$$S(\theta) = \sum_{i=1}^N \left(y_i - f(x_i;\theta)\right)^2$$

#### Cas de la r√©gression lin√©aire

Lorsque l'on conna√Æt l'√©cart-type $\sigma_i$ des erreurs de chaque mesure $y_i$ et si les erreurs sont distribu√©es selon une [loi normale][wiki-loi-normale], on utilise cet √©cart-type pour pond√©rer la contribution de la mesure : une mesure aura d'autant plus de poids que son incertitude sera faible.

On retrouve alors un test du $\chi^2$ :

$$\chi^2(\theta) = \sum_{i=1}^N \left(\frac{y_i - f(x_i;\theta)}{\sigma_i}\right)^2$$

Dans le cas d'une r√©gression lin√©aire simple, on v√©rifie que cette valeur est minimale pour $grad(\chi^2) = 0$ et par d√©veloppement, pour un mod√®le $Y=aX+b+\epsilon$ et si $Var(X)\neq 0$ :

:::strong
$$a = \frac{Cov(X,Y)}{Var(X)}$$

$$b = \bar{Y} - a \bar{X}$$
:::

:::warn
Attention : dans la pratique, on ne conna√Æt jamais $a$ et $b$ (valeurs exactes du mod√®le) ! 

On calcule donc : $\hat{a}$ et $\hat{b}$, valeurs estim√©es de $a$ et $b$ dans les conditions de l'exp√©rience.

Ces valeurs ne sont donc pas exactes - nous verrons dans la suite du cours comment v√©rifier leur v√©racit√©.
:::

#### Cas g√©n√©ral

:::link
Pour un calcul de $grad(\chi^2) = 0$ dans le cas g√©n√©ral, voir la [page Wikipedia de la m√©thode des moindres carr√©s][wiki-moindres-carres].
:::

#### ‚ùåLimites

Attention, la m√©thode des moindres carr√©s g√®re mal les valeurs aberrantes. Plusieurs m√©thodes existent pour √©viter une influence trop forte de ces valeurs :

- Modifier le $\chi^2$ en ne calculant plus le carr√© des r√©sidus mais une fonction bien choisie de ceux-ci (m√©thodes des M-estimateurs) ;
- Remplacer la somme par la m√©diane, qui contrairement √† la moyenne est un estimateur robuste (m√©thode des moindres carr√©s m√©dians).

:::link
La m√©thode des moindres carr√©s est un cas particulier du [maximum de vraisemblance][wiki-max-vraisemblance] - cette m√©thode est utile notamment si les r√©sidus ne sont pas ind√©pendants.
:::

#### Exemple

Reprenons notre exemple pour r√©aliser une r√©gression lin√©aire simple.

On obtient : $\hat{a} = 1.5771$ et $\hat{b} = 60.3928$.

![R√©gression lin√©aire simple](@assets/data/reg-lin-ex2.png)

<div class="caption">Source et cr√©dit : Ana Karina Fermin Rodriguez.</div>

:::exo
Quelle est la signification des param√®tres $a$ et $b$ ?
:::

:::correction

- $a$ est la pente de la droite : une augmentation de l'√¢ge d'un an se traduit par une augmentation ($a > 0$) de la tension estim√©e √† $1,5771$.
- $b$ est l'ordonn√©e √† l'origine : attention √† ne pas extrapoler la droite au del√† des limites du domaine observ√© de $X$. Ici, la droite a √©t√© ajust√©e pour des √¢ges compris entre 40 et 66 ans. Le coefficient fixe uniquement la hauteur de la droite.

:::

### Qualit√© de la pr√©diction

Une fois la pr√©diction r√©alis√©e, on s'int√©resse √† la qualit√© de l'estimation.

Nous allons calculer la qualit√© de la r√©gression $Y = \hat{a} X + \hat{b}$ pour les donn√©es de l'exp√©rience.

En r√©alit√©, si l'on r√©alise plusieurs fois cette exp√©rience, on trouvera d'autres valeurs de $\hat{a}$ et $\hat{b}$ (les vraies valeurs de $a$ et $b$ ne seront jamais connues).

Pour √©valuer la qualit√© de la pr√©diction, il faudrait donc √©galement calculer :

- l'intervalle de confiance de $\hat{a}$.
- l'intervalle de confiance de $\hat{b}$.
- l'intervalle de confiance de $\hat{y_i}$.
- v√©rifier les conditions initiales sur les r√©sidus : hypoth√®se de normalit√©, ...

#### Qualit√© de l'ajustement

La premi√®re √©tape consiste √† calculer la variance expliqu√©e par la r√©gression, c'est-√†-dire la variation des valeurs ajust√©es autour de la moyenne $\bar{y}$ :

$$SSE = \sum_{i=1}^N (\hat{y}_i - \bar{y})^2$$

#### R√©sidus (_SSR_)

La seconde √©tape consiste √† calculer la variance r√©siduelle ou non expliqu√©e, c'est-√†-dire la variation qui n'est pas expliqu√©e par le mod√®le de r√©gression :

1. Calculer la pr√©diction : $\hat{y_i} = a x_i + b$
2. La valeur $e_i = y_i - \hat{y_i}$ s'appelle le r√©sidu : c'est l'√©cart entre la pr√©diction et l'observation (et donc, une approximation de l'erreur $\epsilon$).
3. On calcule la somme des carr√©s des r√©sidus : $SSR = \sum e_i^2$.
4. On peut alors estimer sans biais $\sigma^2$ (√©cart-type des erreurs) par : $\hat{\sigma}^2 = \frac{SSR}{n-2}$.

$$SSR = \sum_{i=1}^N (y_i - \hat{y_i})^2$$

:::tip
Pourquoi utiliser un estimateur sans biais pour $\sigma^2$ : $\hat{\sigma}^2 = \frac{SSR}{n-2}$ et non $\frac{1}{n}SSR$ ?.

Ce biais provient du fait que nos calculs utilisent une estimation de la moyenne (bas√©e sur des valeurs empiriques) et non l'esp√©rance th√©orique de la variable. Cette notion sera approfondie plus en d√©tails en TP.

Pour plus d'information, voir [la page Wikipedia sur le biais d'un estimateur][wiki-bias].
:::

#### Coefficient de d√©termination lin√©aire de Pearson

On en d√©duit alors la variation totale des observations $y_i$ autour de leur moyenne $\bar{y}$ :

$$SST = \sum (y_i - \bar{y})^2 = SSR + SSE$$

On d√©finit alors $R^2$ le _coefficient de d√©termination_ qui mesure la part de la variation de $Y$ expliqu√©e par $X$ :

:::strong
$$R^2 = \frac{SSE}{SST}$$
:::

Lorsque $R^2$ est proche de 1, le mod√®le de r√©gression explique bien la variation totale (condition n√©cessaire mais pas suffisante).

Dans notre exemple : $r^2=0.6191$ : le mod√®le explique $61,91$ % de la variation totale.

:::tip
Pourquoi utiliser la m√™me notation : $R^2$ pour le carr√© du coefficient de corr√©lation lin√©aire et pour le coefficient de d√©termination ?

On [d√©montre](https://en.wikipedia.org/wiki/Residual_sum_of_squares) que ces deux valeurs sont en effet √©quivalentes :

$$R^2=\rho^2$$

:::

#### Tests de significativit√©

##### Significativit√© globale

On peut tester la significativit√© globale d'une r√©gression :

- $H_0$ : $a = 0$ (contre $H_1$ : $a\neq 0$)
- On utilise une statistique de _Fisher(1,n-2)_ : $F=(n-2)\frac{R^2}{1-R^2}$
- On rejette $H_0$ si $F$ est grande, donc on rejette $H_0$ au risque d'erreur $\alpha$ si $\alpha_{obs}\leq \alpha$
- $\alpha_{obs} = \mathbb P_{H_0}(F(1,n-2) > (n-2)\frac{r^2}{1-r^2})$.

##### Tests sur les param√®tres

On peut tester chacun des param√®tres :

- (a) Est-ce que le coefficient $a$ est non nul, autrement dit la variable $X$ a-t-elle r√©ellement une influence sur $Y$ ?
  + $H_0$ : $a = 0$ (contre $H_1$ : $a\neq 0$)
- (b) Est-ce que le coefficient $b$ est non nul, autrement dit faut-il une constante dans le mod√®le ?
  + $H_0$ : $b = 0$ (contre $H_1$ : $b\neq 0$)
- On utilise une statistique $T$ de _Student(n-2)_ : $T = \frac{\hat{a}}{\hat{\sigma}_{\hat{a}}}$ (idem pour $b$).
- On rejette $H_0$ si $|T|$ est grande, donc on rejette $H_0$ au risque d'erreur $\alpha$ si $\alpha_{obs}\leq \alpha$
- $\alpha_{obs} = 2 \mathbb P_{H_0}(T(n-2) > \left|\frac{\hat{a}}{\hat{\sigma}_{\hat{a}}}\right|)$ (idem pour $b$).

### R√©gression lin√©aire multiple

Le mod√®le de r√©gression multiple est une g√©n√©ralisation du mod√®le de r√©gression simple lorsque les variables explicatives sont en nombre fini.

Exemples :

- Quelles variables permettent de pr√©dire les sympt√¥mes anxieux ?
- Est-ce que la satisfaction au travail varie en fonction de l'augmentation des d√©fis √† relever et de l'esprit d'√©quipe ?

On suppose alors que les donn√©es collect√©es suivent le mod√®le suivant :

$$y_i = b_0 + b_1xi_1 + b_2xi_2 + ... + b_pxi_p + \epsilon_i, i = 1, ... , n$$

Tous les r√©sultats pr√©c√©dents se g√©n√©ralisent dans le cas g√©n√©ral.

Il se peut que le nombre $p$ des variables disponibles soit (trop) grand. On peut essayer de r√©duire ces variables :

- en partant du mod√®le complet et en retirant les variables superflues.
- en partant d'une r√©gression simple puis en ajoutant les variables int√©ressantes pour enrichir le mod√®le.

### R√©gression lin√©aire g√©n√©ralis√©e (_GLM_)

Les GLM sont une extension des mod√®les lin√©aires classiques qui peuvent √™tre utilis√©s lorsque les r√©ponses ne sont pas de type num√©rique continues :

- Les donn√©es sont de type comptage (nombre de v√©los, ...)
  + elles suivent th√©oriquement une loi de Poisson de param√®tre $\lambda$.
- Les donn√©es sont de type binaire (Malade/non malade, ...)
  + on parle de _r√©gression logistique_ (chapitre suivant du cours).

#### Pr√©dicteur lin√©aire

Comme dans les mod√®les lin√©aires classiques, les r√©ponses pr√©dites par les mod√®les vont l'√™tre √† partir d'une combinaison lin√©aire des variables pr√©dictives :

$$\eta = \sum_{j=1}^{p} \beta_j\ X_{ij}$$

#### Fonction de lien

Contrairement aux mod√®les lin√©aires classiques, les valeurs pr√©dites par le pr√©dicteur lin√©aire ne correspondent plus √† la pr√©diction moyenne d'une observation, mais √† la **transformation (par une fonction math√©matique) de cette moyenne**.

Autrement dit, il existe une fonction de lien $g$ telle que :

$$g(\bar{y}) = \eta = \sum_{j=1}^{p} \beta_j\ X_{ij}$$

:::link
Pour plus de d√©tails sur la r√©gression lin√©aire g√©n√©ralis√©e, voir [ce lien][slides-glm].
:::

### R√©gression de Poisson

La r√©gression de Poisson est l'application de la r√©gression g√©n√©ralis√©e _GLM_ pour des donn√©es de comptage (donc une loi de Poisson) :

$$g(\bar{y}) = log(\bar{y}) = \sum_{j=1}^{p} \beta_j\ X_{ij}$$

### R√©gression logistique (_LOGIT_)

- Mod√®le de r√©gression binomiale : on veut expliquer au mieux la pr√©sence ou l'absence d'une caract√©ristique (variable al√©atoire de Bernoulli $y$ dite _variable pr√©dite_) par des observations r√©elles nombreuses sur des variables al√©atoires $x_1, ..., x_k$ appel√©es _pr√©dicteurs_.
- Cas particulier du mod√®le lin√©aire g√©n√©ralis√© pour une variable binaire.
- Tr√®s utilis√© en apprentissage automatique.
- Par exemple en m√©decine : trouver les facteurs qui caract√©risent un groupe de sujets malades.

#### Hypoth√®se

$ln\frac{p(X|1)}{p(X|0)}$ (fonction `logit`) est lin√©aire, c'est-√†-dire :

:::strong
$$ln\left(\frac{p}{1-p}\right) = ln\left(\frac{p(Y=1|X)}{1-p(Y=1|X)}\right) = \sum_{j=1}^{p} \beta_j\ X_{ij}$$
:::

Ou apr√®s transformation, sous forme de [loi logistique][wiki-loi-logistique] :

$$p(Y=1 | X) = \frac{1}{1+e^{-(b_0+b_1x_1+...+b_kx_k)}}$$

Remarque : En analyse discriminante, on mod√©lise les densit√©s conditionnelles $p(X | Y=1)$ et $p(X | Y=0)$. Ici, c'est le rapport de ces densit√©s qui est mod√©lis√©. La restriction introduite par l'hypoth√®se est moins forte. De nombreuses distributions r√©pondent √† cette hypoth√®se : [distribution multinormale][wiki-distrib-multinormale], distributions de variables explicatives bool√©ennes, ...

#### Estimation

En pratique, il est rare d'avoir assez d'√©chantillons pour pouvoir estimer les $P(1|X)$ et $P(0|X)$ ce qui rend la m√©thode des moindres carr√©s difficile √† appliquer.

On utilise donc le [maximum de vraisemblance][wiki-max-vraisemblance]. Les algorithmes de calcul approchent ce maximum diff√©remment - il est donc normal d'obtenir des coefficients diff√©rents en utilisant des programmes diff√©rents.

#### Test du mod√®le

Dans la r√©gression logistique, le mod√®le de base est le plus grand nombre de cas, c'est-√†-dire la cat√©gorie (0 ou 1) qui obtient la fr√©quence la plus √©lev√©e.

Il ne serait pas judicieux d'utiliser la moyenne comme dans la r√©gression lin√©aire, puisque la moyenne de 0 et de 1 ne ferait pas de sens. Ainsi, le mod√®le qui fournit la meilleure pr√©diction est l'√©v√©nement qui arrive le plus souvent.

Pour calculer l'am√©lioration du mod√®le on utilise donc plut√¥t la m√©thode du _log-likelihood_.

La perte logarithmique pour le point $j$ est :

- $\ln p(x_j)$ si $y_j = 1$
- $\ln (1 - p(x_j))$ si $y_j = 0$

:::correction
log loss == "surprise" de $y_j$ sachant $x_j$ ($=0$ si pr√©diction parfaite, $\rightarrow \infty$ si pr√©diction fausse et $p_j \rightarrow \{0,1\}$).

$P(y_j=1|x_j) = p(x_j)$ et $P(y_j=0|x_j) = 1-p(x_j)$ (Bernoulli) d'o√π  :
$-y_j\ln p_j - (1 - y_j)\ln (1 - p_j)$
:::

On calcule alors l'am√©lioration du mod√®le √† partir de la probabilit√© -log likelihood (_-LL_) qui est la somme de ces pertes.

On obtient alors le r√©sultat suivant, qui illustre la diff√©rence au carr√© entre le mod√®le de base (la constante ou l'√©v√©nement qui arrive le plus souvent) et le mod√®le avec un ou plusieurs pr√©dicteurs :

$$\chi_{k-1}^2 = 2(LL_{mod√®le} - LL_{base})$$

avec $\chi_{k-1}^2$ une distribution du $\chi^2$ √† $k-1$ degr√©s de libert√©.

La statistique $R$ peut √™tre d√©duite de ce r√©sultat et d'une statistique de [Wald](https://fr.wikipedia.org/wiki/Test_de_Wald) mais est instable pour de grands √©chantillons.

On utilise donc plut√¥t des peudo $R^2$ calcul√©s diff√©remment et qui approchent cette valeur, comme :

$$R_{logit}^2 = 1 - \frac{-2LL_{mod√®le}}{-2LL_{base}}$$

#### Conditions d'utilisation

- Inclure les variables pertinentes : toutes les variables pertinentes doivent √™tre comprises dans le mod√®le et celles qui ne le sont pas, √©limin√©es.
- Ind√©pendance des observations : un individu ne peut pas faire partie des deux groupes de la variable pr√©dite.
- Pas de relation lin√©aire parfaite ou tr√®s √©lev√©e entre deux ou plusieurs pr√©dicteurs. Par cons√©quent, les corr√©lations ne doivent pas √™tre trop fortes entre ceux-ci.
- Pas de valeurs extr√™mes des r√©siduels : influencent les coefficients du mod√®le et limitent la qualit√© de l'ajustement.
- Taille de l'√©chantillon : l'√©chantillon doit √™tre suffisant pour que l'on puisse proc√©der √† l'analyse. On sugg√®re a minima 10 observations par variable ind√©pendante (_Hosmer et Lemeshow, 1989_, voir √©galement _Cohen, 1992_).

#### Qualit√©s

:::tip
- Interpr√©table : NON
- Critique : OUI (tr√®s utilis√© pour le scoring)
- Consistance : NON (sauf si le mod√®le est exact)
- Minimax : NON
- Sans param√®tre : OUI
- Vitesse : OUI
- En ligne : possible
:::

:::link
Voir la [page Wikipedia de la R√©gression logistique][wiki-regression-logistique].
:::

:::correction
Reg logistique : pas de valeurs extr√™mes des r√©siduels : comme dans la r√©gression multiple, des valeurs r√©siduelles standardis√©es plus √©lev√©es que 2,58 ou moins √©lev√©es que -2,58 influencent les coefficients du mod√®le et limitent la qualit√© de l'ajustement.
:::

### R√©gression p√©nalis√©e (_lasso_)

#### Mod√®le lin√©aire

Le lasso fonctionne comme une r√©gression lin√©aire standard mais ajoute une contrainte de p√©nalisation sur la norme $l_1$ des coefficients $\beta$ de la r√©gression :

$$\sum_{j=1}^{p} |\beta_j| \leq t$$.

On cherche alors les coefficients $\beta$ qui satisfont :

$$\min_{\beta_0,\beta_1,\dots,\beta_p} \frac{1}{2}\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{i,j} \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j|$$

- Le param√®tre $t$ contr√¥le le niveau de r√©gularisation des $\beta$ estim√©s.
- $\lambda \geq 0$ est appel√© param√®tre de r√©gularisation.
- $\lambda$ est reli√© √† $t$ par une relation qui d√©pend des donn√©es.
- Si $p > n$ (nombre de variables sup√©rieures au nombre d'individus) le lasso s√©lectionne au plus $n$ variables.

#### G√©n√©ralisation

Le lasso n'est pas uniquement restreint √† la r√©gression lin√©aire, il peut √™tre √©galement utilis√© avec les mod√®les de r√©gression g√©n√©ralis√©e permettant notamment de faire de la r√©gression logistique p√©nalis√©e.

Sa g√©n√©ralisation s'√©crit :

$$\min_{\beta\in\mathbb{R}^p}\frac{1}{n}\sum\limits_{i=1}^n f_\beta(X_{i,.},y_i) +\lambda\|\beta\|_1$$

Avec $f_\beta$ une [fonction objectif](https://fr.wikipedia.org/wiki/Fonction_objectif).

Par exemple, pour une r√©gression logistique, on a :

$$f_\beta(X_{i,.},y_i)=\frac{1}{N} \sum_{i=1}^N y_i (\beta_0 + X_{i,.}^T \beta) - \log (1+e^{(\beta_0+X_{i,.}^T \beta)})$$

#### üåü Avantages et limites ‚ùå

Les principaux avantages du lasso sont :

+ Grande dimension : fonctionne dans les cas o√π le nombre d'individus est inf√©rieur au nombre de variables ($n < p$) si toutefois un faible nombre de ces variables a une influence sur les observations (hypoth√®se de parcimonie).
  * N'est pas vraie dans le cas de la r√©gression lin√©aire classique avec un risque associ√© qui augmente comme la dimension de l'espace des variables m√™me si l'hypoth√®se de parcimonie est v√©rifi√©e.
+ Algorithmes peu co√ªteux en temps de calcul et de stockage
+ S√©lection parcimonieuse : permet de s√©lectionner un sous-ensemble restreint de variables (d√©pendant du param√®tre $\lambda$).
  * Cette s√©lection restreinte permet souvent de mieux interpr√©ter un mod√®le (rasoir d'Ockham).
+ Consistance de la s√©lection : lorsque le vrai vecteur solution $\beta$ est creux $(\|\beta \|_{0}=K<p)$, c'est-√†-dire que seul un sous-ensemble de variables est utilis√© pour la pr√©diction, sous de bonnes conditions, le lasso sera en mesure de s√©lectionner ces variables d'int√©r√™ts avant toutes autres variables.

Ses principaux inconv√©nients sont :

- Fortes corr√©lations : si des variables sont fortement corr√©l√©es entre elles et qu'elles sont importantes pour la pr√©diction, le lasso en privil√©giera une au d√©triment des autres.
  * Un autre cas, o√π les corr√©lations posent probl√®me, est quand les variables d'int√©r√™ts sont corr√©l√©es avec d'autres variables. Dans ce cas, la consistance de la s√©lection du lasso n'est plus assur√©e.
- La tr√®s grande dimension : lorsque notamment la dimension est trop √©lev√©e ($p >> n$) ou si le vrai vecteur $\beta$ n'est pas suffisamment creux (trop de variables d'int√©r√™ts), le lasso ne pourra pas retrouver l'ensemble de ces variables d'int√©r√™ts.

:::link
Pour une vision math√©matique du lasso, voir [ce cours][slides-lasso].
:::

### R√©gression typologique ou clusterwise

L'id√©e de base est de chercher √† cr√©er un ensemble de mod√®les locaux qui serait plus performant qu'un mod√®le global. Au lieu de chercher un mod√®le global unique permettant de d√©crire l'ensemble des observations, les m√©thodes par cluster travaillent en parall√®le sur $k$ mod√®les locaux optimis√©s pour une partition en $k$ clusters.

![exemple Hennig 2000](@assets/data/reg-clusterwise-exemple-hennig-2000.png)

<div class="caption">Figure adapt√©e de (Hennig, 2000) : (a) donn√©es initiale, (b) droite de r√©gression avec un seule mod√®le, (c) droites de r√©gression avec deux mod√®les. Une unique r√©gression pr√©dira toujours la valeur 0 alors que la d√©tection de deux sous-ensembles dans les donn√©es permettra de trouver deux droites de r√©gressions.</div>

Utilise deux approches principales :

- La m√©thode des moindres carr√©s (d√©riv√©e des [k-moyennes][wiki-k-means]) : m√©thode g√©om√©trique (analyse des donn√©es);
- Un m√©lange de mod√®les utilisant le [maximum de vraisemblance][wiki-max-vraisemblance] : m√©langes finis (statistique math√©matique).

En g√©n√©ral, on utilise la m√©thode des moindres carr√©s pour minimiser les r√©sidus :

:::strong
$$\sum_{i=1}^n\sum_{k=1}^K1_k(i)(y_i - (\alpha_k + \beta_k x_i))^2$$

Avec $1_k(i)$ la fonction indicatrice du cluster $k$ : vaut 1 ssi $i\in k$ sinon 0.
:::

#### Algorithme :

- √Ä partir d'une partition initiale, on estime s√©par√©ment $k$ mod√®les de r√©gression.
‚Äì Chaque observation est affect√©e au cluster (ou mod√®le) donnant le plus petit r√©sidu carr√©, c'est-√†-dire la meilleure pr√©diction. Une fois toutes les observations reclass√©es, on a une nouvelle partition.
‚Äì It√©ration
- Si un cluster a un effectif trop faible : r√©gression sur composantes principales du cluster.

## M√©thodes de classification

### M√©thode des k proches voisins (_k-NN_)

On dispose d'une base de donn√©es d'apprentissage constitu√©e de $m$ couples _entr√©e-sortie_.

Pour estimer la sortie associ√©e √† une nouvelle entr√©e $x$, on prend en compte les $k$ √©chantillons d'apprentissage dont l'entr√©e est la plus proche de la nouvelle entr√©e $x$, selon une distance √† d√©finir.

- En classification k-NN, on retiendra la classe la plus repr√©sent√©e parmi les $k$ sorties associ√©es aux $k$ entr√©es les plus proches de la nouvelle entr√©e $x$.
- En r√©gression k-NN, le r√©sultat est la moyenne des valeurs des $k$ plus proches voisins.

```mermaid
---
title: Classification pour k=3 plus proches voisins
---
flowchart TD
  subgraph Classe1
    x1["x1"]
    x4["x4"]
    x5["x5"]
    x7["x7"]
    class x1,x4,x5,x7 blue;
  end

  subgraph ClasseMajoritaire["Classe majoritaire"]
    x8["x8"]
    
    subgraph Classe2["Classe 2"]
      x2["x2"]
      x3["x3"]
      x6["x6"]
      class x2,x3,x6 green;
    end
  end

  %% Arcs depuis x8
  x8 -.-> x1
  x8 -.-> x4
  x8 -.-> x5
  x8 -.-> x6

  x8 ==> x7
  x8 ==> x2
  x8 ==> x3

  class x8 green;
```


#### Algorithme de classification

- Param√®tre : le nombre $k$ de voisins.
  + Souvent le nombre d'attributs +1.
- Donn√©e : un √©chantillon de $m$ exemples et leurs classes.
- La classe d'un exemple $X$ est $c(X)$.
- Entr√©e : un enregistrement $Y$.
- D√©terminer les $k$ plus proches exemples de $Y$ en calculant les distances.
  + Le choix de la distance est primordial au bon fonctionnement de la m√©thode.
  + D√©pend du type des donn√©es et des connaissances pr√©alables du probl√®me.
  + Par ex: pond√©rer la distance euclidienne pour √©viter qu'un attribut domine le calcul.
- Combiner les classes de ces $k$ exemples en une classe $C$.
- Sortie : la classe de $Y$ est $c(Y)=C$.

#### Qualit√©s

:::tip
- Interpr√©table : OUI et NON
- Critique : OUI mais pas tr√®s fiable
- Consistance : NON mais possible si $k = log(n)$ (par exemple)
- Minimax : NON
- Sans param√®tre : NON
- Vitesse : OUI et NON, impl√©mentation possible en $\mathcal O(n log n)$
- En ligne : OUI
:::

:::link
Voir aussi : [l'article Wikipedia sur la m√©thode des k plus proches voisins][wiki-k-neighbors] et [l'article Wikipedia sur la recherche de proches voisins][wiki-closest-neighbors].
:::

#### Exercice

:::exo
On dispose d'une base de donn√©es d'apprentissage constitu√©e de 5 couples entr√©e-sortie :

- (_Dupont_, Admis)
- (_Fernand_, Admis)
- (_David_, Ajourn√©)
- (_Dumont_, Ajourn√©)
- (_Billaut_, Admis)

Pour chaque √©tudiant on dispose aussi de 4 notes dans 4 mati√®res diff√©rentes :

- Dupont : $14, 12, 8, 12$
- Fernand : $12, 12, 6, 10$
- David : $8, 9, 9, 1$
- Dumont : $15, 11, 3, 5$
- Billaut : $12, 9, 14, 11$

On dispose maintenant d'une nouvelle entr√©e _Verneuil_ qui a pour notes : $9, 14, 15, 6$.

En utilisant la m√©thode des $k$ plus proches voisins ($k =3$) et en choisissant la distance suivante : $d(X,Y) = \sqrt{\sum_{i=1}^4{|X_i - Y_i|}}$.

D√©terminez la classe de _Verneuil_.
:::

:::correction

  $d(Verneuil,Dupont)^2 = |9-14| + |14-12| + |15-8| + |6-12| = 20$

  $d(Verneuil,Fernand)^2 = |9-12| + |14-12| + |15-6| + |6-10| = 18$

  $d(Verneuil,David)^2 = |9-8| + |14-9| + |15-9| + |6-1| = 17$

  $d(Verneuil,Dumont)^2 = |9-15| + |14-11| + |15-3| + |6-5| = 22$

  $d(Verneuil,Billaut)^2 = |9-12| + |14-9| + |15-14| + |6-11| = 14$

  _Verneuil_ est le plus proche de : (_Fernand_, Admis), (_David_, Ajoutn√©), (_Billaut_, Admis) => _Verneuil_ est donc dans la classe _Admis_.

:::

### Arbres de d√©cision (_CART_)

- Type d'analyse discriminante non param√©trique.
- But : expliquer une variable r√©ponse (qualitative ou quantitative) √† l'aide d'autres variables.
- Principe : construire un arbre √† l'aide de divisions successives des individus d'un ensemble $E$ en deux segments (appel√©s aussi n≈ìuds) homog√®nes par rapport √† une variable $Y$ en utilisant l'information de $p$ variables $X_1, ..., X_p$.

L'arbre obtenu comporte √† la racine l'√©chantillon total $E$ √† segmenter et les autres segments sont :

- soit des segments interm√©diaires (encore divisibles);
- soit des segments terminaux.

L'ensemble des segments terminaux constitue une partition de l'ensemble $E$ en classes homog√®nes et distinctes, relativement √† la variable $Y$.

Il existe deux principaux types d'arbre de d√©cision en fouille de donn√©es :

- Les _arbres de classification_ (_Classification Tree_) permettent de pr√©dire √† quelle classe la variable cible $Y$ qualitative appartient, dans ce cas la pr√©diction est une √©tiquette de classe,
- Les _arbres de r√©gression_ (_Regression Tree_) permettent de pr√©dire une quantit√© r√©elle $Y$ quantitative (par exemple, le prix d'une maison ou la dur√©e de s√©jour d'un patient dans un h√¥pital), dans ce cas la pr√©diction est une valeur num√©rique.

#### üåü Avantages / Inconv√©nients

:::tip
+ Solutions sous formes graphiques simples √† interpr√©ter.
+ Peu de pr√©paration des donn√©es.
+ Se combinent bien √† d'autres techniques d'apprentissage.
+ Tr√®s calculatoire et efficace √† condition d'avoir de grandes tailles d'√©chantillon.
+ Capable de g√©rer √† la fois les variables quantitatives et qualitatives simultan√©ment.
+ Peu d'hypoth√®ses requises !
- Algorithme bas√© sur une strat√©gie pas √† pas hi√©rarchis√©e, peut passer √† c√¥t√© d'un optimum global.
- Peut construire des arbres tr√®s complexes (sur-apprentissage) qui g√©n√©ralisent mal l'ensemble √©tudi√© : besoin d'√©lagage..
:::

![Exemple de sur-apprentissage](@assets/data/surajustement_modele_2.jpg)

<div class="caption">Un exemple de sur-apprentissage. Source et cr√©dits https://commons.wikimedia.org/wiki/File:Surajustement_Mod%C3%A8le_2.JPG</div>

#### M√©thode _CART Classification And Regression Tree_ : construction d'un arbre binaire

Soient $p$ variables quantitatives ou qualitatives explicatives $X_1, ..., X_p$ et une variable √† expliquer $Y$ observ√©e sur un √©chantillon de $n$ individus.

$Y$ est :

- soit qualitative √† $m$ modalit√©s ${\tau_l, l=1, ... ,m}$.
- soit quantitative r√©elle.

La construction d'un arbre de discrimination binaire consiste √† d√©terminer une s√©quence de n≈ìuds d√©finis chacun par :

- une variable parmi les explicatives ;
- une division qui induit une partition en deux classes.

Une division est elle-m√™me d√©finie par :

- une valeur seuil de la variable (quantitative) s√©lectionn√©e.
- ou un partage en deux groupes des modalit√©s (variable qualitative).

√Ä la racine correspond l'ensemble de l'√©chantillon. La proc√©dure est ensuite it√©r√©e sur chacun des sous-ensembles.

Cet algorithme n√©cessite :

- La d√©finition d'un crit√®re permettant de s√©lectionner la "meilleure" division pour les diff√©rentes variables ;
- Une r√®gle permettant de d√©cider qu'un noeud est terminal (feuille) ;
- L'affectation de chaque feuille √† l'une des classes (par exemple : classe majoritaire dans la feuille) ou √† une valeur de la variable √† expliquer.

#### Crit√®re de division

Une division est dite _admissible_ si aucun des segments descendants n'est vide.

- Si la variable explicative est qualitative ordinale √† $m$ modalit√©s, elle conduit √† $m - 1$ divisions binaires admissibles.
- Si elle est nominale, le nombre de divisions devient √©gal √† $2^{m-1} -1$.
- Pour une variable quantitative √† $m$ valeurs distinctes, on se ram√®ne au cas ordinal.

Objectif : Partager les individus en deux groupes les plus homog√®nes au sens de la variable √† expliquer.

Le crit√®re de division repose sur la d√©finition d'une _fonction d'h√©t√©rog√©n√©it√©_ ou de _d√©sordre_ $D$ d'un noeud :

- $D$ est nulle si et seulement si le segment est homog√®ne : tous les individus appartiennent √† la m√™me modalit√© ou prennent la m√™me valeur de $Y$ ;
- $D$ est maximale lorsque les valeurs de $Y$ sont √©quiprobables ou tr√®s dispers√©es.

La division du noeud $k$ cr√©e deux fils not√©s $(k+1)$ et $(k+2)$.

Parmi toutes les divisions admissibles du noeud $k$, on garde celle qui rend la somme $D_{(k+1)} + D_{(k+2)}$ des d√©sordres des n≈ìuds fils minimale, c'est-√†-dire :

$$\max_{divisions de X_j ; j=1,...,p}{D_k - D_{(k+1)} - D_{(k+2)}}$$

#### R√®gle d'arr√™t

La croissance de l'arbre s'arr√™te √† un noeud qui devient donc feuille :

- Lorsqu'il est homog√®ne c'est-√†-dire lorsqu'il n'existe plus de division admissible ;
- Si le nombre d'observations qu'il contient est inf√©rieur √† un seuil fix√© par l'utilisateur $d_{min}$. En g√©n√©ral $1\leq d_{min}\leq 5$ ;
- Si le nombre de n≈ìuds est sup√©rieur √† $n_{max}$ nombre fix√© par l'utilisateur.

#### Affectation

Une fois les crit√®res d'arr√™t atteints, il faut affecter une valeur √† chaque feuille :

- Si $Y$ est quantitative, attribution de la valeur moyenne aux observations de cette feuille ;
- Si $Y$ est qualitative, chaque feuille est affect√©e √† une modalit√© $\tau_l$ de $Y$ en consid√©rant le mode conditionnel :
  + celle ayant la proportion la plus √©lev√©e √† l'int√©rieur de cette feuille. Il est alors facile de comparer le nombre de donn√©es mal class√©es.
  + la modalit√© la moins co√ªteuse si des co√ªts de mauvais classements sont donn√©s.
  + la classe a posteriori la plus probable au sens bay√©sien si des probabilit√©s a priori sont connues.

#### Crit√®re d'homog√©n√©it√©

##### Cas quantitatif

- Soit une partition de $n$ individus en deux sous populations $E_1$ et $E_2$ de tailles respectives $n_1$ et $n_2$.
- Soit $\mu_{ij}$ la valeur "th√©orique" de $Y$ pour l'individu $i$ du sous-ensemble $E_j$.
- Soit $\mu_{.j} = \frac{1}{n_j}\sum_{i=1}^{n_j}\mu_{ij}$
- Soit $\mu_{..} = \frac{1}{n}\sum_{i,j}\mu_{ij}$

L'h√©t√©rog√©n√©it√© du sous-ensemble $E_j$ est mesur√©e par :

$$D_j = \sum_{i=1}^{n_j}(\mu_{ij}-\mu_{.j})^2$$

Alors l'h√©t√©rog√©n√©it√© de la partition est d√©finie par :

$$D = D_1 + D_2 = \sum_{j=1}^2\sum_{i=1}^{n_j}(\mu_{ij}-\mu_{.j})^2$$

C'est l'inertie intra-groupe qui vaut 0 si et seulement si $\forall i, j, \mu_{ij} = \mu_{.j}$

La diff√©rence entre l'h√©t√©rog√©n√©it√© de l'ensemble non partag√© et celle de la partition est :

$$
\begin{aligned}
\Delta = \sum_{j=1}^2\sum_{i=1}^{n_j}(\mu_{ij}-\mu_{..})^2 - \sum_{j=1}^2\sum_{i=1}^{n_j}(\mu_{ij}-\mu_{.j})^2 \\
    = \sum_{j=1}^2 n_j(\mu_{..}-\mu_{.j})^2 \\
    = \frac{n_1 n_2}{n}(\mu_{.1}-\mu_{.2})^2
\end{aligned}
$$

$\Delta$ correspond au "d√©sordre" des barycentres et est homog√®ne √† la variance intergroupe.

Objectif : √Ä chaque √©tape, maximiser $\Delta$ c-√†-d trouver la _variable explicative_ induisant une partition en deux sous-ensembles associ√©e √† :

- une inertie intra-groupe minimale ;
- ou encore qui rende l'inertie intergroupe maximale (avoir des sous-ensembles dont les valeurs de la variable cible soient les plus dispers√©es possibles).

Estimation des quantit√©s :

$D_j$ estim√© par $\hat{D_j} = \sum_{i=1}^{n_j}(y_{ij}-y_{.j})^2$ et $D$ par $\hat{D}= \sum_{j=1}^2\sum_{i=1}^{n_j}(y_{ij}-y_{.j})^2$

##### Cas qualitatif

Soit $Y$ une variable √† expliquer √† $m$ modalit√©s $\tau_l$. L'arbre induit une partition pour laquelle $n_{+k}$ d√©signe l'effectif du $k^{eme}$ noeud.

Soit $p_{lk} = \mathbb{P}[\tau_l | k]$ avec $\sum_{l=1}^m p_{lk} = 1$ la probabilit√© qu'un √©l√©ment du $k^{eme}$ noeud appartienne √† la $l^{eme}$ classe.

Le d√©sordre du $k^{eme}$ noeud, d√©fini √† partir de l'entropie de Shannon, s'√©crit :

$$D_k = -2\sum_{l=1}^m n_{+k}p_{lk}log(p_{lk})$$

L'entropie permet de mesurer le d√©sordre dans un ensemble de donn√©es et est utilis√©e pour choisir la valeur permettant de maximiser le gain d'information.

L'h√©t√©rog√©n√©it√© de la partition est d√©finie par :

$$D = D_1 + D_2 = -2\sum_{j=1}^2\sum_{l=1}^{m}n_{+k}p_{lk}log(p_{lk})$$

Cette quantit√© est positive et nulle si et seulement si les probabilit√©s $p_{lk}$ ne prennent que des valeurs 0 sauf une √©gale √† 1 correspondant √† l'absence de m√©lange.

Soit $n_{lk}$ l'effectif observ√© de la $l^{eme}$ classe dans le $k^{eme}$ noeud : $n_{+k} = \sum_{l=1}^m n_{lk}$.

Estimation des quantit√©s :

$$\hat{D_k} = -2\sum_{l=1}^m n_{+k}\frac{n_{lk}}{n_{+k}}log\left(\frac{n_{lk}}{n_{+k}}\right)$$

$$\hat{D} = -2\sum_{k=1}^2\sum_{l=1}^m n_{+k}\frac{n_{lk}}{n_{+k}}log\left(\frac{n_{lk}}{n_{+k}}\right)$$

##### Crit√®re de Gini

Le _crit√®re de Gini_ du noeud $k$ est d√©fini par $D_k = \sum_{l\neq h}p_{lk}p_{hk}$ et estim√© par $\hat{D_k} = \sum_{l\neq h}\frac{n_{lk}}{n_{+k}}\frac{n_{hk}}{n_{+k}}$.

- Il mesure avec quelle fr√©quence un √©l√©ment al√©atoire de l'ensemble serait mal class√© si son √©tiquette √©tait choisie al√©atoirement selon la distribution des √©tiquettes dans le sous-ensemble.
- Le d√©sordre $D_k$ est maximal si $p_{lk} = \frac{1}{m}$ : l'√©chantillon pr√©sente autant d'√©l√©ments de chaque modalit√©.
- $D_k$ est nul si l'√©chantillon est pur : $p_{lk} = 1$ et $p_{hk} = 0$ si $h\neq l$.
- $D_k$ repr√©sente la probabilit√© de mauvais classement pour un individu tir√© au hasard parmi les individus du noeud $k$.

Le d√©sordre de l'√©chantillon initial de taille $n$ est estim√© par $\hat{D} = \sum_{l\neq h}\frac{n_l}{n}\frac{n_h}{n}$ o√π $n_l$ repr√©sente l'effectif observ√© de la $l^{eme}$ modalit√© dans l'√©chantillon initial.

La r√©duction d'impuret√© correspond √† une division binaire est alors estim√©e par :

$$\hat{\Delta} = \hat{D} - \frac{n_+1}{n}\hat{D_1} - \frac{n_+2}{n}\hat{D_2}$$

#### √âlagage

Objectif : Rechercher le meilleur compromis entre :

- un arbre tr√®s d√©taill√©, fortement d√©pendant des observations qui ont permis son estimation, qui fournira un mod√®le de pr√©vision tr√®s instable ;
- un arbre trop robuste mais grossier qui donne des pr√©dictions trop approximatives.

Principe :

- Construire une suite embo√Æt√©e de sous-arbres de l'arbre maximum par √©lagage successif.
- Choisir, parmi cette suite, l'arbre optimal au sens d'un crit√®re.

La solution obtenue par algorithme pas √† pas n'est pas n√©cessairement globalement optimale mais cette m√©thode est efficace et fiable.

#### Construction de la s√©quence d'arbres

Pour un arbre $A$ donn√©, on note $K$ le nombre de feuilles ou n≈ìuds terminaux de $A$; la valeur de $K$ exprime la complexit√© de $A$.

La qualit√© de discrimination d'un arbre $A$ se mesure par le crit√®re $D(A) = \sum_{k=1}^K D_k(A)$ o√π $D_k(A)$ est le nombre de mal class√©s ou la d√©viance ou le co√ªt de mauvais classement de la $k^{eme}$ feuille de l'arbre $A$.

La construction de la s√©quence d'arbres embo√Æt√©s repose sur une p√©nalisation de la complexit√© de l'arbre : $C(A) = D(A) + \gamma K$.

- Pour $\gamma = 0$, $A_{max} = A_K$ minimise $C(A)$.
- En faisant cro√Ætre $\gamma$, l'une des divisions de $A_K$ - celle pour laquelle l'am√©lioration de $D$ est la plus faible (inf√©rieure √† $\gamma$) - appara√Æt comme superflue et les deux feuilles sont regroup√©es (√©lagu√©es) dans le noeud p√®re qui devient terminal ; $A_{K-1} \subset A_K$.

Soit $\mathcal N$ un noeud. On appelle $A_{\mathcal N}$ le sous-arbre (ou la branche) de $A$ extrait(e) √† partir de $\mathcal N$ , donc constitu√© des descendants de $\mathcal N$ et de la racine. On appelle $A'$ le sous-arbre de $A$ auquel on a enlev√© la branche $A_\mathcal{N}$ .

On a alors :

$$C(A') = C(A) + C(\mathcal N) - C(A_\mathcal{N})$$

Par cons√©quent :

$$C(A') \geq C(A) \iff \gamma \leq \frac{D(\mathcal N) - D(A_{\mathcal N}}{|A_{\mathcal N}| -1} = \alpha$$

Ceci signifie que si la valeur de $\gamma$ fix√©e est inf√©rieure √† $\alpha$ , le co√ªt du sous-arbre √©lagu√© $A'$ est sup√©rieur √† celui de $A$ : on gardera donc l'arbre complet $A$.

Le proc√©d√© est it√©r√© pour la construction de la s√©quence embo√Æt√©e $A_{max} = A_K \supset A_{K-1} \supset ... \supset A_1$ o√π A_1 (la racine) regroupe l'ensemble de l'√©chantillon.

Un graphe repr√©sente la _d√©croissance_ ou l'_√©boulis_ de la d√©viance (ou du taux de mal class√©) :

- en fonction du nombre croissant de feuilles dans l'arbre ;
- ou en fonction de la valeur d√©croissante du coefficient de p√©nalisation $\gamma$ (graphes √©quivalents).

_√âlagage lorsque l'augmentation de la complexit√© de l'arbre n'est plus compens√©e par la diminution de la d√©viance_.

#### Recherche de l'arbre optimal

Les proc√©dures d'√©lagage diff√®rent par la fa√ßon d'estimer l'erreur de pr√©diction. Quand l'am√©lioration du crit√®re est jug√©e trop petite ou n√©gligeable, on √©lague l'arbre au nombre de feuilles obtenues.

- L'√©valuation de la d√©viance ou du taux de mauvais classement estim√©e par resubstitution sur l'√©chantillon d'apprentissage est biais√©e (trop optimiste).
- Une estimation sans biais est obtenue par l'utilisation d'un autre √©chantillon (validation) ou encore par validation crois√©e.

La proc√©dure de validation crois√©e a une particularit√© : la s√©quence d'arbres obtenue est diff√©rente pour chaque estimation sur l'un des sous-√©chantillons :
- L'erreur moyenne n'est pas calcul√©e pour chaque sous-arbre avec un nombre de feuilles donn√© mais pour chaque sous-arbre correspondant √† une valeur fix√©e du coefficient de p√©nalisation $\gamma$.
- √Ä la valeur de $\gamma$ minimisant l'estimation de l'erreur de pr√©vision, correspond ensuite l'arbre jug√© optimal dans la s√©quence estim√©e sur tout l'√©chantillon d'apprentissage.

Le principe de s√©lection d'un arbre optimal est donc d√©crit par l'algorithme suivant :

- Construction de l'arbre maximal $A_{max}$ .
- Construction de la s√©quence $A_K , ... , A_1$ d'arbres embo√Æt√©s.
- Estimation sans biais (√©chantillon de validation ou validation crois√©e) des d√©viances $D(A_K), ... , D(A_1)$.
- Repr√©sentation de $D(A_k)$ en fonction de $k$ ou de $\gamma$.
- Choix de $k$ rendant $D(A_k)$ minimum.

#### Qualit√©s du classifieur CART

:::tip
- Interpr√©table : OUI !
- Consistance : OUI (sous certaines r√©serves) MAIS instable !
- Minimax : NON !
- Sans param√®tre : NON
- Vitesse : OUI
- En ligne : NON
:::

:::link
Voir la [page Wikipedia sur les arbres de d√©cision][wiki-arbres-decision].
:::

# Apprentissage non supervis√©

A partir de l'√©chantillon d'apprentissage $D_n = (x_1, ..., x_n)$ non √©tiquet√©, on cherche des r√©gularit√©s sous-jacentes pour partitionner l'ensemble :

- Sous forme d'une fonction.
- Sous forme d'un mod√®le complexe.

Afin de r√©sumer, d√©tecter des r√©gularit√©s, comprendre...

- Grand concept du machine learning : permet √† un ordinateur d'en apprendre lui-m√™me sur les donn√©es que l'humain lui fournit.
- Rep√®rent des similarit√©s dans les donn√©es pour pouvoir ensuite les structurer.
- Exemple : similarit√©s entre individus pour les partitionner en diff√©rents groupes (clustering).

```mermaid
---
title: Sch√©ma d'apprentissage non supervis√©
---
stateDiagram-v2
  [*] --> Network : entr√©es
  state Network {
    algorithme
  }
  algorithme --> sortie
  sortie --> algorithme

```

## Classification automatique (_Clustering_)

- M√©thode math√©matique d'analyse de donn√©es pour faciliter l'√©tude d'une population d'effectif important (animaux, plantes, malades, g√®nes, etc...).
- Regroupement en classes : 
  + d'individus le plus semblables possible.
  + classes le plus distinctes possibles.
- Diff√©rentes m√©thodes et algorithmes pour diff√©rentes classifications, dans le cours :
  + Classification hi√©rarchique ascendante.
  + M√©thode des centres mobiles (k-moyennes).
  + M√©thodes √† densit√© (DBSCAN).

Tr√®s utile notamment pour parcourir une base de donn√©es dans le but d'en extraire des clusters.

:::link
Autre exemple d'utilisation : [Approche math√©matique pour le traitement et le clustering de donn√©es sonores][clustering-donnees-sonores].
:::

### Distance euclidienne entre individus d'une m√™me population

Pour regrouper les individus qui se ressemblent et s√©parer ceux qui ne se ressemblent pas, il faut un "crit√®re de ressemblance".

Pour cela on examine l'ensemble des informations dont on dispose concernant les individus (pression art√©rielle, temp√©rature, ...
Par exemple s'il s'agit de malades) not√©es $(x_i, y_i, ...)$ pour le $i^{eme}$ individu, et on imagine que chaque individu est un point $M_i = (x_i, y_i, z_i, ...)$ de l'espace.

S'il n'y a que deux variables relev√©es $(x_i, y_i)$ on obtient ainsi un nuage $\Gamma$ de points dans le plan, $\Gamma = {M_i, i = 1, ... , n}$ o√π $n$ est l'effectif total de la population.

:::strong
La distance euclidienne de deux individus $M_i$ et $M_j$ est par d√©finition $d_2(M_i, M_j) = \sqrt{(x_i - x_j )^2 + (y_i - y_j )^2}$.
:::

Elle est d'autant plus petite que les deux individus sont semblables (du point de vue des valeurs des deux crit√®res retenus) et d'autant plus grande qu'ils sont diff√©rents.

On peut associer √† chaque nuage d'individus une matrice $\mathbb{D} = (d_{ij})_{0\leq i\leq n,0\leq j\leq n} = (d_2(M_i, M_j ))$, dite matrice des distances.

C'est une matrice √† $n$ lignes et $n$ colonnes, √† coefficients positifs, sym√©trique puisque $d_2(M_i,M_j) = d_2(M_j,M_i)$ et nulle sur la diagonale puisque $d_2(M_i,M_i) = 0$.

Pour un nuage d'effectif $n$, il y a donc $\frac{n(n-1)}{2}$ distances √† calculer.

√Ä c√¥t√© de la distance euclidienne, on peut d√©finir d'autres distances (et donc d'autres matrices des distances).

Par exemple :

$$d_1(M_i, M_j) = |xi - xj | + |yi - yj |$$
$$d_\infty(M_i, M_j ) = Max{|xi - xj |, |yi - yj |}$$

### √âcarts entre classes

Supposons le nuage $\Gamma = {M_i, i = 1, ... , n}$ d√©compos√© en plusieurs classes $\Gamma_{1}, \Gamma_{2}, ... , \Gamma_{k}$ et notons $G_1, G_, ... , G_k$ les centres de gravit√© respectifs de chaque classes et $p_1, p_2, ... , p_k$ les poids respectifs de chaque classe que l'on d√©finit de la fa√ßon suivante :

- Le centre de gravit√© $G$ d'un nuage de points $\Gamma$ est le point moyen du nuage, c'est-√†-dire le point $G = (\bar{x}, \bar{y}, ...)$ de coordonn√©es :

:::strong
$$\bar{x} = \frac{1}{n}\sum_{i=1}^n{x_i} , \bar{y} = \frac{1}{n}\sum_{i=1}^n{y_i} , ... $$
:::

- Si l'on suppose que tous les individus ont le m√™me poids √©gal √† $\frac{1}{n}$ , le poids $p_l$ de la classe $\Gamma_l$ est √©gal √† l'effectif de $\Gamma_l$ divis√© par $n$.
  + De cette fa√ßon la somme des poids de toutes les classes vaut $1$.

- Pour mesurer l'√©cart entre deux classes $\Gamma_{l}$ et $\Gamma_{m}$, il existe de nombreuses fa√ßons de proc√©der :
  + Distance du plus proche voisin : $\min_{Mi \in \Gamma_m, Mj \in \Gamma_l}{d(Mi, Mj )  }$.
  + $\max_{Mi \in \Gamma_m, Mj \in \Gamma_l}{d(Mi, Mj )}$.
  + Distance des centres de gravit√© $d_2(G_m, G_l)$.
  + (Le plus souvent) √©cart de Ward :

:::strong
$$ d(\Gamma_m,\Gamma_l) = \frac{p_m p_l}{p_m + p_l}d_2(G_m, G_l)^2 $$

O√π $p_l$ et $p_m$ sont les poids des deux classes.
:::

### Inertie interclasse et inertie intra-classe

On appelle inertie totale I d'un nuage $\Gamma = {M_i, i = 1, ... , n}$ la somme pond√©r√©e des carr√©s des distances de ses points au centre de gravit√© du nuage.

Donc, si $G$ d√©signe le centre de gravit√© de $\Gamma$ et si tous les points du nuage sont de m√™me poids √©gal √† $\frac{1}{n}$ alors l'inertie totale est :

:::strong
$$ I(\Gamma) = \frac{1}{n}\left(d_2(M_1, G)^2 + d_2(M_2, G)^2 + ... + d_2(M_n, G)^2\right)$$
:::

Avec $d_2(M_i, G)$ la distance euclidienne de $M_i$ √† $G$.

Note : Le centre de gravit√© est aussi le point G pour laquelle cette somme pond√©r√©e est minimale.

L'inertie mesure la _dispersion_ du nuage : si le nuage $\Gamma$ est compos√© de $k$ classes $1, 2, ... , k$ disjointes deux √† deux, celles-ci seront d'autant plus homog√®nes que les inerties de chaque classe $I(\Gamma_1), I(\Gamma_2), ... , I(\Gamma_k)$, calcul√©es par rapport √† leurs centres de gravit√© $G_1, G_2, ... , G_k$ respectifs, sont faibles.

La somme de ces inerties est appel√©e _inertie intra-classe_ :

$$I_{intra} = I(\Gamma_1) + I(\Gamma_2) + ... + I(\Gamma_k)$$

Les inerties des classes $I(\Gamma_1), I(\Gamma_2), ...$ sont simplement calcul√©es avec la formule de l'inertie ci-dessus o√π l'on remplace le centre de gravit√© $G$ par celui de la classe $G_1, G_2, ...$ et le poids $\frac{1}{n}$ par celui de la classe.

Attention : _L'inertie totale d'un nuage n'est g√©n√©ralement pas √©gale √† la somme des inerties des classes qui le composent (inertie intra-classe) car il faut prendre en compte √©galement la **dispersion des classes par rapport au centre de gravit√© du nuage**._ 

Cette dispersion est d√©finie par _l'inertie interclasse_ :

$$I_{inter} = p_1 d_2(G_1, G)^2 + p_2 d_2(G_2, G)^2 + ... + p_k d_2(G_k, G)^2$$

O√π $p_j$ d√©signe le poids total de la classe $\Gamma_j$.

On montre le r√©sultat suivant appel√© d√©composition de Huygens :

L'inertie totale d'un nuage de points compos√© de diff√©rentes classes disjointes deux √† deux est la somme de son inertie intra-classe et de son inertie interclasse :

:::strong
$$I(\Gamma) = I(\Gamma_1 \cup \Gamma_2 \cup ... \cup \Gamma_k) = I_{intra} + I_{inter}$$
:::

:::tip
Lorsqu'un nuage est compos√© de plusieurs classes on a vu que, si chacune est tr√®s bien regroup√©e autour de son centre de gravit√©, son inertie intra-classe qui est la somme des inerties de chaque classe sera
petite.

La partition d'un nuage en deux classes sera d'autant meilleure que son inertie intra-classe sera petite, ou que son inertie interclasse sera grande puisque leur somme est l'inertie totale qui reste la m√™me quelle que soit la partition.
:::

## Classification hi√©rarchique ascendante

Pour classifier une population d'effectif $n$ dont les individus sont num√©rot√©s $1, 2, ... , n$ :

- On consid√®re cette population comme la r√©union de $n$ classes √† un seul √©l√©ment
- Puis on regroupe progressivement les classes deux √† deux selon l'algorithme suivant :
  + √âtape 1 : Calculer la matrice des distances $\mathbb{D} = (d(M_i, M_j)_{0\leq i\leq n,0\leq j\leq n}$ ou directement la matrice des distances de Ward des classes r√©duites aux points $\mathbb{W} = \left( \frac{p_i p_j}{p_i + p_j} (d(M_i, M_j)^2 \right)_{1\leq i\leq n,1\leq j\leq n}$.
  + √âtape 2 : Remplacer les deux individus de distance minimale par une classe (√† 2 √©l√©ments) num√©rot√©e $n + 1$, qui sera repr√©sent√©e par le centre de gravit√© des individus et affect√©e de la somme des poids des individus.
  + √âtape 3 : Calculer la perte d'inertie _interclasse_ (ou gain d'inertie _intra-classe_) due au regroupement pr√©c√©dent : il s'agit exactement de l'√©cart de Ward des deux individus regroup√©s.

Apr√®s ces trois √©tapes, la population compte alors $n-1$ classes ($n-2$ classes √† un √©l√©ment et une √† 2 √©l√©ments). On peut donc recommencer √† l'√©tape 1 en rempla√ßant _"individus"_ par _"classes"_ si n√©cessaire (et donc "distance entre individus" par "√©carts entre classes").
Apr√®s $n-1$ it√©rations, tous les individus sont regroup√©s en une classe unique.

On construit alors un arbre, appel√© _dendrogramme_ de la fa√ßon suivante :

- On aligne sur l'axe horizontal des points repr√©sentant les diff√©rents individus et on les joint deux √† deux, successivement, en suivant cet algorithme de classification hi√©rarchique ascendante (commen√ßant par les plus proches, etc...).
- On poursuit ainsi jusqu'√† regroupement de tous les individus en une classe unique.
- Pour plus de lisibilit√©, on pourra disposer les individus dans l'ordre dans lequel les regroupements ont √©t√© effectu√©s.
- Le niveau (hauteur) de chaque noeud de l'arbre est le plus souvent choisi proportionnel √† la nouvelle valeur d'inertie intra-classe ; ce niveau vaut z√©ro lorsque tous les individus sont s√©par√©s (en bas) et est maximal lorsqu'ils sont tous r√©unis en une seule classe (en haut).

En fait, on trace ce dendrogramme afin de visualiser le niveau o√π couper cet arbre pour r√©aliser la meilleure partition de l'ensemble initial.
On peut comprendre qu'il sera optimal de couper le dendrogramme √† un niveau o√π le regroupement entre classes conduit √† une perte d'inertie interclasse maximale.

On peut v√©rifier que l'√©cart de Ward entre deux classes est en fait √©gal √† la perte d'inertie interclasse (ou le gain d'inertie intra-classe) que produirait la r√©union de ces deux classes en une seule. Le niveau des n≈ìuds de l'arbre est donc facile √† calculer √† partir des √©carts de Ward entre les classes.

![Exemple de classification hierarchique](@assets/data/hierarch.gif)

<div class="caption">A gauche, une repr√©sentation des individus. A droite, le dendrogramme associ√© et les √©tapes de sa cr√©ation. Animation : dashee87.github.io</div>

Si on souhaite deux classes, on choisira $(p_0,p_1,p_2)$ et $(p_3,p_4,p_5,p_6)$. Si on souhaite des classes √©loign√©es de plus d'une unit√© de mesure, on choisira $(p_0,p_1,p_2)$, $(p_3)$ et $(p_4,p_5,p_6)$.

### Cas non euclidien : positionnement multidimentionnel (_MDS_)

Il arrive souvent dans les situations concr√®tes que l'on dispose d'une matrice de distances entre les individus √† classifier mais que cette distance ne corresponde pas √† la distance euclidienne entre des points rep√©r√©s par leurs coordonn√©es.

On peut toujours r√©aliser une classification mais l'interpr√©tation donn√©e ci-dessus utilisant le nuage de points, les centres de gravit√©s et l'√©cart de Ward n'a plus de sens.

On peut alors faire pr√©c√©der l'algorithme par une m√©thode appel√©e positionnement multidimensionnel (_MultiDimentional Scaling MDS_) qui permet de trouver, pour n'importe quelle matrice de distances de taille $n^2$ un ensemble de $n$ points rep√©r√©s par leur coordonn√©es euclidiennes dont la matrice de distance est tr√®s proche de la matrice de distances donn√©e, puis calculer l'√©cart de Ward.

## M√©thode des centres mobiles

![M√©thode des centres mobiles](@assets/data/SoftEnragedHypsilophodon.gif)

<div class="caption">Les points rouges sont les centres des classes, actualis√©s √† chaque √©tape. Animation : www.analyticsvidhya.com</div>

On observe bien le ph√©nom√®ne de convergence de ces 10 centres mobiles.

Cette m√©thode s'applique lorsque l'on sait √† l'avance combien de classes on veut obtenir.

La m√©thode centro√Øde la plus classique est la m√©thode des [_k-moyennes_ (_k-means_)][wiki-k-means]. Elle ne n√©cessite qu'un seul choix de d√©part : $k$, le nombre de classes voulues.

Appelons $k$ ce nombre. L'algorithme est le suivant :

- Initialisation : Pour initialiser l'algorithme, on tire au hasard $k$ individus appartenant √† la population : ce sont les $k$ centres initiaux $C_1(0), C_2(0), ... , C_k(0)$.
- √âtape 1 : On regroupe les $n-k$ individus restants autours de ces $k$ centres de sorte √† former $k$ classes $\Gamma_1(0), \Gamma_2(0), ... , \Gamma_k(0)$ de la mani√®re suivante : chaque classe $\Gamma_l(0)$ est constitu√©e des points plus proches du centre $C_l(0)$ que des autres centres $\Gamma_m(0)$ pour $m \neq l$ ([partition de Vorono√Ø](https://fr.wikipedia.org/wiki/Diagramme_de_Vorono%C3%AF)).
- √âtape 2 : On calcule alors les centres de gravit√© $G_1, G_2, ... , G_k$ des $k$ classes obtenues et on d√©signe ces points comme nouveaux centres $C_1(1) = G1, C_2(1) = G_2, ... , C_k(1) = G_k$. On a alors $k$ classes √† $k$ moyennes (centres).

On r√©p√®te les √©tapes 1 et 2 jusqu'√† ce que le d√©coupage en classes obtenu ne soit presque plus modifi√© par une it√©ration suppl√©mentaire (convergence).

![Algorithme des K moyennes](@assets/data/800px-K-means.png)

<div class="caption">Illustration du d√©roulement de l'algorithme des k-moyennes. Source et cr√©dit https://commons.wikimedia.org/wiki/File:K-means.png?uselang=fr</div>

On peut montrer que la variance intra-classe (somme des distances entre les individus d'une m√™me classe) ne peut que d√©cro√Ætre lorsque l'on passe d'un d√©coupage en classes au suivant.

Les classes finales d√©pendent beaucoup des $k$ individus choisis pour l'initialisation : certains algorithmes de _k-means_ it√®rent plusieurs fois le processus avec des initialisations diff√©rentes, dans le but de garder la partition qui minimise le plus la variance intra-classe.

:::link
Les autres algorithmes des m√©thodes centro√Ødes peuvent prendre en compte d'autres repr√©sentants de classes que la moyenne, comme [le m√©do√Øde][wiki-k-medoides], individu le plus central du groupe.
:::

:::link
Exemples de visualisations : [lien][visu-clustering-k-mean].
:::

### Inertie

On appelle _inertie totale_ d'un nuage $\Gamma = {M_i, i = 1, ... , n}$ la somme pond√©r√©e des carr√©s des distances de ses points au centre de gravit√© du nuage.

Donc, si $G$ d√©signe le centre de gravit√© de $\Gamma$ et si tous les
points du nuage sont de m√™me poids √©gal √† $\frac{1}{n}$ , pour rappel l'inertie totale de $\Gamma$ est donn√©e par la formule :

$$\mathcal I(\Gamma) = \frac{1}{n}\sum_{i=1}^n d_2(M_i, G)^2$$

Avec $d_2(M_i, G)$ la distance euclidienne de $M_i$ √† $G$.

On peut montrer que l'inertie intra-classe ne peut que d√©cro√Ætre lorsque l'on passe d'un regroupement en classes $\{\Gamma_1^i, \Gamma_2^i\}$ au suivant $\{\Gamma_1^{i+1}, \Gamma_2^{i+1}\}$ par une it√©ration de l'algorithme des centres mobiles.

Si cette d√©croissance √©tait toujours stricte on serait s√ªr d'atteindre ainsi le minimum.

En pratique, la d√©croissance n'est pas toujours stricte et on n'est donc s√ªr de rien... Mais cet algorithme est populaire car il est facile √† utiliser et il suffit de peu d'it√©rations pour avoir une partition de qualit√©.

:::exo
En utilisant l'algorithme des K-moyennes et la distance de Manhattan $d(X,Y)=\sum_{i=1}^n|X_i - Y_i|$ et A et B comme centres initiaux, g√©n√©rer deux clusters pour les donn√©es suivantes :

$A(2,2) B(3,4) C(7,8) D(9,8) E(5,1) F(8,7)$
:::

:::correction

| Centres | A(2,2) | G(7/2,3/2)   | I(10/3,7/3) |
|         | B(3,4) | H(27/4,27/4) | K(8,23/3)   |
| Points  |        |              |             |
|---------|--------|--------------|-------------|
| A(2,2)  | **A**  | **G**        | **I**       |
| B(3,4)  | __B__  | **G**        | **I**       |
| C(7,8)  | __B__  | __H__        | __K__       |
| D(9,8)  | __B__  | __H__        | __K__       |
| E(5,1)  | **A**  | **G**        | **I**       |
| F(8,7)  | __B__  | __H__        | __K__       |

Les deux clusters sont donc : $\{A;B;E\}$ de centre $I(10/3;7/3)$ et $\{C;D;F\}$ de centre $K(8;23/3)$.
:::

:::correction
Introduction √† la densit√© : exemple √©chec k-mean smiley (distance proche mais classes diff√©rentes).
:::

## Les m√©thodes √† densit√© (_DBSCAN_)

Les classes des m√©thodes √† densit√© correspondent aux zones de densit√© relativement √©lev√©es, c'est-√†-dire les zones o√π beaucoup de points sont proches par rapport √† d'autres zones de l'espace $\mathbb{R}$ en dimension $p$.

:::correction
Montrer graphiquement zones de densit√© sur le smiley.
:::

La m√©thode phare de cette cat√©gorie est appel√©e "density-based spatial clustering of applications with noise" (_DBSCAN_). En plus de former des classes d'individus, l'algorithme rep√®re par la m√™me occasion les valeurs hors du commun que l'on qualifie de bruit. 

Il prend deux param√®tres en entr√©e : $\epsilon$ la distance maximale qui peut d√©finir deux individus comme voisins, et $N$ le nombre minimal d'individus n√©cessaires pour former un groupe. A partir de l√†, l'algorithme est assez intuitif. On aura besoin de stocker deux informations : les cluster successifs et les individus visit√©s au fur et √† mesure.

- La premi√®re √©tape consiste √† choisir un point parmi les $n$ disponibles. Gr√¢ce au param√®tre $\epsilon$, on peut d√©finir le voisinage du point initial, c'est-√†-dire l'ensemble des points que l'on peut qualifier comme ses voisins. 
  + Gr√¢ce au param√®tre $N$, on peut dire que si cet ensemble est constitu√© de moins de $N$ points, alors le point initial correspond √† du bruit. On le stocke alors dans les individus visit√©s.
  + √Ä l'inverse si le voisinage comprend plus de $N$ points, alors on peut initialiser un cluster avec le point de d√©part. On √©tudie chaque point de son voisinage initial.
- Et pour tout point de ce voisinage, si son propre voisinage comporte plus de $N$ √©l√©ments, alors on √©tend le voisinage initial en le r√©unissant avec le voisinage du point visit√©.
- Puis on ajoute ce point dans le cluster. Une fois que tous les points du voisinage ont √©t√© test√©s, ceux retenus dans le cluster sont stock√©s comme individus visit√©s. Le cluster obtenu est stock√© dans la liste des cluster.

Tant que tous les individus n'ont pas √©t√© visit√©s, on r√©it√®re cette √©tape en commen√ßant par choisir un individu parmi ceux qui sont encore disponibles. Et on obtient finalement notre liste de groupes d'individus ainsi que les individus correspondant √† du bruit.

![Algorithme DBSCAN](@assets/data/smiley.gif)

<div class="caption">L'algorithme DBSCAN pas √† pas. Les cercles en mouvement correspondent aux voisinages successifs. Il y a ici 4 zones o√π la densit√© est √©lev√©e. La forme des cluster s'adapte √† la forme des donn√©es propos√©es. Animation : zhuanlan.zhihu.com</div>

:::link
Pour en savoir plus : [article Wikipedia DBSCAN][wiki-dbscan].
:::

:::link
Exemples de visualisations : [lien][visu-clustering-dbscan].
:::

## M√©thodes mixtes

Lorsque le nombre d'individus est tr√®s grands et qu'il est alors difficile de choisir d'avance le nombre de classes, on effectue une classification mixte comme indiqu√© sur la figure suivante :

![Exemple de classification mixte](@assets/data/classification-mixte.jpg)

<div class="caption">Exemple de classification mixte</div>

- Si l'on a des milliers, voir des dizaines de milliers d'individus √† classifier, on commence par les r√©partir en un (trop) grand nombre de classes (par exemple $k = 100$) par la m√©thode des centres mobiles.
- Puis, on ne retient que les centres des classes (avec leur poids qui sera proportionnel au nombre d'individus dans chaque classe) ${(C_1^n,p_1),(C_2^n,p_2), ... , (C_100^n,p_100)}$ et on effectue une classification hi√©rarchique ascendante sur ces centres.
- Une partition est alors obtenue par coupure du dendrogramme que l'on choisit aussi judicieusement que possible (par exemple au plus grand saut) pour avoir le bon nombre de classes.
- On peut alors calculer leurs centres de gravit√© et finalement allouer chaque individu au centre le plus proche, ce qui consolide la partition.

## Conclusion

:::tip
- Au-del√† de ces 3 cat√©gories d'algorithmes, beaucoup d'autres m√©thodes moins intuitives existent comme la maximisation de l'esp√©rance, qui repose sur des outils math√©matiques probabilistes ou des r√©seaux de neurones.
- Chaque m√©thode pr√©sente ses avantages mais aussi ses limites, selon le type de donn√©es √† traiter ‚ùå:
  + Les m√©thodes hi√©rarchiques peuvent √™tre gourmandes en calculs.
  + La m√©thode des k-moyennes donne des groupes √† la forme uniquement convexe.
  + Pour chaque m√©thode les param√®tres d'initialisation ne sont pas forc√©ment optimaux.
- Sachant cela, l'objectif reste de minimiser les variances intra-classes tout en s'√©vitant des temps de calcul trop contraignants.
:::

# R√©duction de dimensions

## Analyse factorielle

### G√©n√©ralit√©s

- Outils fondamentaux de l'analyse des tableaux de donn√©es qui ne pr√©sentent pas de structure particuli√®re.
- Permet de condenser les donn√©es de nombreuses variables en quelques variables latentes seulement pour simplifier la compr√©hension des r√©sultats.
- Permet de _r√©duire les dimensions_ des donn√©es en une ou plusieurs _super variables_ latentes.
- Variables latentes exprim√©es comme _combinaison lin√©aire_ des variables observ√©es.

:::link
Pour une approche math√©matique de l'analyse factorielle, voir le tr√®s bon [cours de Rodolphe Palm][cours-analyse-facto-maths] (attention, il faut √™tre √† l'aise avec le calcul matriciel avanc√© : vecteurs propres, d√©terminants, ...).
:::

#### üåü Avantages

:::tip
- Condenser des variables.
- D√©couvrir des rassemblements de donn√©es.
- Simplifier la compr√©hension.
:::

##### Exemple 1 : profiles type

Dans une √©tude visant √† conna√Ætre la typologie des clients qui utilisent le plus son site Internet, une entreprise va r√©colter de nombreuses data sur les individus √©tudi√©s :

- caract√©ristiques socio-d√©mographiques des r√©pondants (sexe, √¢ge, lieu d'habitation, CSP, coordonn√©es)
- habitudes d'achat (canal d'achat privil√©gi√©, nombre de visites par mois en ligne)
- leurs opinions

L'analyse factorielle cr√©e des profils type d'individus : plus facile √† observer et expliquer.

##### Exemple 2 : repr√©sentation graphique

En cat√©gorisant les donn√©es, il est possible de les repr√©senter sous forme de nuage de points dans une matrice ou un graphique :

- Permet de visualiser les rapprochements, les corr√©lations et les oppositions entre les diff√©rents types de donn√©es.
- Permet de visualiser l'influence d'un facteur sur les r√©ponses donn√©es
- Facilite l'interpr√©tation des r√©sultats : l'influence de l'√¢ge des individus sur l'utilisation de tel canal d'achat, ...

#### Utilisation

- Lors d'√©tudes de segmentation pour regrouper les consommateurs et les clients en segments.
- Comme √©tape interm√©diaire pour r√©duire le nombre de variables avant cr√©ation des segments par [k-moyennes][wiki-k-means]. Voir le chapitre "M√©thode des centres mobiles".
- Dans un but descriptif : condense l'information contenue dans un tableau constitu√© d'un grand nombre de lignes et de colonnes en quelques repr√©sentations graphiques √† deux dimensions, accompagn√©es de tableaux reprenant les valeurs num√©riques de caract√©ristiques destin√©es √† aider l'utilisateur lors de l'interpr√©tation.
- Simplifie les r√©sultats d'une enqu√™te complexe : longues √©tudes comportant plusieurs grands blocs de questions dont les r√©ponses sont sous forme d'√©chelle comme l'√©chelle de Likert (r√©ponses allant de "pas du tout d'accord" √† "tout √† fait d'accord" )
  + condense les r√©ponses.
  + cr√©e des groupes de facteurs.
  + r√©sultats plus visuels et compr√©hensibles.
  + Les questions √† √©chelle qui portent sur le comportement (habitudes d'achat, etc.) et la psychologie (pr√©f√©rences, croyances, etc.) des consommateurs sont particuli√®rement adapt√©es.

## Analyse factorielle discriminante

On dispose de $n$ observations sur lesquelles on a relev√© :

- les valeurs d'une variable cat√©gorielle comportant quelques modalit√©s $(2, 3, ...)$ : c'est le groupe ou diagnostic.
- les valeurs de $p$ variables num√©riques : $X_1, X_2, ... , X_p$ : ce sont les pr√©dicteurs.

On se pose des questions telles que :

- Dans quelle mesure la valeur de $Y$ est-elle li√©e aux valeurs de $X_1, X_2, ... , X_p$ ?
- √âtant donn√© d'autres observations, pour lesquelles $X_1, X_2, ... , X_p$ sont connues, mais $Y$ ne l'est pas, est-il possible de pr√©voir $Y$ (le groupe), et avec quel degr√© de certitude ?

Exemples de situations o√π une telle m√©thode peut √™tre int√©ressante :

- On √©tudie les diff√©rentes esp√®ces de poissons peuplant un lac, mais la d√©termination exacte de l'esp√®ce suppose que l'on sacrifie l'animal. Peut-on se contenter de relever diff√©rents param√®tres concernant les poissons pr√©lev√©s, et d√©duire l'esp√®ce √† partir de ces param√®tres avec un degr√© de certitude raisonnable ?
- Pour d√©terminer le type d'utilisation de parcelles agricoles, pourrait-on utiliser les informations donn√©es par des images satellites ?

La m√©thode est √©galement utilis√©e sans que l'on ait un objectif de pr√©diction; on souhaite seulement d√©terminer les pr√©dicteurs les plus li√©s au groupe d'appartenance.

Pr√©cautions et limites de la m√©thode ‚ùå:

- Suppose que les variables pr√©dictrices poss√®dent des propri√©t√©s de r√©gularit√© satisfaisantes : distribution normale (voire multinormale) des variables $X_i$ dans les diff√©rentes populations.
- Peut conduire √† des r√©sultats incorrects si les variables $X_i$ sont trop fortement corr√©l√©es entre elles.

### Exemple 1

On a relev√© les valeurs de deux variables $X_1$ et $X_2$ sur 40 individus statistiques r√©partis en deux groupes. Le nuage de points repr√©sentant ces observations est le suivant :

![Exemple d'analyse discriminante](@assets/data/analyse-disc-1.png)

<div class="caption">Exemple d'analyse discriminante</div>

Prise isol√©ment, aucune des deux variables $X_1$ et $X_2$ ne permet de diff√©rencier les deux groupes $G_1$ et $G_2$. Cependant, on voit bien que les deux groupes occupent des r√©gions du plan bien sp√©cifiques.

On voit intuitivement que notre probl√®me pourrait √™tre r√©solu en consid√©rant une variable abstraite, combinaison lin√©aire de $X_1$ et $X_2$ (approximativement $X_1 + X_2$) d√©finie telle que :

-¬†la variance (dispersion) intra-groupes soit la plus petite possible.
-¬†la variance inter-groupes (variance calcul√©e √† partir des points moyens pond√©r√©s des groupes) soit la plus grande possible.

Ainsi, sur notre exemple, la droite d'√©quation $X_2 = -X_1 + 19$ semble s√©parer correctement les deux groupes et il semblerait que c'est en projetant les points sur la droite $X_2 = X_1$ que l'on obtiendra une dispersion minimale dans les groupes et maximale entre les groupes.

![Une autre analyse discriminante](@assets/data/analyse-disc-2.png)

<div class="caption">Une autre analyse discriminante</div>

Apr√®s analyse factorielle discriminante, on obtient ce graphique o√π les points bien class√©s sont repr√©sent√©s par des cercles, les points mal class√©s par des triangles et les points suppl√©mentaires par des carr√©s. La couleur (rouge ou noir) correspond au groupe calcul√©.

### Exemple 2 : _distance de Mahalanobis_.

Dans l'exemple pr√©c√©dent, les deux groupes pr√©sentent √† peu pr√®s la m√™me dispersion de valeurs. Cependant, dans d'autres situations, l'un des groupes peut √™tre nettement plus dispers√© que l'autre. 

Consid√©rons la situation suivante, o√π l'on a repr√©sent√© la distribution des valeurs issues de deux groupes sur un "facteur discriminant".

Dans le premier groupe, cette distribution est normale, de moyenne 0 et d'√©cart type 1. Dans le second groupe, elle est normale, de moyenne 5 et d'√©cart type 3. On souhaite, par exemple, affecter la valeur $x=2$ √† l'un des deux groupes. Pour la distance euclidienne, cette valeur est plus pr√®s du centre du premier groupe (valeur $\bar{X_1}=0$) que du centre du second groupe (valeur $\bar{X_2}=5$). Cependant, $x=2$ a plus de chances d'√™tre une observation provenant du second groupe qu'une observation provenant du premier groupe.

Pour r√©soudre ce probl√®me, on introduit une distance particuli√®re : la _distance de Mahalanobis_ pour √©valuer la distance entre un point et le centre d'un groupe. Pour calculer cette distance, on fait intervenir les √©carts entre $x$ et les centres de groupes :

$$d_1^2(\bar{x_1},2) = \left(\frac{2-0}{1}\right)^2 = 4,d_2^2(\bar{x_2},2) = \left(\frac{2-5}{3}\right)^2 = 1$$

La d√©finition de la distance de Mahalanobis est nettement plus compliqu√©e lorsqu'il y a plusieurs variables √† prendre en compte, car elle fait intervenir les covariances des variables prises deux √† deux.

### Approche pr√©dictive

:::link
L'analyse discriminante peut √©galement √™tre pr√©dictive (apprentissage supervis√©) : il s'agit de l'[analyse discriminante lin√©aire][wiki-adl] (comparable √† la r√©gression logistique).
:::

## ACP : Analyse en Composantes Principales (_PCA_) 

- Analyse factorielle la plus courante
- Transforme des variables corr√©l√©es statistiquement (li√©es entre elles) en nouvelles variables d√©corr√©l√©es les unes des autres : les _composantes principales_.
- Avantages üåü :
  + R√©duit le nombre de variables.
  + Simplifie une analyse.
  + Identifie le facteur qui provoque le plus de variance.

### Exemple

Prenons l'exemple d'une entreprise qui propose des solutions informatiques B2B. Les freins √† l'achat des potentiels clients peuvent √™tre :

- Un prix excessif
- Un co√ªt li√© √† la mise en place trop √©lev√©
- Un d√©saccord dans l'organisation cliente
- Un produit/service non conforme avec la strat√©gie commerciale
- Le besoin de d√©velopper davantage son ROI avant l'acquisition de la solution
- Un contrat avec un autre prestataire
- Les b√©n√©fices du produit ne l'emportent pas sur les co√ªts
- Pas de raison de changer de solution
- Incompatibilit√© informatique
- Les ressources techniques internes sont insuffisantes
- Une fonctionnalit√© manquante dans l'offre propos√©e qui ne r√©pond pas au besoin
- Autre

L'analyse factorielle peut r√©v√©ler des tendances et √©volutions de ces r√©ponses.

Voici la r√©partition des r√©ponses pour chaque facteur ou variable.
Les entreprises clientes ont donc √©t√© class√©es en fonction de leurs similitudes (groupe 1, groupe 2 et groupe 3). Ce facteur de classification peut √™tre la taille de l'entreprise, l'anciennet√©, ou tout autre type de donn√©es quantitatives.

![Classement en 3 groupes de similitudes](@assets/data/facteur-de-classification.png)

<div class="caption">Classement en 3 groupes de similitudes. Source: https://www.qualtrics.com/m/assets/fr/wp-content/uploads/2020/04/facteur-de-classification.png</div>

On observe dans ce cas que des sous-ensembles de variables se cr√©ent pour chaque composante ou facteur.

Le premier pond√®re fortement les variables li√©es au co√ªt, le deuxi√®me les variables li√©es √† l'informatique et le troisi√®me les variables li√©es aux facteurs organisationnels. Gr√¢ce √† cette repr√©sentation il est possible de donner des noms √† ces Composantes Principales :

- Co√ªt
- Informatique
- Organisation

![Classement en Composantes Principales](@assets/data/composantes-principales-png.png)

<div class="caption">Classement en Composantes Principales. Source: https://www.qualtrics.com/m/assets/fr/wp-content/uploads/2020/04/composantes-principales-png.png</div>

En repr√©sentant les valeurs sous forme matricielle, l'observation est simplifi√©e.

Si nous devions regrouper les clients potentiels interrog√©s en fonction de ces trois super variables, nous pouvons clairement voir des tendances se dessiner. Les prospects ont tendance √† nommer l'organisation (axe 1) ou le co√ªt (axe 2) comme barri√®re importante mais pas les deux √† fois.

![Classement des prospects](@assets/data/cluster-factuer.png)

<div class="caption">Classement des prospects. Source : https://www.qualtrics.com/m/assets/fr/wp-content/uploads/2020/04/cluster-factuer.png</div>

# Ressources

- [Vid√©o : P√¢te √† tartiner et variable continue (La statistique expliqu√©e √† mon chat)](https://www.youtube.com/watch?v=THk2GBxkg4o)
- [Vid√©o : Pourquoi gagnez-vous moins que le salaire moyen ? (La statistique expliqu√©e √† mon chat)](https://www.youtube.com/watch?v=uIx2xvdwIIo)
- [Vid√©o :  P-valeur ou je fais un malheur ! (La statistique expliqu√©e √† mon chat)](https://www.youtube.com/watch?v=xVIt51ybvu0)
- [Cours statistique descriptive √† une dimension][zds-stat-desc-1d]
- [Moyenne (Wikipedia)][wiki-moyenne]
- [Moyenne d'ordre p (Wikipedia)][wiki-moyenne-holder]
- [M√©diane (Wikipedia)][wiki-mediane]
- [√âcart-type (Wikipedia)][wiki-deviation]
- [√âcart-type - erreurs de calcul par ordinateur][deviation-computing-error]
- [√âcart-type - algorithme √©prouv√© de calcul par ordinateur][deviation-computing-solution]
- [Cours et exercices de statistiques descriptives √† une dimension][zds-stats]
- [Th√©or√®me de Bayes (Wikipedia)][wiki-bayes]
- [Loi normale (Wikipedia)][wiki-loi-normale]
- [Loi du chi carr√© (Wikipedia)][wiki-loi-chi-2]
- [Table des quantiles du chi-2][wiki-chi-2-table]
- [M√©thode des moindres carr√©s (Wikipedia)][wiki-moindres-carres]
- [Maximum de vraisemblance (Wikipedia)][wiki-max-vraisemblance]
- [Inf√©rence bay√©sienne (Wikipedia)][wiki-inference-bayesienne]
- [M√©thode des k plus proches voisins (Wikipedia)][wiki-k-neighbors]
- [Recherche de proches voisins (Wikipedia)][wiki-closest-neighbors]
- [Partitionnement en K moyennes (Wikipedia)][wiki-k-means]
- [Loi normale multidimensionnelle (distribution multinormale) (Wikipedia)][wiki-distrib-multinormale]
- [R√©gression lin√©aire g√©n√©ralis√©e (Slides)][slides-glm]
- [Loi logistique (Wikipedia)][wiki-loi-logistique]
- [R√©gression logistique (Wikipedia)][wiki-regression-logistique]
- [Cours sur la r√©gression p√©nalis√©e (Lasso)][slides-lasso]
- [Arbres de d√©cision (Wikipedia)][wiki-arbres-decision]
- [Cours de math√©matiques sur l'analyse factorielle][cours-analyse-facto-maths]
- [Introduction √† plusieurs types d'analyses factorielles][cours-analyses-facto-multiples]
- [Exemples d'analyses factorielles en R][analyse-facto-r]
- [Classification centroide - les k-medoides (Wikipedia)][wiki-k-medoides]
- [Visualisation de classification k-moyenne][visu-clustering-k-mean]
- [Visualisation de classification DBSCAN][visu-clustering-dbscan]
- [Approche math√©matique pour le traitement et le clustering de donn√©es sonores][clustering-donnees-sonores]
- [L'analyse discriminante lin√©aire (Wikipedia)][wiki-adl]
- [L'analyse discriminante factorielle (Wikipedia)][wiki-adf]

# Liens en programmation

- [Scipy : Statistiques en Python](https://scipy-lectures.org/packages/statistics/index.html#statistics)
- [scikit-learn: Machine Learning in Python](https://scipy-lectures.org/packages/scikit-learn/index.html)
- [Livre : Python pour le data scientist : Des bases du langage au machine learning](https://www.scholarvox.com/catalog/book/docid/88911514)


[zds-stat-desc-1d]: https://zestedesavoir.com/tutoriels/1669/statistique-descriptive-a-une-dimension/
[wiki-moyenne]: https://fr.wikipedia.org/wiki/Moyenne
[wiki-moyenne-holder]: https://fr.wikipedia.org/wiki/Moyenne_d%27ordre_p
[wiki-mediane]: https://fr.wikipedia.org/wiki/M%C3%A9diane_(statistiques)
[wiki-deviation]: https://fr.wikipedia.org/wiki/%C3%89cart_type
[wiki-bayes]: https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_Bayes
[wiki-loi-normale]: https://fr.wikipedia.org/wiki/Loi_normale
[wiki-loi-chi-2]: https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2
[wiki-chi-2-table]: https://fr.wikipedia.org/wiki/Loi_du_%CF%87%C2%B2#Table_de_valeurs_des_quantiles
[wiki-moindres-carres]: https://fr.wikipedia.org/wiki/M%C3%A9thode_des_moindres_carr%C3%A9s
[wiki-max-vraisemblance]: https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance
[wiki-inference-bayesienne]: https://fr.wikipedia.org/wiki/Inf%C3%A9rence_bay%C3%A9sienne
[wiki-k-neighbors]: https://fr.wikipedia.org/wiki/M%C3%A9thode_des_k_plus_proches_voisins
[wiki-closest-neighbors]: https://fr.wikipedia.org/wiki/Recherche_des_plus_proches_voisins
[wiki-k-means]: https://fr.wikipedia.org/wiki/K-moyennes
[wiki-k-medoides]: https://fr.wikipedia.org/wiki/Algorithme_des_k-m%C3%A9do%C3%AFdes
[wiki-dbscan]: https://en.wikipedia.org/wiki/DBSCAN
[wiki-distrib-multinormale]: https://fr.wikipedia.org/wiki/Loi_normale_multidimensionnelle
[slides-glm]: https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-modlin-mlg.pdf
[wiki-loi-logistique]: https://fr.wikipedia.org/wiki/Loi_logistique
[wiki-regression-logistique]: https://fr.wikipedia.org/wiki/R%C3%A9gression_logistique
[slides-lasso]: https://pbil.univ-lyon1.fr/members/fpicard/franckpicard_fichiers/master/coursLasso.pdf
[wiki-arbres-decision]: https://fr.wikipedia.org/wiki/Arbre_de_d%C3%A9cision_(apprentissage)
[deviation-computing-error]: https://www.johndcook.com/blog/2008/09/26/comparing-three-methods-of-computing-standard-deviation/
[deviation-computing-solution]: https://www.johndcook.com/blog/standard_deviation/
[zds-stats]: https://zestedesavoir.com/tutoriels/1669/statistique-descriptive-a-une-dimension/
[cours-analyse-facto-maths]: https://orbi.uliege.be/handle/2268/113271
[cours-analyses-facto-multiples]: https://jmeunierp8.github.io/ManuelJamovi/s15.html
[analyse-facto-r]: https://dimension.usherbrooke.ca/v2ssrchapitre11.html
[clustering-donnees-sonores]: https://larevueia.fr/approche-mathematique-pour-le-traitement-et-le-clustering-des-donnees-sonores/
[visu-clustering-k-mean]: https://www.naftaliharris.com/blog/visualizing-k-means-clustering/
[visu-clustering-dbscan]: https://www.naftaliharris.com/blog/visualizing-k-means-clustering/
[wiki-adl]: https://fr.wikipedia.org/wiki/Analyse_discriminante_lin%C3%A9aire
[wiki-adf]: https://fr.wikipedia.org/wiki/Analyse_discriminante
[wiki-bias]: https://en.wikipedia.org/wiki/Bias_of_an_estimator

