---
license: ¬© 2025 Tom Avenel under Û∞µ´  BY-SA 4.0
layout: '@layouts/CoursePartLayout.astro'
title: Tester avec et pour l'Intelligence Artificielle
tags:
- tests
- IA
---

## üñ•Ô∏è Tester avec l'IA

üéØ Comment utiliser l'IA pour tester du code et des applications ?

### G√©n√©ration de tests unitaires

- L'IA peut lire une fonction et proposer des **tests unitaires** automatiquement : cas normaux, limites, erreurs.
  - par outil int√©gr√© **GitHub Copilot**, ‚Ä¶ : compl√®te automatiquement des tests
  - par prompt **ChatGPT**, ‚Ä¶ : en copiant/collant du code et en demandant "g√©n√®re les tests unitaires manquants".

### Automatisation de tests fonctionnels

- Objectif : au lieu d'√©crire manuellement tous les sc√©narios Selenium/Playwright, d√©crire le sc√©nario en langage naturel et l'IA g√©n√®re le script de test automatis√©.
  - Exemple : "Teste que l'utilisateur peut se connecter, ajouter un produit au panier et valider la commande."

### Analyse de couverture

- L'IA peut pointer les parties du code **non couvertes** par les tests existants et proposer des sc√©narios manquants.
  - Exemple : utiliser `coverage.py` ou `JaCoCo`, puis donner le rapport √† lIA qui propose de nouveaux tests.

:::tip
- L'IA est particuli√®rement utile pour imaginer des entr√©es improbables : valeurs nulles, tr√®s grandes, caract√®res sp√©ciaux, mauvais formats JSON, etc.
:::

### Tests exploratoires

- L'IA simule un utilisateur curieux : elle clique un peu partout, essaie des valeurs √©tranges, et d√©tecte des bugs ou comportements inattendus. Cette strat√©gie √©tait auparavant quasiment exclusivement manuelle.
- Des outils √©mergent dans ce domaine : **Mabl**, **Testim.io**, **AutonomIQ**.

### V√©rification par prompt

- Faire interagir l'IA avec l'API ou l'application comme un **QA engineer virtuel**.
- L'IA envoie des requ√™tes, regarde les r√©ponses et dit si elles sont conformes aux specs.

### Suggestions de correctifs

- L'IA analyse les erreurs de test et propose automatiquement une solution, soit :
  - une solution au probl√®me de mise en place de l'environnement de test
  - un correctif applicatif dans le code

---

## Tester une IA g√©n√©rative (LLM)

Tests sp√©cifiques pour une application utilisant une IA g√©n√©rative :

1. Tests de robustesse (input-output)

  - Cr√©er des jeux de prompts de r√©f√©rence (sc√©narios repr√©sentatifs, edge cases, inputs adversariaux).
  - √âvaluer si les r√©ponses g√©n√©r√©es respectent les contraintes attendues (format, style, pr√©sence/absence de certains √©l√©ments).

2. Tests de non-r√©gression sur donn√©es gel√©es

  - Sauvegarder des paires input & output valid√©s.
  - Comparer les g√©n√©rations futures √† ces sorties "acceptables" (en tol√©rant une variabilit√©, via r√®gles ou scoring).

3. Tests bas√©s sur des m√©triques automatiques

  - Textuelles : BLEU, ROUGE, METEOR, BERTScore, perplexit√©.
  - Images : FID, CLIPScore.
  - Attention : ces m√©triques donnent une tendance mais ne suffisent pas seules.

4. Tests de conformit√© aux r√®gles

  - Absence de contenu interdit (toxicit√©, biais, hallucinations).
  - Respect du format demand√© (JSON bien form√©, sch√©ma valide, longueur max).
  - Coh√©rence factuelle (via croisement avec bases de v√©rit√© ou mod√®les de v√©rification).
  - Prompt injection attacks : v√©rifier que l'IA ne fuit pas de donn√©es sensibles ou n'ex√©cute pas d'ordres interdits.

5. √âvaluation humaine

  - Tests manuels exploratoires : v√©rifier pertinence, cr√©ativit√©, clart√©.
  - Panels utilisateurs / experts : scorer la qualit√© sur crit√®res d√©finis (fid√©lit√©, utilit√©, s√©curit√©).
  - A/B testing : comparer plusieurs versions de mod√®le ou de r√©glages (temp√©rature, prompt, fine-tuning).

---

## M√©triques automatiques

- Les sorties d'une IA g√©n√©rative sont **non-d√©terministes** (variabilit√© due √† la temp√©rature, au sampling, etc.).
- On ne peut pas tester avec un simple `assert sortie == attendu`.
- Les m√©triques donnent un **score de similarit√© ou de qualit√©**, qui peut servir de seuil de validation ou d'indicateur de r√©gression.

L'id√©e est de disposer de crit√®res **quantitatifs et reproductibles** pour √©valuer la qualit√© des g√©n√©rations de l'IA. Ces m√©triques ne remplacent pas l'√©valuation humaine, mais elles permettent d'automatiser une partie des tests et de comparer diff√©rentes versions d'un mod√®le ou de prompts.

---

### M√©triques pour le texte g√©n√©r√©

1. **Bas√©es sur les n-grammes** (comparaison avec une r√©f√©rence humaine) :

  - **BLEU** (Bilingual Evaluation Understudy) : compare la proportion de n-grammes communs.
  - **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) : tr√®s utilis√© en r√©sum√© automatique, mesure rappel et pr√©cision sur des s√©quences.
  - **METEOR** : am√©liore BLEU en tenant compte de synonymes et de la flexibilit√© grammaticale.
  - Exemple d'usage : comparer une r√©ponse g√©n√©r√©e √† un "gold standard" r√©dig√© par un expert.

2. **Bas√©es sur l'embedding s√©mantique** :

  - **BERTScore** : calcule la similarit√© s√©mantique entre r√©f√©rence et g√©n√©ration via embeddings contextualis√©s (BERT, RoBERTa).
  - **Cosine similarity** sur embeddings (par ex. OpenAI Embeddings, Sentence-BERT).
  - Utile quand on veut √©valuer la fid√©lit√© au sens plut√¥t qu'√† la forme exacte.

3. **Bas√©es sur la fluence du texte** :

  - **Perplexit√©** : mesure la probabilit√© qu'un autre mod√®le de langage attribue √† la g√©n√©ration (texte plus "naturel" = perplexit√© faible).
  - Peut servir √† d√©tecter des textes incoh√©rents ou artificiels.

---

### M√©triques pour l'image g√©n√©r√©e

- **FID (Fr√©chet Inception Distance)** : compare la distribution des features entre images g√©n√©r√©es et vraies images.
- **IS (Inception Score)** : mesure la qualit√© et la diversit√© des images g√©n√©r√©es.
- **CLIPScore** : utilise CLIP pour mesurer la coh√©rence entre un texte (prompt) et une image g√©n√©r√©e.

---

### Strat√©gie pratique d'utilisation

1. **D√©finir un corpus de prompts de r√©f√©rence** (sc√©narios critiques, cas normaux, cas limites).
2. **G√©n√©rer des sorties avec l'IA** et les comparer √† :
  - des **r√©f√©rences humaines** (si disponibles),
  - ou des **crit√®res formels** (sch√©ma, format).
3. **Calculer les m√©triques** (ex. BLEU, BERTScore, FID selon le type de contenu).
  - fonctionnent mieux avec un **ensemble de sorties** (moyenne statistique) qu'au cas par cas.
4. **Fixer des seuils d'acceptabilit√©** (ex. BLEU ‚â• 0.4 pour un r√©sum√©, perplexit√© ‚â§ 50 pour un texte coh√©rent).
5. **Surveiller les tendances** : comparer les scores entre versions de mod√®les ou de prompts pour d√©tecter des r√©gressions.
6. Compl√©ter par :

  - **r√®gles de conformit√©** (format, s√©curit√©),
  - **tests humains** pour juger nuance, cr√©ativit√©, pertinence.

:::tip
En pratique, on conseille d'**orchestrer plusieurs m√©triques** en parall√®le (par ex. BLEU + BERTScore + validation de format) pour obtenir une vision plus robuste.
:::

---

## Test de Retrieval-Augmented Generation (RAG)

Un **RAG (Retrieval-Augmented Generation)** combine deux briques :

1. **R (Retrieval)** : aller chercher les bons documents dans une base (vectorielle, index, moteur de recherche).
2. **AG (Augmented Generation)** : g√©n√©rer une r√©ponse avec un LLM en s'appuyant sur ces documents.

Tester un RAG, c'est donc tester **√† la fois la recherche et la g√©n√©ration** - et leur int√©gration.

---

### Retrieval

üéØ Objectif : v√©rifier que les bons documents sont bien retrouv√©s.

1. Tests unitaires :

  - V√©rifier que l'indexation fonctionne (documents bien ing√©r√©s, embeddings valides)
  - V√©rifier que le **sch√©ma** (m√©tadonn√©es, _chunks_) est respect√©

2. Tests de qualit√© de recherche :

  - Cr√©er un **jeu de requ√™tes de r√©f√©rence** avec leurs documents attendus (_ground truth_).
  - Mesurer la qualit√© avec des m√©triques de recherche d'information :

    - **Recall@k** (les bons documents apparaissent-ils dans le top-k r√©sultats ?)
    - **Precision@k** (parmi les r√©sultats, combien sont pertinents ?)
    - **MRR (Mean Reciprocal Rank)** (le premier document pertinent est-il bien class√© en haut ?)

3. Tests de robustesse :

  - Cas "difficiles" : synonymes, paraphrases, fautes de frappe.
  - Cas pi√©g√©s : requ√™tes ambigu√´s ou bruit√©es.

---

### Augmented Generation

üéØ Objectif : v√©rifier que le LLM produit une r√©ponse correcte **√† partir du contexte fourni**.

1. Conformit√© au format :

  - La r√©ponse respecte-t-elle le format attendu (JSON, liste, texte court) ?
  - Validation automatique avec un sch√©ma ou un parseur.

2. Pertinence et fid√©lit√© :

  - **Fact-checking** : la r√©ponse est-elle support√©e par les documents fournis ?

    - M√©triques possibles : **Faithfulness score** (_LLM judge_ : utiliser un autre LLM comme correcteur automatique).
    - V√©rification automatique : chaque phrase doit pouvoir √™tre trac√©e √† un _chunk_ (morceau de document).

3. Non-hallucination :

  - D√©tecter les parties de la r√©ponse qui n'ont aucun support dans les documents.
  - Tests avec prompts adversariaux (questions hors contexte : le RAG doit r√©pondre "je ne sais pas" ou rester neutre).

---

### Int√©gration R + AG

üéØ Objectif : v√©rifier que les deux briques s'assemblent correctement.

1. End-to-end testing : donner une question et observer la r√©ponse.

2. Cha√Æne de tra√ßabilit√© :

  - V√©rifier que la r√©ponse cite ou r√©f√©rence les sources.
  - V√©rifier que les documents affich√©s √† l'utilisateur correspondent vraiment √† ceux utilis√©s en entr√©e du LLM.

3. R√©silience aux erreurs :

  - Que se passe-t-il si aucun document pertinent n'est trouv√© ?
  - Gestion du bruit (documents partiellement pertinents).
  - Tests de charge (latence du retrieval + g√©n√©ration).

---

