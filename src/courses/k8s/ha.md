---
license: ¬© 2025 Tom Avenel under Û∞µ´  BY-SA 4.0
title: Haute Disponibilit√© d'un Cluster Kubernetes
layout: '@layouts/CoursePartLayout.astro'
tags:
- kubernetes
- devops
- HA
---

## Objectifs

- **√âliminer le point de d√©faillance unique (SPOF)** de l'API server : ne pas d√©pendre d'un seul n≈ìud pour l'acc√®s au Kubernetes API.
- **Assurer la continuit√© de l'orchestration** : scheduler, contr√¥leurs et autoscaler doivent pouvoir continuer √† fonctionner m√™me si une instance tombe.
- **Maintenir l'√©tat du cluster** : etcd doit rester disponible avec un quorum minimal de membres pour garantir coh√©rence et tol√©rance aux pannes.
- **Servir les workloads applicatifs** m√™me si le control plane est partiellement indisponible.

---

## Architecture de r√©f√©rence HA

- üõ† Control Plane multiples
- üß† Base d'√©tat etcd distribu√©e
- L'acc√®s aux n≈ìuds du control plane (`api-server`) se fait via un **point d'acc√®s unique hautement disponible** (`ControlPlaneEndpoint`) en utilisant :
  - un **load balancer** TCP (`HAProxy` + `keepalived`) externe ou stack√© dans les control-plane
  - une **virtual IP** (`kube-vip`)
  - un DNS _round-robin_ (non support√© officiellement mais fonctionne)
  - H/A API endpoint dans un cluster manag√©, virtual IP Cloud, tunnel _Node_ <-> _API Server_ (`k3s`), ‚Ä¶
- Dans de rares cas (par exemple _k3s_), il est possible de passer d'un cluster mono control-plane √† un cluster H/A √† tout moment.

:::link
Voir aussi :

- <https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/>
- <https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/>
- <https://kifarunix.com/setup-highly-available-kubernetes-cluster-with-haproxy-and-keepalived/>

:::

---

## Perte d'un Control Plane

- Contexte : un n≈ìud _control-plane_ est brutalement arr√™t√© ou ne r√©pond plus
- Plusieurs composants et boucles de contr√¥le interviennent en parall√®le :
  - kubelet / Node Lease
  - etcd
  - API Server + Load Balancer
  - Controller Manager
  - Scheduler
  - Node Lifecycle Controller
- Les d√©lais d√©pendent surtout :
  - des heartbeats
  - des leases
  - des taints
  - des tol√©rances de pods

---

### Effet imm√©diat

- Le n≈ìud arr√™t√© ne peut donc plus :
  - Renouveler son **Node Lease**
  - Envoyer son **Node Status**
  - Ex√©cuter les **static pods** du control-plane
- Le cluster ne r√©agit pas encore.

---

### Health Check du Load Balancer

Le Load Balancer externe :

- d√©tecte que l'API Server du n≈ìud est mort
- le retire de la rotation
- Les clients Kubernetes (`kubectl`, ‚Ä¶) retentent automatiquement sur les autres API Servers.
- Timing d√©pendant du load-balancer (typiquement 10 √† 30 secondes)

---

### etcd

Si etcd est **stack√©** sur les control-planes :

- Si le quorum reste valide : rien de bloquant
- Si quorum perdu : **√©critures bloqu√©es imm√©diatement**

---

### R√©√©lection des leaders

- Leader election via _Lease API_
- `kube-controller-manager`
- `kube-scheduler`
- `cloud-controller-manager` (si pr√©sent)

---

### Node `NotReady`

- Le _Node Lifecycle Controller_ :
  - Marque le n≈ìud `NotReady` :
    - `NodeMonitorGracePeriod` (d√©faut 40 s)
    - `NodeStatusUpdateFrequency` (d√©faut 10 s)
  - Puis applique des taints :
    - `node.kubernetes.io/not-ready:NoExecute`
    - `node.kubernetes.io/unreachable:NoExecute`
- Les pods tol√®rent ces taints par d√©faut pendant 5 min (`tolerationSeconds=300s`) :
  - Pods toujours `Running`
  - Le service peut √™tre d√©grad√©.
  - Le scheduler ne lance pas de rempla√ßant imm√©diatement.
- Rescheduling des Pods apr√®s expiration des 300 s :
  - `Deployment` / `ReplicaSet` / `StatefulSet` recr√©ent les pods
  - Le `Scheduler` les place sur un autre n≈ìud

---

### Static Pods et Suppression du Node

- Static Pods situ√©s dans `/etc/kubernetes/manifests` :
  - **Ne sont pas recr√©√©s ailleurs**
  - HA assur√© car chaque control-plane a d√©j√† ses pods
  - Aucune migration automatique.
- Kubernetes **ne supprime pas automatiquement** le Node :
  - `kubectl delete node <nodeName>`

---

### Noeud Worker

- Contexte : arr√™t brutal / perte d'un Worker
- Comportement global similaire (mais pas d'etcd, leader, ‚Ä¶)
- Sp√©cificit√© des `PersistantVolumeClaim` utilis√©s : le CSI d√©tache le volume du node mort puis l'attache ailleurs

---

### Timeline

| Temps apr√®s panne | √âv√©nement                       |
| ----------------- | ------------------------------- |
| 0‚Äì5 s             | Silence kubelet                 |
| 1‚Äì2 s             | R√©√©lection leader etcd          |
| 5‚Äì30 s            | LB retire API Server            |
| 10‚Äì15 s           | R√©√©lection scheduler/controller |
| ~40 s             | Node `NotReady`                 |
| 40‚Äì60 s           | Taints appliqu√©s                |
| 300 s             | Pods recr√©√©s ailleurs           |

Pour r√©duire le MTTR :

- `--node-monitor-grace-period`
- `--pod-eviction-timeout`
- `tolerationSeconds` des pods :
  - en HA, **300 s ‚Üí 30‚Äì60 s**
- Param√®tres leader election
- Health checks du Load Balancer

---
