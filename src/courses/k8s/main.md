---
license: Â© 2025 Tom Avenel under ó°µ«  BY-SA 4.0
title: Kubernetes
layout: '@layouts/CoursePartLayout.astro'
tags:
- docker
- kubernetes
- devops
---

## ğŸš€ Comparaison des Plateformes de Conteneurs

---

### ğŸŒŸ Introduction

> Une plateforme de conteneurs est un ensemble d'outils et de services qui permettent de gÃ©rer le cycle de vie des applications conteneurisÃ©es. ğŸ“¦

- **Orchestration** : Gestion automatisÃ©e du dÃ©ploiement, de la mise Ã  l'Ã©chelle, et de la mise en rÃ©seau des conteneurs. ğŸ”„
- **Ã‰volutivitÃ©** : CapacitÃ© Ã  ajuster les ressources et les services en fonction de la demande. ğŸ“ˆ
- **Isolation** : ExÃ©cution sÃ©curisÃ©e et isolÃ©e des applications pour Ã©viter les conflits. ğŸ”’
- **PortabilitÃ©** : ExÃ©cution cohÃ©rente des applications sur diffÃ©rents environnements (dÃ©veloppement, test, production ; on-premise et multi-cloud). ğŸŒ

---

### ğŸ§© Kubernetes

- **Description** : Plateforme open-source pour l'automatisation du dÃ©ploiement, la mise Ã  l'Ã©chelle et la gestion des applications conteneurisÃ©es. ğŸŒ
- De loin l'orchestrateur **le plus utilisÃ© avec DockerÂ®** ğŸ†
- **Avantages** ğŸŒŸ :
  - Grande communautÃ© et Ã©cosystÃ¨me ğŸ‘¥
  - Hautement extensible avec de nombreux outils et extensions ğŸ› ï¸
  - Prise en charge de charges de travail complexes ğŸ‹ï¸
- **InconvÃ©nients** âŒ:
  - Courbe d'apprentissage abrupte ğŸ“š
  - Configuration complexe âš™ï¸
- Pour les **dÃ©ploiements complexes et Ã©volutifs** ğŸŒ

---

### ğŸš€ OpenShift

- **Description** : Plateforme de conteneurs de Red Hat, basÃ©e sur Kubernetes, avec des fonctionnalitÃ©s supplÃ©mentaires pour les entreprises. ğŸ¢
- **Avantages** ğŸŒŸ :
  - IntÃ©gration facile avec d'autres produits Red Hat ğŸ”„
  - Interface utilisateur intuitive ğŸ–¥ï¸
  - SÃ©curitÃ© et conformitÃ© renforcÃ©es ğŸ”’
- **InconvÃ©nients** âŒ:
  - CoÃ»t Ã©levÃ© pour les fonctionnalitÃ©s d'entreprise ğŸ’°
  - Moins flexible que Kubernetes seul ğŸ¤¸
- Pour les **solutions d'entreprise avec support** ğŸ¢

---

### ğŸ³ Docker Swarm

- **Description** : Solution d'orchestration de conteneurs intÃ©grÃ©e Ã  Docker, simple et facile Ã  utiliser. ğŸ‹
- **Avantages** ğŸŒŸ :
  - IntÃ©gration transparente avec Docker ğŸ”„
  - Facile Ã  configurer et Ã  utiliser ğŸ› ï¸
  - IdÃ©al pour les petits dÃ©ploiements ğŸ 
- **InconvÃ©nients** âŒ:
  - Manque de fonctionnalitÃ©s avancÃ©es ğŸ›‘
  - CommunautÃ© et Ã©cosystÃ¨me plus petits ğŸ‘¥
- Pour les **environnements simples et rapides** ğŸ¡

---

### ğŸ—ï¸ Apache Mesos

- **Description** : Projet open-source pour la gestion des ressources dans les centres de donnÃ©es, prenant en charge les conteneurs et les charges de travail non conteneurisÃ©es. ğŸ¢
- **Avantages** ğŸŒŸ :
  - FlexibilitÃ© pour gÃ©rer divers types de charges de travail ğŸ”„
  - Ã‰volutivitÃ© et robustesse ğŸ“ˆ
- **InconvÃ©nients** âŒ:
  - ComplexitÃ© de configuration et de gestion âš™ï¸
  - Moins axÃ© sur les conteneurs que les autres solutions ğŸ¯
- Pour les **environnements hybrides et complexes** ğŸ—ï¸

---

### ğŸ“Š Comparaison

| Plateforme | FacilitÃ© d'utilisation | Ã‰volutivitÃ© | Ã‰cosystÃ¨me | CoÃ»t |
|------------|-----------------------|-------------|------------|------|
| Kubernetes | â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | ğŸ†“ |
| OpenShift | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸ | ğŸ’° |
| Swarm | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸ | ğŸ†“ |
| Mesos | â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸ | ğŸ†“ |

---

## ğŸ­ PrÃ©sentation de Kubernetes

---

`Kubernetes` (ou `k8s`) est un orchestrateur de dÃ©ploiement et de gestion de conteneurs applicatifs dans un cluster de machines virtuelles. ğŸš€

- IndÃ©pendant de DockerÂ® mais mÃªme runtime `containerd` => peut tourner les mÃªmes images ğŸ³
- Configure et gÃ¨re un cluster applicatif complexe : nÅ“uds du cluster, rÃ©seau, stockage, ... ğŸŒ
- PossibilitÃ© de gÃ©rer tout le cluster via API `kubectl` ğŸ”§
- Mais configuration recommandÃ©e via `Yaml` / `Json` pour audit ğŸ“

---

## ğŸ’¡ Recommandations

- `DockerÂ®` seul / `docker compose` pour CI/CD et outils internes ğŸ› ï¸
- `k8sÂ®` pour gestion applicative de l'environnement de production ğŸ—ï¸
- `k8sÂ®` duplique des fonctionnalitÃ©s de DockerÂ® => prÃ©fÃ©rer 100% DockerÂ® ou k8sÂ® ğŸ”„

---

## ğŸ“¦ Technologies de conteneurs supportÃ©es

1. `containerd` : projet open-source crÃ©Ã© pour Kubernetes (runtime de `Docker` : _Docker sans la CLI_) ğŸ³
2. `Docker Engine` : _Docker avec la CLI_ ğŸ³
3. `Podman` : alternative _serverless_ Ã  Docker ğŸ³
4. `CRI-O` : conteneurs lÃ©gers ğŸ“¦
5. `Mirantis Container Runtime (MCR)` (anciennement _Docker Enterprise_) ğŸ¢

---

```mermaid
---
title: Architecture des technologies de conteneurs
---
flowchart TD
    A["Docker (CLI)"]
    B[Kubernetes]
    
    A --> ContainerD
    B --> CRI

    CRI["Container Runtime Interface (CRI)<br/>[Kubernetes API]"]
    CRI --> ContainerD
    CRI --> CRIO

    ContainerD["ContainerD (pull images, network, storage)"]
    CRIO[CRI-O]

    ContainerD --> OCI
    CRIO --> OCI

    OCI["Open Container Interface (OCI) spec"]
    OCI --> runC

    runC["runC (create / run containers)"]
    runC --> Container[Container]
```

---

## ğŸŒ Plugin rÃ©seau (CNI)

- **Container Networking Interface** (_CNI_) : ğŸŒ
  - Permet la communication rÃ©seau au sein du cluster ğŸŒ
  - Parfois intÃ©grÃ© Ã  la distribution, sinon Ã  installer sÃ©parÃ©ment ğŸ› ï¸
  - [GitHub - CNI](https://github.com/containernetworking/cni/) ğŸ”—
  - Par dÃ©faut, _Kubelet_ charge les configurations des plugins rÃ©seau depuis : `/etc/cni/net.d` ğŸ“‚

---

### ğŸ”„ CNI (Kubernetes) vs CNM (Docker)

- **Docker** : ğŸ³
  - RÃ©seaux **multiples** et **isolÃ©s** ğŸŒ
  - DNS par **rÃ©seau** ğŸ“¡
  - **Pas d'interconnexion** des rÃ©seaux âŒ

- **Kubernetes** : ğŸš€
  - **1 seul** rÃ©seau de conteneurs (_flat_) ğŸŒ
  - DNS par **`Namespace`** ğŸ“¡
  - **Aucune isolation** des rÃ©seaux par dÃ©faut (utiliser des `NetworkPolicies`) âš ï¸

---

### ğŸŒ Flannel

- RÃ©seau de sous-rÃ©seaux pour Kubernetes ğŸŒ
- Fonctionne avec divers backends (VXLAN, UDP, etc.) ğŸ”„
- Offre une isolation rÃ©seau par pod ğŸ”’
- Plus simple Ã  configurer que les autres options ğŸ› ï¸
- InconvÃ©nients : Peut introduire une latence supplÃ©mentaire, moins de fonctionnalitÃ©s avancÃ©es (`NetworkPolicies`, â€¦), moins adaptÃ© aux trÃ¨s grands clusters âš ï¸

:::link
Pour plus d'information sur Flannel, voir : <https://msazure.club/flannel-networking-demystify/>
:::

---

### ğŸ›¡ï¸ Calico

- Supporte plusieurs modes de rÃ©seau : ğŸŒ
  - routage direct
  - VXLAN seul
  - IPIP + BGP
  - eBPF
  - cross-subnet possible (tout ou aucun traffic ou uniquement la partie qui traverse un subnet) : AWS multi-AZ, azure vnets, rÃ©seaux L2 hÃ©tÃ©rogÃ¨nes
- Propose une isolation rÃ©seau granulaire (par pod) ğŸ”’
- IntÃ¨gre de la sÃ©curitÃ© ğŸ›¡ï¸
- ConÃ§u pour des (trÃ¨s) grands clusters ğŸ—ï¸
- S'intÃ¨gre bien avec l'infrastructure existante ğŸ”„
- Souvent utilisÃ© dans les dÃ©ploiements Cloud â˜ï¸
- InconvÃ©nients : Complexe, besoin de compatibilitÃ© rÃ©seau (BGP) âš ï¸

:::link
Voir aussi :

- Calico en VXLAN ou IPIP : <https://docs.tigera.io/calico/latest/networking/configuring/vxlan-ipip>
- Introduction Ã  eBPF et utilisation en Calico : <https://docs.tigera.io/calico/latest/about/kubernetes-training/about-ebpf>

:::

---

### ğŸ•¸ï¸ Weave

- CrÃ©e un rÃ©seau virtuel entre tous les conteneurs ğŸŒ
- Utilise le DNS intÃ©grÃ© de Docker ğŸ“¡
- Propose une isolation rÃ©seau par pod ğŸ”’
- Facile Ã  configurer mais peut Ãªtre moins performant que les autres options ğŸ› ï¸

---

### âš¡ Cilium

- Utilise _eBPF_ (_Berkeley Packet Filter_) âš¡
  - (TrÃ¨s) performant, dÃ©bit Ã©levÃ© et latence rÃ©duite âš¡
- MÃ©triques dÃ©taillÃ©es sur le trafic rÃ©seau ğŸ“Š
- Supporte dynamiquement l'ajout et la suppression de nÅ“uds ğŸ”„
- ConÃ§u pour gÃ©rer des clusters de grande taille ğŸ—ï¸
- InconvÃ©nients : Complexe (eBPF et concepts rÃ©seau avancÃ©s), eBPF doit Ãªtre activÃ© dans le noyau Linux âš ï¸

:::tip

- Cilium fournit un outil de monitoring (_Hubble_) avec une CLI et UI permettant de visualiser les communications au sein du cluster.
- Cilium fournit un "_Cluster Mesh_" (âš ï¸ Ã  ne pas confondre avec un _Service Mesh_ k8s) permettant une communication load balancÃ© entre _Service_ de diffÃ©rents clusters.

:::

---

### â” Autres

- D'autres CNI exotiques existent :
- `Canal` (`Flannel` + `Calico`) pour ajouter les `NetworkPolicies` (ou avant le support des VXLAN par Flannel)
- CNIs Cloud : _Amazon VPC CNI_ (_EKS_), _Azure CNI_ (_AKS_), _GCP CNI_ (_GKE_), _NSX-T CNI_ (_VMware Tanzu_), _Cisco ACI CNI_, â€¦
- _SR-IOV CNI_ (haute performance, accÃ¨s direct au _NIC_)
- _Macvlan CNI_
- `kindnetd` pour _kind_
- â€¦

---

| CritÃ¨re | Calico | Flannel | Weave Net | Cilium |
|-------------|------------|-------------|---------------|------------|
| **Type de RÃ©seau** | Couche 3 (IPIP, BGP, VXLAN) | Couche 3 (VXLAN, UDP) | Couche 2 (Overlay) | Couche 3 (eBPF) |
| **SÃ©curitÃ©** | Politiques de rÃ©seau granulaires | Politiques de rÃ©seau basiques | Politiques de rÃ©seau basiques | Politiques de rÃ©seau granulaires |
| **Performance** | Haute | Moyenne | Moyenne | TrÃ¨s haute |
| **ScalabilitÃ©** | TrÃ¨s Ã©levÃ©e | Moyenne | Moyenne | TrÃ¨s Ã©levÃ©e |
| **ComplexitÃ©** | Moyenne Ã  Ã©levÃ©e | Faible | Faible Ã  moyenne | Ã‰levÃ©e |
| **FonctionnalitÃ©s** | AvancÃ©es (BGP, IPIP, VXLAN) | Basiques | Basiques Ã  moyennes | AvancÃ©es (eBPF, DNS, chiffrement) |
| **CompatibilitÃ©** | Kubernetes, OpenShift, Docker | Kubernetes, Docker | Kubernetes, Docker, Mesos | Kubernetes |
| **RÃ©silience** | Ã‰levÃ©e | Moyenne | Ã‰levÃ©e | Ã‰levÃ©e |

---

## ğŸ“¦ Distributions Kubernetes

---

1. **Kubeadm** ğŸ› ï¸
   - Outil officiel
   - Installation de chaque composant sÃ©parÃ©ment
   - Le plus configurable mais le plus complexe

---

2. **Kubespray** ğŸ”„
   - Utilise `Ansible` pour (re)dÃ©ployer automatiquement un cluster
   - Compatible _bare-metal_ et _cloud_ â˜ï¸

---

3. **Rancher (RKE)** ğŸ—ï¸
   - Plateforme complÃ¨te pour gÃ©rer des clusters Kubernetes
   - Propose des fonctionnalitÃ©s avancÃ©es comme la gestion multi-cluster ğŸŒ
   - Offre une interface graphique intuitive ğŸ–¥ï¸

---

4. **K3s (Rancher Labs)** ğŸ“¦
   - Version allÃ©gÃ©e de Kubernetes conÃ§ue pour les environnements embarquÃ©s
   - Consomme moins de ressources que Kubernetes standard ğŸ”‹
   - IdÃ©al pour les systÃ¨mes Ã  faible puissance âš¡
   - Utilise le CNI `flannel` ğŸŒ
   - Voir aussi : _k3d_ (_k3s in Docker_) : similaire _kind_ (voir ci-dessous) pour k3s ğŸ³

---

5. **K0s (CNCF)** ğŸ“¦
   - Autre version allÃ©gÃ©e Kubernetes
   - TrÃ¨s minimale, aucun composant additionnel ğŸ”§
   - Compatible on-premise, edge, IoT, â€¦ ğŸŒ

---

6. **OpenShift** ğŸ¢
   - Distribution propriÃ©taire de Red Hat basÃ©e sur Kubernetes
   - Inclut des fonctionnalitÃ©s supplÃ©mentaires comme l'orchestration d'applications ğŸ› ï¸
   - Forte sÃ©curitÃ© et conformitÃ© ğŸ”’

---

7. **Docker Kubernetes Service (DKS)** ğŸ³
   - Surveillance intÃ©grÃ©e du cluster et des applications ğŸ‘ï¸
   - Nombreux drivers storage ğŸ’¾

---

8. **MicroK8s (Ubuntu)** ğŸ“¦
   - Distribution lÃ©gÃ¨re et sÃ©curisÃ©e de Kubernetes
   - ConÃ§ue pour les environnements Ubuntu ğŸ§
   - Propose des fonctionnalitÃ©s avancÃ©es comme l'installation de paquets ğŸ“¦

---

9. **Minikube** ğŸ§ª
   - Version lÃ©gÃ¨re pour le dÃ©veloppement et le test
   - Fonctionne sur un seul ordinateur ğŸ’»
   - IdÃ©al pour dÃ©butants et environnement de dÃ©veloppement ğŸ› ï¸

---

10. **Docker Desktop** ğŸ³
    - IntÃ¨gre Kubernetes nativement
    - Offre une expÃ©rience utilisateur simplifiÃ©e ğŸ–¥ï¸
    - AdaptÃ© aux dÃ©veloppeurs utilisant Docker ğŸ› ï¸

---

11. **Kind (Kubernetes IN Docker)** ğŸ§ª
    - DÃ©ploie Kubernetes dans un conteneur pour le dÃ©veloppement et le test
    - CrÃ©e rapidement un ou plusieurs clusters localement ğŸ—ï¸
    - Utile pour tester plusieurs clusters : upgrade, changements d'infrastructure, â€¦ ğŸ”„
    - CNI custom : `kindnetd` ğŸŒ
    - Utilise `kubeadm` ğŸ› ï¸

---

12. **Talos Linux** ğŸ§
    - Distribution Linux dÃ©diÃ©e
    - OS immuable : pas de SSH, shell, â€¦ ğŸ”’

---

### â˜ï¸ Plateformes managÃ©es

- Amazon Elastic Kubernetes Service (EKS) ğŸŒ
- Google Kubernetes Engine (GKE) ğŸŒ
- Azure Kubernetes Services (AKS) ğŸŒ
- Oracle Kubernetes Engine (OKE) ğŸŒ
- IBMCloud K8s ğŸŒ
- OVHCloud K8s ğŸŒ

:::tip
Les services gÃ©rÃ©s coÃ»tent souvent 30 Ã  50 % de plus que les services autogÃ©rÃ©s, mais rÃ©duisent considÃ©rablement les frais opÃ©rationnels ([source](https://testkube.io/blog/when-to-adopt-kubernetes-pay-now-or-pay-later-dilemma)).
:::

---

:::link
Il existe de (trÃ¨s) nombreuses maniÃ¨res d'installer Kubernetes : voir <https://github.com/zwindler/101-ways-to-deploy-kubernetes>
:::

---

## ğŸ—ï¸ Architecture

---

### ğŸ› ï¸ Installation

- Plusieurs mÃ©thodes :
  - `kubeadm` : l'outil officiel (installation de chaque composant sÃ©parÃ©ment) ğŸ› ï¸
  - Installeur intÃ©grÃ© dans la distribution : `k3s`, `minikube`, `microk8s`, â€¦ ğŸ“¦
  - Versions managÃ©es : outils dÃ©diÃ©s au fournisseur de Cloud â˜ï¸

---

### ğŸ“‚ ModÃ¨le

- Un cluster k8s est composÃ© de plusieurs `Node` ğŸŒ
- Chaque `Node` fait tourner des `Pod` (ensemble de conteneurs - c'est l'unitÃ© atomique de k8s !) ğŸ“¦
- Un `Deployment` gÃ¨re _dÃ©clarativement_ des ressources Ã  dÃ©ployer (pods, replicas, mise Ã  jour, â€¦) ğŸ”„
- Un `Service` permet d'exposer les ports d'un pod (interne ou externe) ğŸŒ
  - _Aucun lien avec un `service` de `docker-compose` !_ âš ï¸

---

### ğŸ·ï¸ Types de Nodes

- Node de rÃ´le `master` : le `control pane`, gÃ¨re le cluster (orchestration, API server, â€¦) ğŸ¢
- Node de type `worker` (sans rÃ´le) : exÃ©cute les pods et leur fournit les ressources ğŸ› ï¸

---

### âŒLimites

- k8s est fait pour gÃ©rer de gros clusters : ğŸ—ï¸
- Limitations Kubernetes v1.31 :
  - < 5,000 Node ğŸŒ
  - < 110 Pod / Node ğŸ“¦
  - < 150,000 Pod (total) ğŸ“¦
  - < 300,000 Containers (total) ğŸ“¦

---

![Architecture d'un cluster Kubernetes](https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg)

<div class="caption">Architecture d'un cluster Kubernetes (source: kubernetes.io)</div>

---

```mermaid
---
title: Architecture d'un Pod
---
flowchart TD
    subgraph Node
        subgraph "Pod : 1 adresse IP"
            Logger[Conteneur docker : logger]
            Nginx[Conteneur docker : nginx]
        end
    end

  class Logger,Nginx blue
```

<div class="caption">Architecture d'un Pod</div>

---

## ğŸ§© Composants

- `APIServer` : API de gestion du cluster ğŸŒ
- `etcd` : Stockage de la configuration du cluster ğŸ“‚
- `Controller Manager` : GÃ¨re les `WorkerNode` depuis le `MasterNode` ğŸ¢
- `Kubelet` : ExÃ©cute et gÃ¨re les conteneurs sur les `Node` ğŸ“¦
- `Kube-proxy` : Ã‰quilibre le trafic sur chaque `Node` ğŸŒ
- `Scheduler` : Assigne les `Pod` Ã  un `Node` ğŸ“…

---

### ğŸ“‚ etcd

- Backend k8s : Ã‰tat du cluster (le reste est stateless) ğŸ“‚
  - Store clÃ©=valeur ğŸ”‘
- Dans ou en dehors du cluster ğŸŒ
- 1 leader (par consensus) ğŸ†
  - DÃ©ployer un nombre impair d'instances ğŸ”¢
  - Supporte N/3 instances dÃ©faillantes âš ï¸
- Jamais utilisÃ© directement (`APIServer`) âš ï¸
- Critique ! ğŸš¨
  - Machine dÃ©diÃ©e ou environnement isolÃ© ğŸ¢
  - Bonnes performances rÃ©seau / disque âš¡

---

### ğŸ”„ ControllerManager

- Compare l'Ã©tat dÃ©sirÃ© (dÃ©claratif) Ã  l'Ã©tat actuel ğŸ”„
- En dÃ©duit (et applique) les actions nÃ©cessaires (`APIServer`) ğŸ› ï¸
- Beaucoup de contrÃ´leurs diffÃ©rents ğŸ§©
  - PossibilitÃ© d'installer des contrÃ´leurs externes pour gÃ©rer de nouvelles ressources (`Custom Resource Definition`) ğŸ”§
  - Exemple : Load Balancer AWS, â€¦ â˜ï¸
- Boucles de rÃ©conciliation : ğŸ”„
  - Reconstruit des ressources si besoin pendant le cycle de vie du cluster ğŸ”„
  - Sans besoin d'intervention ğŸ› ï¸
- Contient toute l'intelligence de Kubernetes ğŸ§ 

---

```mermaid
---
title: "Boucle de rÃ©conciliation"
---
flowchart TD
    Observation --> Diff
    Diff --> Action
    Action --> Observation

```

---

### ğŸ“… Scheduler

- Assigne les `Pod` (en state: `Pending`) aux `Node` ğŸ“…
  - Techniquement : crÃ©e un `Binding` et change le `nodeName` du `Pod` ğŸ”§
- Calcule de score par _filtrage_ puis _score_ : ğŸ“Š
  1. _Filtrage_ : CapacitÃ©, tolÃ©rance, affinitÃ©, sÃ©lecteurs, â€¦ ğŸ”
  2. _Score_ : Load-balancing, â€¦ âš–ï¸
- PossibilitÃ© d'installer un `Scheduler` customisÃ© ğŸ”§

---

### ğŸ“¦ Kubelet

- 1 `Kubelet` par `Node` ğŸ“¦
  - Un `kubelet` est souvent installÃ© sur le `MasterNode` pour y gÃ©rer ses composants dans des pods (optionnel) ğŸ¢
  - En gÃ©nÃ©ral, on y ajoute le `taint` : `node-role.kubernetes.io/master:NoSchedule` pour ne pas utiliser le _Master_ comme un _Worker_. âš ï¸
- Connexion permanente Ã  l'`APIServer` ğŸŒ
- DÃ©ploie le `Pod` s'il a le `nodeName` du `Node` courant : ğŸ“¦
  1. RÃ©cupÃ©ration de l'image (format `OCI`) ğŸ“¥
  2. CrÃ©ation des ressources : `Volumes`, `Networks`, `Containers` ğŸ› ï¸
  3. Ã‰tats du `Pod` : `pending` -> `running` / `failed` -> `succeeded` (terminÃ©) ğŸ”„
  4. Remonte l'information Ã  l'`APIServer` ğŸ“¤

---

### ğŸŒ Kube-proxy

- GÃ¨re le rÃ©seau sur chaque `Node` (entre Pods et vers extÃ©rieur) ğŸŒ
- Plusieurs modes : ğŸ”„
  - Tout trafic par (anciennement: `iptables`) `nftables`, rÃ¨gles `DNAT` (âš ï¸ CPU si beaucoup de rÃ¨gles) âš ï¸
    - Load-balancer : _round-robin_ ğŸ”„
  - Si CNI `Cilium` : RÃ¨gles `eBPF` dans le noyau, plus besoin de `Kube-proxy` ğŸŒŸ
    - Voir section sur les CNI ğŸ“š
- Connexion entre `Pods` : Niveau 3 (_IP_) ğŸŒ
- Connexion par `Services` : Niveau 4 (_TCP_, _UDP_) ğŸŒ
- Connexion par `Ingress` : Niveau 7 (_HTTP_) ğŸŒ

---

:::link
**Voir : <https://2021-05-enix.container.training/5.yml.html#50> pour un exemple de fonctionnement du _Control Plane_ suite Ã  la crÃ©ation d'un `Deployment`**
:::

---

## ğŸ› ï¸ Gestion du cluster

- Fichiers de configuration `yml` (Ã  privilÃ©gier autant que possible !) ğŸ“„
- Interface en ligne de commande `kubectl` (surtout pour lancer les fichiers de config) ğŸ–¥ï¸
- Interface web (peu utilisÃ©e) ğŸŒ

---

## ğŸ“‚ Ressources basiques du cluster

---

### ğŸ”„ Interactions entre ressources

- Les `Pod` exÃ©cutent les microservices. ğŸ“¦
- Les `Service` exposent ces pods pour permettre leur communication et leur accÃ¨s. ğŸŒ
- Les `ConfigMap` et `Secret` injectent les configurations et les donnÃ©es sensibles. ğŸ”
- Le/Les `Ingress` gÃ¨rent le trafic externe (routage par _URI_ ou header _host_) et les certificats SSL/TLS. ğŸŒ
- Les `PersistentVolume` et `StatefulSet` supportent les applications avec Ã©tat. ğŸ’¾
- Les `DaemonSet` assurent le fonctionnement des outils d'administration sur chaque nÅ“ud. ğŸ› ï¸

---

### ğŸ“¦ Gestion des applications

- `Deployment` : GÃ¨re le dÃ©ploiement d'un `ReplicaSet` ğŸ“¦
  - Et la mise Ã  jour des applications (rolling update, rollback, scaling) ğŸ”„
- `ReplicaSet` : CrÃ©e et gÃ¨re le suivi (rÃ©plicas) d'un pod ğŸ“¦
  - Ne pas utiliser de `ReplicaSet` directement mais passer par un `Deployment` (plus puissant) ğŸ› ï¸
- `Pod` : GÃ¨re un ensemble de conteneurs partageant la mÃªme isolation : stack rÃ©seau, stockage, â€¦ ğŸ“¦
  - DÃ©marrÃ© directement ou (mieux) par un `deployment` crÃ©ant un `ReplicaSet` ğŸ“¦
  - Ã‰phÃ©mÃ¨re : Pas de donnÃ©es critiques dans le pod âš ï¸
  - 1 IP par pod partagÃ©e entre tous les conteneurs (mais l'IP peut changer) ğŸŒ
    - AccÃ¨s par `localhost` aux autres conteneurs et **partage des ports ouverts** ğŸ”„

---

```mermaid
---
title: Deployment, ReplicaSet et Pods
---
flowchart TD
    subgraph "Deployment : replicas = 2"
        subgraph "ReplicaSet : 2 Pods"
            Pod1[Pod 1 : nginx]
            Pod2[Pod 2 : nginx]
        end
    end

    class Pod1,Pod2 green
```

<div class="caption">Un Deployment gÃ©rant un ReplicaSet gÃ©rant un Pod</div>

---

### ğŸ·ï¸ Labels

- Attributs clÃ©=valeur des objets du cluster ğŸ·ï¸
- UtilisÃ© par Kubernetes ğŸ› ï¸
- `NodeSelector` : Lance un pod sur un `Node` ayant ce label ğŸ·ï¸
- `NodeAffinity` : DÃ©crit des affinitÃ©s entre un `Pod` et un `Node` ğŸ·ï¸
- `podAffinity`, `podAntiAffinity` : (Anti)affinitÃ© entre `Pod` ğŸ·ï¸
- Il existe aussi des `annotations` : Idem mais NON utilisÃ© par k8s ensuite ğŸ“

---

#### ğŸ› Labels et debug

- Beaucoup de ressources utilisent les labels pour sÃ©lectionner les ressources (`Pod`, â€¦) Ã  manager ğŸ·ï¸
- Pour debugger un `Pod` fautif, on peut changer son `Label` : ğŸ›
  - Le Pod fautif sera retirÃ© du Service (plus de Load balancing) âš–ï¸
  - Un nouveau Pod est crÃ©Ã© par le `ReplicaSet` ou le `DaemonSet` ğŸ“¦
  - Le Pod fautif est toujours actif pour du debug ğŸ›

---

### ğŸŒ Service

- Service DNS load-balancÃ© permettant d'accÃ©der Ã  1 ou plusieurs Pods ğŸŒ
- Exemple : `<service_name>.<namespace>`
- Plusieurs types de services :
  - `ClusterIP` accessible uniquement dans le cluster
  - `NodePort` pour un accÃ¨s depuis l'extÃ©rieur
  - `LoadBalancer` pour utiliser un Load Balancer externe (cloud)
  - `ExternalName` pour un alias DNS

---

### ğŸ› ï¸ Configuration des applications

- `ConfigMap` pour modifier la configuration des applications ğŸ“
  - DÃ©corrÃ©lÃ© du code de l'application ğŸ› ï¸
- `Secret` (mots de passe, â€¦) : Assez similaire ğŸ”
  - [DiffÃ©rents types de Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#secret-types) ğŸ”—
  - âš ï¸ Par dÃ©faut, **simple encodage** : Voir les [bonnes pratiques de sÃ©curitÃ©](https://kubernetes.io/docs/concepts/security/secrets-good-practices/) ğŸ”’
  - [Chiffrement possible](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/) des accÃ¨s _REST_ mais l'_API Server_ ne peut plus dÃ©marrer automatiquement (si trÃ¨s fort besoin de sÃ©curitÃ© uniquement) ğŸ”’
- `ConfigMap` et `Secret` peuvent Ãªtre _immuable_ ğŸ”’

---

### ğŸ’¾ Stockage

- `PersistentVolume` (`PV`) => Vision _storage_ du cluster Kubernetes, un espace de stockage ğŸ’¾
- `PersistentVolumeClaim` (`PVC`) => Un type de `Volume` permettant de rÃ©quisitionner et d'utiliser un `PV` dans un `Pod` ğŸ“

---

## ğŸ› ï¸ Ressources avancÃ©es

---

### ğŸ›¡ï¸ DaemonSet

- Assure que des pods tournent sur tous les nÅ“uds du cluster ğŸ› ï¸
- Utile pour monitoring & logs ğŸ“Š
- Exemple : Installation d'un _Load Balancer_ `MetalLB` sur tous les _Node_ du cluster âš–ï¸

---

### ğŸ’¾ StatefulSet

- DÃ©ploie des applications avec Ã©tat : BDD, â€¦ ğŸ’¾
- Metadatas semblables au `Deployment` mais trÃ¨s diffÃ©rent des autres ressources en pratique (beaucoup de cas particuliers Ã  k8s)
- Ressources **ordonnÃ©es** (ordre de lancement) ğŸ“œ
- Un `PV` par _Pod_ (vs. _ReplicaSet_ oÃ¹ les volumes sont partagÃ©s) ğŸ’¾
- _Persistent volume claim templates_ (`spec.volumeClaimTemplates`) : CrÃ©e un `PVC` par _Pod_ nommÃ© `<claim-name>.<stateful-set-name>.<pod-index>` ğŸ“
- Un mÃªme volume montÃ© dans un pod (`PVC`) le reste pour toujours (mÃªme aprÃ¨s recrÃ©ation) ğŸ”„
- Un DNS dÃ©diÃ© (_service headless_) : ğŸ“¡
  - Load-balancing sur tous les pods du set âš–ï¸
  - SÃ©lection d'un pod en particulier ğŸ¯

---

### â³ Job et CronJob

- Pour travaux "longs" (> minutes / heures) â³
- `Job` : DÃ©marre un `Pod`, en cas d'Ã©chec, relance jusqu'au _backoff limit_ (default=6) ğŸ”„
  - ParamÃ¨tres : `completions` (default=1) => Nombre d'exÃ©cutions, `parallelism` (default=1) âš™ï¸
- `CronJob` : NÃ©cessite un `schedule` (idem _Cron_ sur _UNIX_) â°

---

### ğŸ’¿ Device Plugin

- Permet de dÃ©clarer des ressources au _Kubelet_ pour les utiliser des les _Pod_
- ex : `nvidia-device-plugin` pour les GPU nvidia

:::tip
Les _Device_ sont assez limitÃ©s (statiques, rÃ©quisition unique, â€¦). Kubernetes introduit les [_Dynamic Ressource Allocation_ (_DRA_)](https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/)
:::

:::link
Pour dÃ©bugger les Pods avec Device, voir : <https://kubernetes.io/blog/2025/07/03/navigating-failures-in-pods-with-devices/>
:::

---

### ğŸ” cert-manager (TLS)

- CRD Ã  ajouter au Cluster pour gÃ©nÃ©rer et signer des `Certificat` ğŸ”
- Stocke la `key` et le `crt` dans un `Secret` ğŸ”’
  - RÃ©utilisables dans `Ingress`, â€¦ ğŸŒ
- Utilise des `Issuer` (namespace-limited) ou des `ClusterIssuer` (cluster-wide) ğŸ·ï¸

---

## ğŸ› ï¸ Configuration du cluster

- `Metadata` pour chaque ressource (nom, labels, annotations, â€¦) ğŸ·ï¸
- `Namespace` : Espaces de noms isolant des ressources ğŸ·ï¸
  - Cloisonne une partie du cluster ğŸ—ï¸
  - Similaire namespace Linux ğŸ§
  - Namespaces spÃ©ciaux : ğŸ·ï¸
    - `kube-public` : Ressources accessibles Ã  tous (par ex pour le _bootstrap_ du cluster) ğŸŒ
    - `kube-system` : Composants Kubernetes ğŸ—ï¸
    - `default` : Si aucun namespace spÃ©cifiÃ© ğŸ·ï¸
- RÃ´les ğŸ‘¥

---

## ğŸ›¡ï¸ Service Mesh

- Ajoute les services d'infrastructure communs ğŸ› ï¸
  - Authentification ğŸ”
  - SÃ©curitÃ© ğŸ›¡ï¸
  - Logs ğŸ“
- GÃ¨re la communication sÃ©curisÃ©e entre conteneurs sur des architectures micro-services ğŸŒ
- Ã€ installer : `Istio`, `linkerd`, `consul`, â€¦ ğŸ› ï¸
  - Voir la [page des outils Devops](https://www.avenel.pro/tools#-kubernetes-specific) ğŸ”—

---

## ğŸ“š Commandes de base de KubernetesÂ®

Voir la [cheatsheet sur KubernetesÂ®](https://www.avenel.pro/k8s/cheatsheet) ğŸ“š

---

## Structure d'un fichier k8s

```yaml
apiVersion: v1 # Version de l'APIServer k8s
kind: â€¦ # Le type de ressource Ã  gÃ©rer : Pod, Deployment, Service, â€¦
metadata: # MÃ©tadatas de la ressource
  name: â€¦ # nom (interne) de la ressource Ã  crÃ©er et/ou monitorer
  namespace: mon-namespace # Namespace spÃ©cial (optionnel - sinon default)
  labels: # ajout de labels (optionnel)
    ma-cle: ma-valeur 
  [â€¦]
spec: # Les spÃ©cifications de la ressource. DiffÃ©rent pour chaque type de ressource
  [â€¦]
```

---
