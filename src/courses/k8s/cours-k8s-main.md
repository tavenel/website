---
license: Â© 2025 Tom Avenel under ó°µ«  BY-SA 4.0
title: Kubernetes
layout: '@layouts/CoursePartLayout.astro'
tags:
- docker
- kubernetes
- devops
---

## ğŸš€ Comparaison des Plateformes de Conteneurs

---

### ğŸŒŸ Introduction

> Une plateforme de conteneurs est un ensemble d'outils et de services qui permettent de gÃ©rer le cycle de vie des applications conteneurisÃ©es. ğŸ“¦

- **Orchestration** : Gestion automatisÃ©e du dÃ©ploiement, de la mise Ã  l'Ã©chelle, et de la mise en rÃ©seau des conteneurs. ğŸ”„
- **Ã‰volutivitÃ©** : CapacitÃ© Ã  ajuster les ressources et les services en fonction de la demande. ğŸ“ˆ
- **Isolation** : ExÃ©cution sÃ©curisÃ©e et isolÃ©e des applications pour Ã©viter les conflits. ğŸ”’
- **PortabilitÃ©** : ExÃ©cution cohÃ©rente des applications sur diffÃ©rents environnements (dÃ©veloppement, test, production ; on-premise et multi-cloud). ğŸŒ

---

### ğŸ§© Kubernetes

- **Description** : Plateforme open-source pour l'automatisation du dÃ©ploiement, la mise Ã  l'Ã©chelle et la gestion des applications conteneurisÃ©es. ğŸŒ
- De loin l'orchestrateur **le plus utilisÃ© avec DockerÂ®** ğŸ†
- **Avantages** ğŸŒŸ :
  - Grande communautÃ© et Ã©cosystÃ¨me ğŸ‘¥
  - Hautement extensible avec de nombreux outils et extensions ğŸ› ï¸
  - Prise en charge de charges de travail complexes ğŸ‹ï¸
- **InconvÃ©nients** âŒ:
  - Courbe d'apprentissage abrupte ğŸ“š
  - Configuration complexe âš™ï¸
- Pour les **dÃ©ploiements complexes et Ã©volutifs** ğŸŒ

---

### ğŸš€ OpenShift

- **Description** : Plateforme de conteneurs de Red Hat, basÃ©e sur Kubernetes, avec des fonctionnalitÃ©s supplÃ©mentaires pour les entreprises. ğŸ¢
- **Avantages** ğŸŒŸ :
  - IntÃ©gration facile avec d'autres produits Red Hat ğŸ”„
  - Interface utilisateur intuitive ğŸ–¥ï¸
  - SÃ©curitÃ© et conformitÃ© renforcÃ©es ğŸ”’
- **InconvÃ©nients** âŒ:
  - CoÃ»t Ã©levÃ© pour les fonctionnalitÃ©s d'entreprise ğŸ’°
  - Moins flexible que Kubernetes seul ğŸ¤¸
- Pour les **solutions d'entreprise avec support** ğŸ¢

---

### ğŸ³ Docker Swarm

- **Description** : Solution d'orchestration de conteneurs intÃ©grÃ©e Ã  Docker, simple et facile Ã  utiliser. ğŸ‹
- **Avantages** ğŸŒŸ :
  - IntÃ©gration transparente avec Docker ğŸ”„
  - Facile Ã  configurer et Ã  utiliser ğŸ› ï¸
  - IdÃ©al pour les petits dÃ©ploiements ğŸ 
- **InconvÃ©nients** âŒ:
  - Manque de fonctionnalitÃ©s avancÃ©es ğŸ›‘
  - CommunautÃ© et Ã©cosystÃ¨me plus petits ğŸ‘¥
- Pour les **environnements simples et rapides** ğŸ¡

---

### ğŸ—ï¸ Apache Mesos

- **Description** : Projet open-source pour la gestion des ressources dans les centres de donnÃ©es, prenant en charge les conteneurs et les charges de travail non conteneurisÃ©es. ğŸ¢
- **Avantages** ğŸŒŸ :
  - FlexibilitÃ© pour gÃ©rer divers types de charges de travail ğŸ”„
  - Ã‰volutivitÃ© et robustesse ğŸ“ˆ
- **InconvÃ©nients** âŒ:
  - ComplexitÃ© de configuration et de gestion âš™ï¸
  - Moins axÃ© sur les conteneurs que les autres solutions ğŸ¯
- Pour les **environnements hybrides et complexes** ğŸ—ï¸

---

### ğŸ“Š Comparaison

| Plateforme | FacilitÃ© d'utilisation | Ã‰volutivitÃ© | Ã‰cosystÃ¨me | CoÃ»t |
|------------|-----------------------|-------------|------------|------|
| Kubernetes | â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | ğŸ†“ |
| OpenShift | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸ | ğŸ’° |
| Swarm | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸ | ğŸ†“ |
| Mesos | â­ï¸â­ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ | â­ï¸â­ï¸ | ğŸ†“ |

---

## ğŸ­ PrÃ©sentation de Kubernetes

---

`Kubernetes` (ou `k8s`) est un orchestrateur de dÃ©ploiement et de gestion de conteneurs applicatifs dans un cluster de machines virtuelles. ğŸš€

* IndÃ©pendant de DockerÂ® mais mÃªme runtime `containerd` => peut tourner les mÃªmes images ğŸ³
* Configure et gÃ¨re un cluster applicatif complexe : nÅ“uds du cluster, rÃ©seau, stockage, ... ğŸŒ
* PossibilitÃ© de gÃ©rer tout le cluster via API `kubectl` ğŸ”§
* Mais configuration recommandÃ©e via `Yaml` / `Json` pour audit ğŸ“

---

## ğŸ’¡ Recommandations

* `DockerÂ®` seul / `docker compose` pour CI/CD et outils internes ğŸ› ï¸
* `k8sÂ®` pour gestion applicative de l'environnement de production ğŸ—ï¸
* `k8sÂ®` duplique des fonctionnalitÃ©s de DockerÂ® => prÃ©fÃ©rer 100% DockerÂ® ou k8sÂ® ğŸ”„

---

## ğŸ“¦ Technologies de conteneurs supportÃ©es

1. `containerd` : projet open-source crÃ©Ã© pour Kubernetes (runtime de `Docker` : _Docker sans la CLI_) ğŸ³
2. `Docker Engine` : _Docker avec la CLI_ ğŸ³
3. `Podman` : alternative _serverless_ Ã  Docker ğŸ³
4. `CRI-O` : conteneurs lÃ©gers ğŸ“¦
5. `Mirantis Container Runtime (MCR)` (anciennement _Docker Enterprise_) ğŸ¢

---

```mermaid
---
title: Architecture des technologies de conteneurs
---
flowchart TD
    A["Docker (CLI)"]
    B[Kubernetes]
    
    A --> ContainerD
    B --> CRI

    CRI["Container Runtime Interface (CRI)<br/>[Kubernetes API]"]
    CRI --> ContainerD
    CRI --> CRIO

    ContainerD["ContainerD (pull images, network, storage)"]
    CRIO[CRI-O]

    ContainerD --> OCI
    CRIO --> OCI

    OCI["Open Container Interface (OCI) spec"]
    OCI --> runC

    runC["runC (create / run containers)"]
    runC --> Container[Container]
```

---

## ğŸŒ Plugin rÃ©seau (CNI)

- **Container Networking Interface** (_CNI_) : ğŸŒ
  - Permet la communication rÃ©seau au sein du cluster ğŸŒ
  - Parfois intÃ©grÃ© Ã  la distribution, sinon Ã  installer sÃ©parÃ©ment ğŸ› ï¸
  - [GitHub - CNI](https://github.com/containernetworking/cni/) ğŸ”—
  - Par dÃ©faut, _Kubelet_ charge les configurations des plugins rÃ©seau depuis : `/etc/cni/net.d` ğŸ“‚

---

### ğŸ”„ CNI (Kubernetes) vs CNM (Docker)

- **Docker** : ğŸ³
  - RÃ©seaux **multiples** et **isolÃ©s** ğŸŒ
  - DNS par **rÃ©seau** ğŸ“¡
  - **Pas d'interconnexion** des rÃ©seaux âŒ

- **Kubernetes** : ğŸš€
  - **1 seul** rÃ©seau de conteneurs (_flat_) ğŸŒ
  - DNS par **`Namespace`** ğŸ“¡
  - **Aucune isolation** des rÃ©seaux par dÃ©faut (utiliser des `NetworkPolicies`) âš ï¸

---

### ğŸŒ Flannel

- Est un rÃ©seau de sous-rÃ©seaux pour Kubernetes ğŸŒ
- Fonctionne avec divers backends (VXLAN, UDP, etc.) ğŸ”„
- Offre une isolation rÃ©seau par pod ğŸ”’
- Plus simple Ã  configurer que les autres options ğŸ› ï¸
- InconvÃ©nients : Peut introduire une latence supplÃ©mentaire, moins de fonctionnalitÃ©s avancÃ©es (`NetworkPolicies`, â€¦), moins adaptÃ© aux trÃ¨s grands clusters âš ï¸

---

### ğŸ›¡ï¸ Calico

- Supporte plusieurs modes de rÃ©seau : BGP, IPIP, VXLAN ğŸŒ
- Propose une isolation rÃ©seau granulaire (par pod) ğŸ”’
- IntÃ¨gre de la sÃ©curitÃ© ğŸ›¡ï¸
- ConÃ§u pour des (trÃ¨s) grands clusters ğŸ—ï¸
- S'intÃ¨gre bien avec l'infrastructure existante ğŸ”„
- Souvent utilisÃ© dans les dÃ©ploiements Cloud â˜ï¸
- InconvÃ©nients : Complexe, besoin de compatibilitÃ© rÃ©seau (BGP) âš ï¸

---

### ğŸ•¸ï¸ Weave

- CrÃ©e un rÃ©seau virtuel entre tous les conteneurs ğŸŒ
- Utilise le DNS intÃ©grÃ© de Docker ğŸ“¡
- Propose une isolation rÃ©seau par pod ğŸ”’
- Facile Ã  configurer mais peut Ãªtre moins performant que les autres options ğŸ› ï¸

---

### âš¡ Cilium

- Utilise _eBPF_ (_Berkeley Packet Filter_) âš¡
  - (TrÃ¨s) performant, dÃ©bit Ã©levÃ© et latence rÃ©duite âš¡
- MÃ©triques dÃ©taillÃ©es sur le trafic rÃ©seau ğŸ“Š
- Supporte dynamiquement l'ajout et la suppression de nÅ“uds ğŸ”„
- ConÃ§u pour gÃ©rer des clusters de grande taille ğŸ—ï¸
- InconvÃ©nients : Complexe (eBPF et concepts rÃ©seau avancÃ©s), eBPF doit Ãªtre activÃ© dans le noyau Linux âš ï¸

:::tip
- Cilium fournit un outil de monitoring (_Hubble_) avec une CLI et UI permettant de visualiser les communications au sein du cluster.
- Cilium fournit un "_Cluster Mesh_" (âš ï¸ Ã  ne pas confondre avec un _Service Mesh_ k8s) permettant une communication entre _Service_ de diffÃ©rents clusters.
:::

---

| CritÃ¨re | Calico | Flannel | Weave Net | Cilium |
|-------------|------------|-------------|---------------|------------|
| **Type de RÃ©seau** | Couche 3 (IPIP, BGP, VXLAN) | Couche 3 (VXLAN, UDP) | Couche 2 (Overlay) | Couche 3 (eBPF) |
| **SÃ©curitÃ©** | Politiques de rÃ©seau granulaires | Politiques de rÃ©seau basiques | Politiques de rÃ©seau basiques | Politiques de rÃ©seau granulaires |
| **Performance** | Haute | Moyenne | Moyenne | TrÃ¨s haute |
| **ScalabilitÃ©** | TrÃ¨s Ã©levÃ©e | Moyenne | Moyenne | TrÃ¨s Ã©levÃ©e |
| **ComplexitÃ©** | Moyenne Ã  Ã©levÃ©e | Faible | Faible Ã  moyenne | Ã‰levÃ©e |
| **FonctionnalitÃ©s** | AvancÃ©es (BGP, IPIP, VXLAN) | Basiques | Basiques Ã  moyennes | AvancÃ©es (eBPF, DNS, chiffrement) |
| **CompatibilitÃ©** | Kubernetes, OpenShift, Docker | Kubernetes, Docker | Kubernetes, Docker, Mesos | Kubernetes |
| **RÃ©silience** | Ã‰levÃ©e | Moyenne | Ã‰levÃ©e | Ã‰levÃ©e |

---

## ğŸ“¦ Distributions Kubernetes

---

1. **Kubeadm** ğŸ› ï¸
   - Outil officiel
   - Installation de chaque composant sÃ©parÃ©ment
   - Le plus configurable mais le plus complexe

---

2. **Kubespray** ğŸ”„
   - Utilise `Ansible` pour (re)dÃ©ployer automatiquement un cluster
   - Compatible _bare-metal_ et _cloud_ â˜ï¸

---

3. **Rancher (RKE)** ğŸ—ï¸
   - Plateforme complÃ¨te pour gÃ©rer des clusters Kubernetes
   - Propose des fonctionnalitÃ©s avancÃ©es comme la gestion multi-cluster ğŸŒ
   - Offre une interface graphique intuitive ğŸ–¥ï¸

---

4. **K3s (Rancher Labs)** ğŸ“¦
   - Version allÃ©gÃ©e de Kubernetes conÃ§ue pour les environnements embarquÃ©s
   - Consomme moins de ressources que Kubernetes standard ğŸ”‹
   - IdÃ©al pour les systÃ¨mes Ã  faible puissance âš¡
   - Utilise le CNI `flannel` ğŸŒ
   - Voir aussi : _k3d_ (_k3s in Docker_) : similaire _kind_ (voir ci-dessous) pour k3s ğŸ³

---

5. **K0s (CNCF)** ğŸ“¦
   - Autre version allÃ©gÃ©e Kubernetes
   - TrÃ¨s minimale, aucun composant additionnel ğŸ”§
   - Compatible on-premise, edge, IoT, â€¦ ğŸŒ

---

6. **OpenShift** ğŸ¢
   - Distribution propriÃ©taire de Red Hat basÃ©e sur Kubernetes
   - Inclut des fonctionnalitÃ©s supplÃ©mentaires comme l'orchestration d'applications ğŸ› ï¸
   - Forte sÃ©curitÃ© et conformitÃ© ğŸ”’

---

7. **Docker Kubernetes Service (DKS)** ğŸ³
   - Surveillance intÃ©grÃ©e du cluster et des applications ğŸ‘ï¸
   - Nombreux drivers storage ğŸ’¾

---

8. **MicroK8s (Ubuntu)** ğŸ“¦
   - Distribution lÃ©gÃ¨re et sÃ©curisÃ©e de Kubernetes
   - ConÃ§ue pour les environnements Ubuntu ğŸ§
   - Propose des fonctionnalitÃ©s avancÃ©es comme l'installation de paquets ğŸ“¦

---

9. **Minikube** ğŸ§ª
   - Version lÃ©gÃ¨re pour le dÃ©veloppement et le test
   - Fonctionne sur un seul ordinateur ğŸ’»
   - IdÃ©al pour dÃ©butants et environnement de dÃ©veloppement ğŸ› ï¸

---

10. **Docker Desktop** ğŸ³
    - IntÃ¨gre Kubernetes nativement
    - Offre une expÃ©rience utilisateur simplifiÃ©e ğŸ–¥ï¸
    - AdaptÃ© aux dÃ©veloppeurs utilisant Docker ğŸ› ï¸

---

11. **Kind (Kubernetes IN Docker)** ğŸ§ª
    - DÃ©ploie Kubernetes dans un conteneur pour le dÃ©veloppement et le test
    - CrÃ©e rapidement un ou plusieurs clusters localement ğŸ—ï¸
    - Utile pour tester plusieurs clusters : upgrade, changements d'infrastructure, â€¦ ğŸ”„
    - CNI custom : `kindnetd` ğŸŒ
    - Utilise `kubeadm` ğŸ› ï¸

---

12. **Talos Linux** ğŸ§
    - Distribution Linux dÃ©diÃ©e
    - OS immuable : pas de SSH, shell, â€¦ ğŸ”’

---

### â˜ï¸ Plateformes managÃ©es

- Amazon Elastic Kubernetes Service (EKS) ğŸŒ
- Google Kubernetes Engine (GKE) ğŸŒ
- Azure Kubernetes Services (AKS) ğŸŒ
- Oracle Kubernetes Engine (OKE) ğŸŒ
- IBMCloud K8s ğŸŒ
- OVHCloud K8s ğŸŒ

---

## ğŸ—ï¸ Architecture

---

### ğŸ› ï¸ Installation

- `kubeadm` : l'outil officiel (installation de chaque composant sÃ©parÃ©ment) ğŸ› ï¸
- IntÃ©grÃ© dans la distribution : `k3s`, `minikube`, `microk8s`, â€¦ ğŸ“¦
- Versions managÃ©es : outils dÃ©diÃ©s au fournisseur de Cloud â˜ï¸

---

### ğŸ“‚ ModÃ¨le

- Un cluster k8s est composÃ© de plusieurs `Node` ğŸŒ
- Chaque `Node` fait tourner des `Pod` (ensemble de conteneurs - c'est l'unitÃ© atomique de k8s !) ğŸ“¦
- Un `Deployment` gÃ¨re _dÃ©clarativement_ des ressources Ã  dÃ©ployer (pods, replicas, mise Ã  jour, â€¦) ğŸ”„
- Un `Service` permet d'exposer les ports d'un pod (interne ou externe) ğŸŒ
  - _Aucun lien avec un `service` de `docker-compose` !_ âš ï¸

---

### ğŸ·ï¸ Types de Nodes

- Node de rÃ´le `master` : le `control pane`, gÃ¨re le cluster (orchestration, API server, â€¦) ğŸ¢
- Node de type `worker` (sans rÃ´le) : exÃ©cute les pods et leur fournit les ressources ğŸ› ï¸

---

### âŒLimites

- k8s est fait pour gÃ©rer de gros clusters : ğŸ—ï¸
- Limitations Kubernetes v1.31 :
  - < 5,000 Node ğŸŒ
  - < 110 Pod / Node ğŸ“¦
  - < 150,000 Pod (total) ğŸ“¦
  - < 300,000 Containers (total) ğŸ“¦

---

![Architecture d'un cluster Kubernetes](https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg)

<div class="caption">Architecture d'un cluster Kubernetes (source: kubernetes.io)</div>

---

```mermaid
---
title: Architecture d'un Pod
---
flowchart TD
    subgraph Node
        subgraph "Pod : 1 adresse IP"
            Logger[Conteneur docker : logger]
            Nginx[Conteneur docker : nginx]
        end
    end

  class Logger,Nginx blue
```

<div class="caption">Architecture d'un Pod</div> 

---

## ğŸ§© Composants

- `APIServer` : API de gestion du cluster ğŸŒ
- `etcd` : Stockage de la configuration du cluster ğŸ“‚
- `Controller Manager` : GÃ¨re les `WorkerNode` depuis le `MasterNode` ğŸ¢
- `Kubelet` : ExÃ©cute et gÃ¨re les conteneurs sur les `Node` ğŸ“¦
- `Kube-proxy` : Ã‰quilibre le trafic sur chaque `Node` ğŸŒ
- `Scheduler` : Assigne les `Pod` Ã  un `Node` ğŸ“…

---

### ğŸ“‚ etcd

- Backend k8s : Ã‰tat du cluster (le reste est stateless) ğŸ“‚
  - Store clÃ©=valeur ğŸ”‘
- Dans ou en dehors du cluster ğŸŒ
- 1 leader (par consensus) ğŸ†
  - DÃ©ployer un nombre impair d'instances ğŸ”¢
  - Supporte N/3 instances dÃ©faillantes âš ï¸
- Jamais utilisÃ© directement (`APIServer`) âš ï¸
- Critique ! ğŸš¨
  - Machine dÃ©diÃ©e ou environnement isolÃ© ğŸ¢
  - Bonnes performances rÃ©seau / disque âš¡

---

### ğŸ”„ ControllerManager

- Compare l'Ã©tat dÃ©sirÃ© (dÃ©claratif) Ã  l'Ã©tat actuel ğŸ”„
- En dÃ©duit (et applique) les actions nÃ©cessaires (`APIServer`) ğŸ› ï¸
- Beaucoup de contrÃ´leurs diffÃ©rents ğŸ§©
  - PossibilitÃ© d'installer des contrÃ´leurs externes pour gÃ©rer de nouvelles ressources (`Custom Resource Definition`) ğŸ”§
  - Exemple : Load Balancer AWS, â€¦ â˜ï¸
- Boucles de rÃ©conciliation : ğŸ”„
  - Reconstruit des ressources si besoin pendant le cycle de vie du cluster ğŸ”„
  - Sans besoin d'intervention ğŸ› ï¸
- Contient toute l'intelligence de Kubernetes ğŸ§ 

---

```mermaid
---
title: "Boucle de rÃ©conciliation"
---
flowchart TD
    Observation --> Diff
    Diff --> Action
    Action --> Observation

```

---

### ğŸ“… Scheduler

- Assigne les `Pod` (en state: `Pending`) aux `Node` ğŸ“…
  - Techniquement : crÃ©e un `Binding` et change le `nodeName` du `Pod` ğŸ”§
- Calcule de score par _filtrage_ puis _score_ : ğŸ“Š
  1. _Filtrage_ : CapacitÃ©, tolÃ©rance, affinitÃ©, sÃ©lecteurs, â€¦ ğŸ”
  2. _Score_ : Load-balancing, â€¦ âš–ï¸
- PossibilitÃ© d'installer un `Scheduler` customisÃ© ğŸ”§

---

### ğŸ“¦ Kubelet

- 1 `Kubelet` par `Node` ğŸ“¦
  - Un `kubelet` est souvent installÃ© sur le `MasterNode` pour y gÃ©rer ses composants dans des pods (optionnel) ğŸ¢
  - En gÃ©nÃ©ral, on y ajoute le `taint` : `node-role.kubernetes.io/master:NoSchedule` pour ne pas utiliser le _Master_ comme un _Worker_. âš ï¸
- Connexion permanente Ã  l'`APIServer` ğŸŒ
- DÃ©ploie le `Pod` s'il a le `nodeName` du `Node` courant : ğŸ“¦
  1. RÃ©cupÃ©ration de l'image (format `OCI`) ğŸ“¥
  2. CrÃ©ation des ressources : `Volumes`, `Networks`, `Containers` ğŸ› ï¸
  3. Ã‰tats du `Pod` : `pending` -> `running` / `failed` -> `succeeded` (terminÃ©) ğŸ”„
  4. Remonte l'information Ã  l'`APIServer` ğŸ“¤

---

### ğŸŒ Kube-proxy

- GÃ¨re le rÃ©seau sur chaque `Node` (entre Pods et vers extÃ©rieur) ğŸŒ
- Plusieurs modes : ğŸ”„
  - Tout trafic par `iptables`, rÃ¨gles `DNAT` (âš ï¸ CPU si beaucoup de rÃ¨gles) âš ï¸
    - Load-balancer : _round-robin_ ğŸ”„
  - `ipvs` : Module noyau gÃ©rant un ensemble de rÃ¨gles d'un coup (plus performant) âš¡
    - Load-balancer avancÃ© âš–ï¸
  - Si CNI `Cilium` : RÃ¨gles `eBPF` dans le noyau, plus besoin de `Kube-proxy` ğŸŒŸ
    - Voir section sur les CNI ğŸ“š
- Connexion entre `Pods` : Niveau 3 (_IP_) ğŸŒ
- Connexion par `Services` : Niveau 4 (_TCP_, _UDP_) ğŸŒ
- Connexion par `Ingress` : Niveau 7 (_HTTP_) ğŸŒ

---

Voir : <https://2021-05-enix.container.training/5.yml.html#50> pour un exemple de fonctionnement du _Control Plane_ suite Ã  la crÃ©ation d'un `Deployment`

---

## ğŸ› ï¸ Gestion du cluster

- Fichiers de configuration `yml` (Ã  privilÃ©gier autant que possible !) ğŸ“„
- Interface en ligne de commande `kubectl` (surtout pour lancer les fichiers de config) ğŸ–¥ï¸
- Interface web (peu utilisÃ©e) ğŸŒ

---

## ğŸ“‚ Ressources basiques du cluster

---

### ğŸ”„ Interactions entre ressources

- Les `Pod` exÃ©cutent les microservices. ğŸ“¦
- Les `Service` exposent ces pods pour permettre leur communication et leur accÃ¨s. ğŸŒ
- Les `ConfigMap` et `Secret` injectent les configurations et les donnÃ©es sensibles. ğŸ”
- Le/Les `Ingress` gÃ¨rent le trafic externe (routage par _URI_ ou header _host_) et les certificats SSL/TLS. ğŸŒ
- Les `PersistentVolume` et `StatefulSet` supportent les applications avec Ã©tat. ğŸ’¾
- Les `DaemonSet` assurent le fonctionnement des outils dâ€™administration sur chaque nÅ“ud. ğŸ› ï¸

---

### ğŸ“¦ Gestion des applications

- `Deployment` : GÃ¨re le dÃ©ploiement d'un `ReplicaSet` ğŸ“¦
  - Et la mise Ã  jour des applications (rolling update, rollback, scaling) ğŸ”„
- `ReplicaSet` : CrÃ©e et gÃ¨re le suivi (rÃ©plicas) d'un pod ğŸ“¦
  - Ne pas utiliser de `ReplicaSet` directement mais passer par un `Deployment` (plus puissant) ğŸ› ï¸
- `Pod` : GÃ¨re un ensemble de conteneurs partageant la mÃªme isolation : stack rÃ©seau, stockage, â€¦ ğŸ“¦
  - DÃ©marrÃ© directement ou (mieux) par un `deployment` crÃ©ant un `ReplicaSet` ğŸ“¦
  - Ã‰phÃ©mÃ¨re : Pas de donnÃ©es critiques dans le pod âš ï¸
  - 1 IP par pod partagÃ©e entre tous les conteneurs (mais l'IP peut changer) ğŸŒ
    - AccÃ¨s par `localhost` aux autres conteneurs et **partage des ports ouverts** ğŸ”„

---

```mermaid
---
title: Deployment, ReplicaSet et Pods
---
flowchart TD
    subgraph "Deployment : replicas = 2"
        subgraph "ReplicaSet : 2 Pods"
            Pod1[Pod 1 : nginx]
            Pod2[Pod 2 : nginx]
        end
    end

    class Pod1,Pod2 green
```

<div class="caption">Un Deployment gÃ©rant un ReplicaSet gÃ©rant un Pod</div> 

---

### ğŸ·ï¸ Labels

- Attributs clÃ©=valeur des objets du cluster ğŸ·ï¸
- UtilisÃ© par Kubernetes ğŸ› ï¸
- `NodeSelector` : Lance un pod sur un `Node` ayant ce label ğŸ·ï¸
- `NodeAffinity` : DÃ©crit des affinitÃ©s entre un `Pod` et un `Node` ğŸ·ï¸
- `podAffinity`, `podAntiAffinity` : (Anti)affinitÃ© entre `Pod` ğŸ·ï¸
- Il existe aussi des `annotations` : Idem mais NON utilisÃ© par k8s ensuite ğŸ“

---

#### ğŸ› Labels et debug

- Beaucoup de ressources utilisent les labels pour sÃ©lectionner les ressources (`Pod`, â€¦) Ã  manager ğŸ·ï¸
- Pour debugger un `Pod` fautif, on peut changer son `Label` : ğŸ›
  - Le Pod fautif sera retirÃ© du Service (plus de Load balancing) âš–ï¸
  - Un nouveau Pod est crÃ©Ã© par le `ReplicaSet` ou le `DaemonSet` ğŸ“¦
  - Le Pod fautif est toujours actif pour du debug ğŸ›

---

### ğŸŒ Service

- Service DNS permettant d'accÃ©der Ã  1 (ou plusieurs) Pods ğŸŒ
  - Nom DNS court (dans le namespace) : `<service_name>.<namespace>` (ou `<service_name>` si dans le mÃªme `namespace`) ğŸ“¡
  - Nom DNS complet : `<service_name>.<namespace>.svc.<cluster-domain>` ğŸ“¡
  - Exemple : `mon_service.mon_namespace.svc.mon_cluster` ğŸ“¡
- Association `Service` <-> `Pod`(s) grÃ¢ce aux _labels_ ğŸ·ï¸
  - **Avec gestion des rÃ©plicas** ğŸ”„
- Au moins 2 CIDR (plages rÃ©seau) : CIDR Pod et CIDR Services ğŸŒ

---

### ğŸŒ Service: ClusterIP

- Expose Ã  l'intÃ©rieur du cluster uniquement ğŸ¢
- CrÃ©e une Virtual IP ğŸŒ
- AccÃ¨s via le nom du service ğŸ“¡
- Load balancer interne sur les Pods âš–ï¸

---

```mermaid
---
title: ClusterIP Multi-Nodes
---
flowchart TD

  subgraph Cluster ["Cluster"]

    svcA["Service blue<br/>clusterIP 10.0.0.5<br/>port 81"]
    svcB["Service green<br/>clusterIP 10.0.0.7<br/>port 82"]
    class svcA blue
    class svcB green

    subgraph node1 ["Node 1"]
        pod1_1["pod-blue-1<br/>10.4.32.2<br/>port 8181"]
        class pod1_1 blue
    end

    subgraph node2 ["Node 2"]
        pod2_1["pod-blue-2<br/>10.4.32.5<br/>port 8181"]
        pod2_2["pod-green-1<br/>10.4.32.6<br/>port 8282"]
        class pod2_1 blue
        class pod2_2 green
    end

    subgraph node3 ["Node 3"]
        pod3_1["pod-green-2<br/>10.4.32.8<br/>port 8282"]
        class pod3_1 green
    end

    svcA -.-> pod1_1
    svcA -.-> pod2_1

    svcB -.-> pod2_2
    svcB -.-> pod3_1

  end
```

<div class="caption">ClusterIP multi Nodes</div>

---

```mermaid
---
title: Communication entre Pods par ClusterIP - par Pod 1
---
flowchart TD

    svcB["Service green<br/>clusterIP 10.0.0.7<br/>port 82"]
    class svcB green

    subgraph Cluster ["Cluster"]

        subgraph node1 ["Node 1"]
            pod1_1["pod-blue-1<br/>10.4.32.2<br/>port 8181"]
            class pod1_1 blue
        end

        subgraph node2 ["Node 2"]
            pod2_1["pod-blue-2<br/>10.4.32.5<br/>port 8181"]
            pod2_2["pod-green-1<br/>10.4.32.6<br/>port 8282"]
            class pod2_1 blue
            class pod2_2 green
        end

        subgraph node3 ["Node 3"]
            pod3_1["pod-green-2<br/>10.4.32.8<br/>port 8282"]
            class pod3_1 green
        end
    end

    %% Connexions services â†’ pods
    svcB -.-> pod2_2
    svcB -.-> pod3_1

    %% Communication entre pod et service
    pod1_1 e1@-->|"1 - http:// green:82"| svcB
    svcB e2@-->|"2 - http:// 10.4.32.6:8282"| pod2_2

    e1@{ animate : true }
    e2@{ animate : true }
```

```mermaid
---
title: Communication entre Pods par ClusterIP - par Pod 2
---
flowchart TD

    svcB["Service green<br/>clusterIP 10.0.0.7<br/>port 82"]
    class svcB green

    subgraph Cluster ["Cluster"]

        subgraph node1 ["Node 1"]
            pod1_1["pod-blue-1<br/>10.4.32.2<br/>port 8181"]
            class pod1_1 blue
        end

        subgraph node2 ["Node 2"]
            pod2_1["pod-blue-2<br/>10.4.32.5<br/>port 8181"]
            pod2_2["pod-green-1<br/>10.4.32.6<br/>port 8282"]
            class pod2_1 blue
            class pod2_2 green
        end

        subgraph node3 ["Node 3"]
            pod3_1["pod-green-2<br/>10.4.32.8<br/>port 8282"]
            class pod3_1 green
        end
    end

    %% Connexions services â†’ pods
    svcB -.-> pod2_2
    svcB -.-> pod3_1

    %% Communication entre pod et service
    pod1_1 e1@-->|"1 - http:// green:82"| svcB
    svcB e2@-->|"2 - http:// 10.4.32.8:8282"| pod3_1

    e1@{ animate : true }
    e2@{ animate : true }
```

<div class="caption">Communication entre Pods par ClusterIP. Le service Green est load-balancÃ© sur pod-green-1 et pod-green-2.</div>

---

### ğŸŒ Service: NodePort

- Extension du `ClusterIP` ğŸŒ
- Expose Ã  l'extÃ©rieur du cluster ğŸŒ
- AccÃ¨s via des ports sur les Nodes du cluster ğŸŒ
- Load balancer interne sur les Pods âš–ï¸

---

```mermaid
---
title: NodePort sur port 30001 du Node 1
---
flowchart TD

    svcA["Service blue<br/>NodePort 10.0.0.5<br/>port 81<br/>nodePort 30001"]
    svcB["Service green<br/>NodePort 10.0.0.7<br/>port 82<br/>nodePort 30002"]
    class svcA blue
    class svcB green

    subgraph node1 ["Node 1<br/>172.10.10.1<br/>:30001 :30002"]
        pod1_1["pod-blue-1<br/>10.4.32.2<br/>port 8181"]
        class pod1_1 blue
    end

    subgraph node2 ["Node 2<br/>172.10.10.2<br/>:30001 :30002"]
        pod2_1["pod-blue-2<br/>10.4.32.5<br/>port 8181"]
        pod2_2["pod-green-1<br/>10.4.32.6<br/>port 8282"]
        class pod2_1 blue
        class pod2_2 green
    end

    subgraph node3 ["Node 3<br/>172.10.10.3<br/>:30001 :30002"]
        pod3_1["pod-green-2<br/>10.4.32.8<br/>port 8282"]
        class pod3_1 green
    end

   %% Connexions services â†’ pods
    svcA -.-> pod1_1
    svcA -.-> pod2_1

    svcB -.-> pod2_2
    svcB -.-> pod3_1

    %% Utilisateur externe
    User(["User"])

    %% Connexions externes
    User e1@--> |"1 - http:// 127.10.10.1:30001"| node1
    User e2@--> |"2 - http:// pod-blue-1:8181"| pod1_1

    e1@{ animate : true }
    e2@{ animate : true }
```

```mermaid
---
title: NodePort sur port 30002 du Node 1
---
flowchart TD

    svcA["Service blue<br/>NodePort 10.0.0.5<br/>port 81<br/>nodePort 30001"]
    svcB["Service green<br/>NodePort 10.0.0.7<br/>port 82<br/>nodePort 30002"]
    class svcA blue
    class svcB green

    subgraph node1 ["Node 1<br/>172.10.10.1<br/>:30001 :30002"]
        pod1_1["pod-blue-1<br/>10.4.32.2<br/>port 8181"]
        class pod1_1 blue
    end

    subgraph node2 ["Node 2<br/>172.10.10.2<br/>:30001 :30002"]
        pod2_1["pod-blue-2<br/>10.4.32.5<br/>port 8181"]
        pod2_2["pod-green-1<br/>10.4.32.6<br/>port 8282"]
        class pod2_1 blue
        class pod2_2 green
    end

    subgraph node3 ["Node 3<br/>172.10.10.3<br/>:30001 :30002"]
        pod3_1["pod-green-2<br/>10.4.32.8<br/>port 8282"]
        class pod3_1 green
    end

   %% Connexions services â†’ pods
    svcA -.-> pod1_1
    svcA -.-> pod2_1

    svcB -.-> pod2_2
    svcB -.-> pod3_1

    %% Utilisateur externe
    User(["User"])

    %% Connexions externes
    User e1@--> |"1 - http:// 127.10.10.1:30002"| node1
    User e2@--> |"2 - http:// pod-green-2:8181"| pod3_1

    e1@{ animate : true }
    e2@{ animate : true }
```

<div class="caption">Communication par NodePort. La communication vers l'adresse IP du Node est redirigÃ©e vers un Pod du Service.</div>

---

### ğŸŒ Service: LoadBalancer

- LoadBalancer pour l'accÃ¨s au `Pod` depuis l'extÃ©rieur ğŸŒ
  - IdÃ©alement directement, sinon par un `NodePort` ğŸŒ
- Permet d'avoir un accÃ¨s unique Ã  plusieurs conteneurs d'un Pod tournant sur plusieurs Nodes ğŸŒ
- LiÃ© au service de _Load Balancing_ **externe** du Cloud Provider (_ELB_, _Azure LB_, _GCLB_, â€¦) â˜ï¸
  - Dans le cluster : idem `ClusterIP` ğŸŒ
  - Programme un _Load Balancer_ Cloud puis ajoute l'IP **externe** au `Service` ğŸŒ
  - On-premise, installer `MetalLB` ğŸ¢

---

### ğŸŒ Service: ExternalName

- RÃ©fÃ©rence un DNS interne ou externe (alias) ğŸ“¡
- Exemple : BDD externe au cluster ğŸ’¾
- Pas de Load balancer âš–ï¸

---

### ğŸ”— EndpointSlice

- Lien `Service` <-> `Pod` ğŸ”—

---

### ğŸŒ Ingress

- Point d'accÃ¨s publique HTTP/HTTPS unique pour l'accÃ¨s aux diffÃ©rentes Pods (diffÃ©rent d'un Service) ğŸŒ
- Agit comme un _Reverse-proxy_ qui redirige la requÃªte vers le `Service` ğŸ”„
- RÃ¨gles de routage avancÃ©es ğŸ“œ
- En principe, crÃ©e un service `LoadBalancer` (point d'entrÃ©e de l'Ingress) âš–ï¸
- Requiert une implÃ©mentation d'`Ingress Controller` Ã  installer : ğŸ› ï¸
  - `Nginx Ingress Controller` : Standard, stable, supporte HTTPS et annotations avancÃ©es ğŸŒ
  - `HAProxy Ingress` : Performant âš¡
  - `Traefik` : LÃ©ger, dynamique (cloud, microservices) â˜ï¸
  - `Consul Ingress / Istio Gateway` : IntÃ©gration avec les _service mesh_ Consul / Istio ğŸŒ

---

```mermaid
---
title: Schema d'un Ingress basÃ© path.
---

graph LR;
  client([client])-. Ingress-managed load balancer .->ingress[Ingress, 178.91.123.132];
  ingress-->|/foo|service1[Service service1:4200];
  ingress-->|/bar|service2[Service service2:8080];
  subgraph cluster
  ingress;
  service1-->pod1[Pod];
  service1-->pod2[Pod];
  service2-->pod3[Pod];
  service2-->pod4[Pod];
  end
```

```mermaid
---
title: Schema d'un Ingress basÃ© hostname.
---

graph LR;
  client([client])-. Ingress-managed load balancer .->ingress[Ingress, 178.91.123.132];
  ingress-->|Host: foo.bar.com|service1[Service service1:80];
  ingress-->|Host: bar.foo.com|service2[Service service2:80];
  subgraph cluster
  ingress;
  service1-->pod1[Pod];
  service1-->pod2[Pod];
  service2-->pod3[Pod];
  service2-->pod4[Pod];
  end
```

<div class="caption">Source: <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></div>

---

## ğŸ” cert-manager (TLS)

- CRD Ã  ajouter au Cluster pour gÃ©nÃ©rer et signer des `Certificat` ğŸ”
- Stocke la `key` et le `crt` dans un `Secret` ğŸ”’
  - RÃ©utilisables dans `Ingress`, â€¦ ğŸŒ
- Utilise des `Issuer` (namespace-limited) ou des `ClusterIssuer` (cluster-wide) ğŸ·ï¸

---

## ğŸ›¡ï¸ Service Mesh

- Ajoute les services d'infrastructure communs ğŸ› ï¸
  - Authentification ğŸ”
  - SÃ©curitÃ© ğŸ›¡ï¸
  - Logs ğŸ“
- GÃ¨re la communication sÃ©curisÃ©e entre conteneurs sur des architectures micro-services ğŸŒ
- Ã€ installer : `Istio`, `linkerd`, `consul`, â€¦ ğŸ› ï¸
  - Voir la [page des outils Devops](https://www.avenel.pro/tools#-kubernetes-specific) ğŸ”—

---

## ğŸŒ Gateway API

- Nouvelle API Kubernetes (successeur Ingress) ğŸŒ
  - OrientÃ© rÃ´les, portable, extensible ğŸ”„
  - Routage multi-namespace ğŸ·ï¸
  - DÃ©corrÃ©lÃ© de l'installation de Kubernetes ğŸ› ï¸
- `GatewayClass` : Ensemble de `Gateway` avec configuration commune et gÃ©rÃ© par un contrÃ´leur ğŸ·ï¸
- `Gateway` : DÃ©finit une instance d'infrastructure de gestion du trafic : Cloud load-balancing, â€¦ â˜ï¸
- `HTTPRoute` : RÃ¨gles pour mapper le trafic d'une `Gateway` Ã  un endpoint rÃ©seau (`Service`) ğŸŒ

```mermaid
---
title: Gateway API
---

graph LR;
  client([client])-. requÃªte HTTP .->gateway[Gateway];
  gateway-->httpRoute[HTTPRoute];
  httpRoute-->|RÃ¨gle de routage|service[Service];
  service-->pod1[Pod];
  service-->pod2[Pod];
```

---

## ğŸŒ CIDRs

- Kubernetes utilise uniquement 3 rÃ©seaux : ğŸŒ
  - Un CIDR pour faire communiquer les _Nodes_ ğŸŒ
  - Un CIDR _flat_ (en principe isolÃ©) pour les Pods ğŸ“¦
  - Un CIDR publique (routÃ© par le plugin CNI) pour communiquer au sein du Cluster (pour les `Service`, â€¦) ğŸŒ
- Peuvent s'ajouter des _external IP_ (Load Balancer, â€¦) ğŸŒ

---

## ğŸ› ï¸ Configuration des applications

- `ConfigMap` pour modifier la configuration des applications ğŸ“
  - DÃ©corrÃ©lÃ© du code de l'application ğŸ› ï¸
- `Secret` (mots de passe, â€¦) : Assez similaire ğŸ”
  - [DiffÃ©rents types de Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#secret-types) ğŸ”—
  - âš ï¸ Par dÃ©faut, **simple encodage** : Voir les [bonnes pratiques de sÃ©curitÃ©](https://kubernetes.io/docs/concepts/security/secrets-good-practices/) ğŸ”’
  - [Chiffrement possible](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/) des accÃ¨s _REST_ mais l'_API Server_ ne peut plus dÃ©marrer automatiquement (si trÃ¨s fort besoin de sÃ©curitÃ© uniquement) ğŸ”’
- `ConfigMap` et `Secret` peuvent Ãªtre _immuable_ ğŸ”’

---

## ğŸ’¾ Stockage

---

### ğŸ“‚ Volume

- `Volume` : **Points de montage** d'un Pod ğŸ“‚
- Pas de ressource dans l'_API Server_ (~`kubectl get volumes`~) âš ï¸
- TrÃ¨s similaire Ã  _Docker_ ğŸ³
- Pour accÃ¨s aux configs, persistence, filesystem temporaire, â€¦ ğŸ“‚
- Accessible Ã  tous les _Conteneurs_ du _Pod_ ğŸ“¦
- DÃ©truit (ou dÃ©tachÃ© si _remote_) Ã  la destruction du Pod (persiste au redÃ©marrage du conteneur) âš ï¸

---

### ğŸ“‚ Quelques types de Volumes

- `emptyDir` : Volume vide, supprimÃ© avec le Pod (mais partage entre conteneurs du pod) ğŸ—‘ï¸
- `hostPath` : Monte un rÃ©pertoire du Host vers le Pod ğŸ“‚
- `configMap` : Monte des fichiers de configuration ğŸ“
- `PersistentVolume` : `iscsi`, `nfs`, `cephfs` ğŸ’¾
- [Doc: Types de Volumes supportÃ©s](https://kubernetes.io/docs/concepts/storage/volumes/) ğŸ“š

---

:::tip
- Il est possible d'injecter des volumes issus d'images OCI : [Injecter des volumes issus d'images OCI](https://kubernetes.io/docs/tasks/configure-pod-container/image-volumes/) ğŸ“¦
- Exemple : Image Docker custom `FROM scratch` + un binaire Ã  injecter dans le conteneur principal ğŸ³
:::

---

### ğŸ’¾ PersistentVolume

- `PersistentVolume` (PV) : Vision _storage_ du cluster Kubernetes ğŸ’¾
- **Stockage extÃ©rieur** Ã  la vision _conteneur/pod_ ğŸ“¦
- ReprÃ©sente un disque concret : Local, NFS, iSCSI, SMB, EBS, SAN, â€¦ ğŸ’¾
  - Existe dans l'_API Server_ : `kubectl get persistentvolumes` ğŸ“‚
  - DurÃ©e de vie indÃ©pendante du pod ğŸ”„
  - ~Ne peut **pas Ãªtre associÃ© directement**~ Ã  un _Pod_ âš ï¸
  - [Doc: Types de PV supportÃ©s](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes) ğŸ“š
- `PersistentVolumeClaim` : RÃ©quisition d'un `PV` ğŸ“
  - Permet l'association d'un disque Ã  un _Pod_ ğŸ“¦
  - Ã‰tats : `Pending` (crÃ©ation `PVC`) -> `Bound` (attachÃ© au `Pod`) -> `Terminating` (attente de suppression) ğŸ”„

---

```mermaid
---
title: PV et PVC
---
flowchart TD

    %% Composants
    pv["PersistentVolume"]
    sc["StorageClass"]
    db[(Physical Volume)]
    class sc red
    class db green

    %% Pod et PVC imbriquÃ©s
    subgraph pod ["pod"]
        pvc["PersistentVolumeClaim"]
    end
    class pod blue

    %% Relations
    sc -.-> pv
    sc -.-> pvc
    pv --> db
```

<div class="caption">StorageClass, PersistentVolume, PersistentVolumeClaim et volume physique.</div>

---

### ğŸ“Œ En rÃ©sumÃ© :

- `Volume` => Vision _container_ : Un point de montage pour configs, persistence, filesystem temporaire, â€¦ ğŸ“‚
- `PersistentVolume` (`PV`) => Vision _storage_ du cluster Kubernetes, un espace de stockage ğŸ’¾
- `PersistentVolumeClaim` (`PVC`) => Un type de _Volume_ permettant de rÃ©quisitionner et d'utiliser un `PV` ğŸ“

---

### ğŸ’¾ Quelques solutions de stockage


| Solution | Type | Mode d'accÃ¨s | Cas d'usage |
|---------|------|--------------|-------------|
| _AWS EBS CSI_ | Stockage en bloc | `RWO` (noeud unique) | Stockage haute performance sur AWS ğŸŒ |
| _Google Persistent Disk CSI_ | Stockage en bloc | `RWO` (noeud unique) | Applications cloud-native sur GCP â˜ï¸ |
| _Ceph RBD CSI_ | Stockage distribuÃ© | `RWO`, `RWX` | Bases de donnÃ©es distribuÃ©es ğŸ—ƒï¸ |
| _Longhorn CSI_ | Stockage local | `RWO`, `RWX` | Stockage persistant natif Kubernetes ğŸ“¦ |

---

### ğŸ“‚ Volumes statiques - Ordre des opÃ©rations

- CrÃ©ation du volume `PV` par l'utilisateur : Taille, type de stockage, â€¦ ğŸ“¦
- CrÃ©ation du `PVC` par l'utilisateur : Taille et type de stockage requis (correspond Ã  un PV existant qui rÃ©pond Ã  ces critÃ¨res) ğŸ“
- Association entre `PVC` et `PV` par Kubernetes ğŸ”—
- Utilisation du `Volume` par un `Pod` ğŸ“¦

---

### ğŸ”„ Volumes dynamiques - Ordre des opÃ©rations

- `PVC` : L'utilisateur demande un volume persistant et spÃ©cifie une `StorageClass` ğŸ“
- _Provisionnement_ du `Volume` via le driver `CSI` (_Container Storage Interface_) associÃ© Ã  la `StorageClass` ğŸ“¦
- _Attachement du volume_ au _Node_ par le `CSI` ğŸ”—
- _Montage du volume_ dans le _conteneur_ depuis le _Node_ ğŸ“¦

---

### ğŸ”’ Modes d'accÃ¨s

`PV` et `PVC` ont des _access modes_ : ğŸ”’

- `ReadWriteOnce` : Un seul _Node_ peut accÃ©der au volume Ã  la fois ğŸ”’
- `ReadWriteMany` : Plusieurs _Node_ peuvent accÃ©der au volume simultanÃ©ment ğŸ”’
- `ReadOnlyMany` : Plusieurs _Node_ peuvent accÃ©der au volume (mais pas Ã©crire dedans) ğŸ”’
- `ReadWriteOncePod` : Un seul _Pod_ peut accÃ©der au volume ğŸ”’

- Un `PV` liste les modes d'accÃ¨s **qu'il supporte** ğŸ”’
- Un `PVC` liste des **contraintes** sur les droits d'accÃ¨s : Seul un `PV` les supportant peut Ãªtre rÃ©quisitionnÃ© ğŸ”’

Voir [la documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) ğŸ“š

---

## ğŸ› ï¸ Ressources avancÃ©es

---

### ğŸ›¡ï¸ DaemonSet

- Assure que des pods tournent sur tous les nÅ“uds du cluster ğŸ› ï¸
- Utile pour monitoring & logs ğŸ“Š
- Exemple : Installation d'un _Load Balancer_ `MetalLB` sur tous les _Node_ du cluster âš–ï¸

---

### ğŸ’¾ StatefulSet

- DÃ©ploie des applications avec Ã©tat : BDD, â€¦ ğŸ’¾
- Ressources **ordonnÃ©es** (ordre de lancement) ğŸ“œ
- Un `PV` par _Pod_ (vs. _ReplicaSet_ oÃ¹ les volumes sont partagÃ©s) ğŸ’¾
- _Persistent volume claim templates_ (`spec.volumeClaimTemplates`) : CrÃ©e un `PVC` par _Pod_ nommÃ© `<claim-name>.<stateful-set-name>.<pod-index>` ğŸ“
- Un mÃªme volume montÃ© dans un pod (`PVC`) le reste pour toujours (mÃªme aprÃ¨s recrÃ©ation) ğŸ”„
- Un DNS dÃ©diÃ© (_service headless_) : ğŸ“¡
  - Load-balancing sur tous les pods du set âš–ï¸
  - SÃ©lection d'un pod en particulier ğŸ¯

---

### â³ Job et CronJob

- Pour travaux "longs" (> minutes / heures) â³
- `Job` : DÃ©marre un `Pod`, en cas d'Ã©chec, relance jusqu'au _backoff limit_ (default=6) ğŸ”„
  - ParamÃ¨tres : `completions` (default=1) => Nombre d'exÃ©cutions, `parallelism` (default=1) âš™ï¸
- `CronJob` : NÃ©cessite un `schedule` (idem _Cron_ sur _UNIX_) â°

---

## ğŸ› ï¸ Configuration du cluster

- Metadata ğŸ·ï¸
- `Namespace` : Espaces de noms isolant des ressources ğŸ·ï¸
  - Cloisonne une partie du cluster ğŸ—ï¸
  - Idem namespace Linux ğŸ§
  - Namespaces spÃ©ciaux : ğŸ·ï¸
    - `kube-public` : Ressources accessibles Ã  tous (par ex pour le _bootstrap_ du cluster) ğŸŒ
    - `kube-system` : Composants Kubernetes ğŸ—ï¸
    - `default` : Si aucun namespace spÃ©cifiÃ© ğŸ·ï¸
- RÃ´les ğŸ‘¥

---

## ğŸ“š Commandes de base de KubernetesÂ®

Voir la [cheatsheet sur KubernetesÂ®](https://www.avenel.pro/k8s/cheatsheet) ğŸ“š

---

## Structure d'un fichier k8s

```yaml
apiVersion: v1 # Version de l'APIServer k8s
kind: â€¦ # Le type de ressource Ã  gÃ©rer : Pod, Deployment, Service, â€¦
metadata: # MÃ©tadatas de la ressource
  name: â€¦ # nom (interne) de la ressource Ã  crÃ©er et/ou monitorer
  namespace: mon-namespace # Namespace spÃ©cial (optionnel - sinon default)
  labels: # ajout de labels (optionnel)
    ma-cle: ma-valeur 
  [â€¦]
spec: # Les spÃ©cifications de la ressource. DiffÃ©rent pour chaque type de ressource
  [â€¦]
```

---

