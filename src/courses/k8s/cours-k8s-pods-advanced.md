---
license: Â© 2025 Tom Avenel under ó°µ«  BY-SA 4.0
title: Configuration avancÃ©e des Pods
layout: '@layouts/CoursePartLayout.astro'
tags:
- docker
- kubernetes
- devops
---

## ğŸ› ï¸ Sondes Healthcheck
- `ReadinessProbe`
  - ğŸ”„ Remplacement du Pod si dÃ©fectueux
  - ğŸŒ Exemple : dÃ©pendance service externe
  - â³ Laisser de la marge : ne pas tuer en boucle un conteneur qui dÃ©marre !
- `LivenessProbe`
  - ğŸ“Š Monitoring du Pod
  - â˜ ï¸ Kill du conteneur si Ã©chec
	- ğŸ”„ Et donc (souvent) redÃ©marrage automatique du Pod
  - ğŸš« Jamais de dÃ©pendance vers l'extÃ©rieur du Pod
- `StartupProbe`
  - âŒ Doit renvoyer un Ã©chec tant que l'application n'est pas initialisÃ©e
- ğŸ”¢ 3 modes : `exec` (commande), `httpGet`, `tcpSocket`
- â±ï¸ Si vÃ©rification > 1 seconde, prÃ©fÃ©rer prÃ©calculer (asynchrone) et retourner un cache

:::link
Voir aussi : <https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/probes/>
:::

---

### âš ï¸ Healthcheck exec : processus orphelins
- ğŸ§ En Linux, quand un processus se termine :
  - Son parent gÃ¨re son _exit status_ (`wait()`/`waitpid()`) => Ã©tat _zombie_
  - Si le processus a Ã©tÃ© tuÃ©, ses enfants sont rattachÃ©s au `PID=1` (responsable de tuer les zombies)
  - âœ… OK sur systÃ¨me "standard" (`/sbin/init`, â€¦) mais ici `PID=1` est le processus principal du conteneur
- ğŸ§Ÿ Besoin d'un tueur de zombies en cas d'`exec`
  - <https://github.com/krallin/tini> : utiliser un mini `init`
  - ğŸ”„ Ou [partager le namespace PID entre tous les conteneurs du Pod](https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/) : `gcr.io/pause` tuera les zombies

---

## ğŸ“ Limiter les ressources d'un Pod

- ğŸ’» Ressources :
  - `ResourceRequirements` : limite le CPU et/ou la mÃ©moire
    - Dans les fichiers `Deployment`, `StatefulSet`, `DaemonSet`
    - Block `resource:`
  - `LimitRange` : limites par dÃ©faut du cluster
- ğŸ“Š 2 types de limites :
  - `requests` : minimum requis par conteneur
    - Pour la rÃ©partition sur les Node (`Scheduler`)
  - `limits` : maximum par `Pod`
    - Pour la santÃ© des Pod (`Kubelet`)
- âš™ï¸ Requirement: installer un `MetricsServer` dans le cluster
- ğŸ“š Utilise les fonctionnalitÃ©s de Docker : voir le [cours Docker sur le site](https://www.avenel.pro/docker)

---

:::warn
### Attention aux limits

- Les **requests** CPU/mÃ©moire sont indispensables pour garantir un minimum de ressources et Ã©viter l'Ã©viction.
- Les **limits** sont plus polÃ©miques et plusieurs Ã©coles s'affrontent.
  - **CPUÂ : souvent pas de limits** :
		- Limiter le CPU peut causer du **throttling** inutile, surtout pour les apps multithread.
		- Pourtant, dans certains cas (exâ€¯: apps sensibles Ã  la latence ou I/O-bound), un contrÃ´le via limit permet d'isoler les workloads et garantir des performances prÃ©visibles.
  - **MÃ©moireÂ : limites gÃ©nÃ©ralement conseillÃ©es**
    - La mÃ©moire est **non compressible** : dÃ©passer un `limit` implique un *OOM kill*, ce qui peut protÃ©ger le _Node_ entier.
    - Fixer `memory limit == request` permet d'Ã©viter la surconsommation par certains pods, et d'alerter quand il faut ajuster les allocations via monitoring + OOM kills
ğŸ’¸ Sources : [1](https://medium.com/@carlosalbertoalvesscorreia/would-the-kubernetes-cpu-limit-be-an-anti-pattern-2b07d92d7bd8) [2](https://www.perfectscale.io/blog/kubernetes-cpu-limits) [3](https://home.robusta.dev/blog/stop-using-cpu-limits) [4](https://medium.com/directeam/kubernetes-resources-under-the-hood-part-3-6ee7d6015965) [5](https://stormforge.io/blog/flexibility-matters-when-setting-kubernetes-resource-limits/?utm_campaign=FY25_Q3_Learnk8s&utm_medium=newsletter&utm_source=Learnk8s)
:::

---

:::warn
### Workloads dynamiques

- Certains workloads (ex. _JVM_) ont des besoins variables : pic au dÃ©marrage, heap liÃ© Ã  memory limit, etc.
- Des limits statiques peuvent soit **empÃªcher le dÃ©marrage**, soit surprovisionner. Ex: pour le CPU, certaines versions de Java utilisent tous les cÅ“urs du _Node_ Ã  moins de dÃ©finir `XX:ActiveProcessorCount`, ce qui peut provoquer du throttling.
- PrivilÃ©gier donc une **limitation dynamique** en fonction du cycle de vie et des caractÃ©ristiques de l'application.
:::

---

## ğŸ“ˆ Scaling

- ğŸ”„ Scaling horizontal : plusieurs instances de l'application
  - ğŸ› ï¸ Commande `kubectl`
  - Ou automatiquement `HorizontalPodAutoscaler` (`HPA`) : natif k8s mais requiert un [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
- ğŸ“Š Scaling vertical : redimensionner les ressources de l'application (mÃ©moire, CPU)
  - ğŸ”„ Par mise Ã  jour du dÃ©ploiement et crÃ©ation d'un nouveau Pod
  - Ou automatiquement : `VerticalPodAutoscaler` [extension Ã  installer](https://github.com/kubernetes/autoscaler/tree/9f87b78df0f1d6e142234bb32e8acbd71295585a/vertical-pod-autoscaler)

---

## ğŸ”’ SÃ©curitÃ©

- ğŸ›¡ï¸ Appliquer un `SecurityContext` :
  - Changer le `UID`, `GID`
	- Drop de _capabilities_
	- Filesystem _R/O_
	- â€¦

---

## ğŸ”„ StratÃ©gies de dÃ©ploiements

- ğŸ› ï¸ k8s propose 2 stratÃ©gies de dÃ©ploiements :
  - **Rolling update** :
    1. CrÃ©ation pod v2 en coexistance avec v1
    2. IntÃ©gration v2
    3. Suppression v1
  - **Recreate** (sans coexistance) :
    1. Suppression v1
    2. CrÃ©ation v2

---

- ğŸ”„ Autres stratÃ©gies manuelles ou en ajoutant d'autres outils :
  - **Blue/Green** : coexistance des 2 versions (dont la nouvelle pour test)
  - **Canary deployment** : coexistance avec migration progressive des requÃªtes vers v2 : avec Ingress [Nginx](https://kubernetes.github.io/ingress-nginx/examples/canary/) ou [Traefik](https://2021-05-enix.container.training/2.yml.html#658)
- ğŸ› ï¸ Utiliser un outil comme <https://github.com/weaveworks/flagger> pour des dÃ©ploiements plus poussÃ©s

---

## ğŸ¯ Assigner un Pod Ã  un Node spÃ©cifique

- `NodeName` : assigner un `Pod` Ã  un `Node` (test uniquement)
- `Node Selector` : sÃ©lectionner un `Node` par `Label`, ex : CI/CD, Node spÃ©cialisÃ© stockage pour Pod BDD, â€¦
- `Node Affinity` : contraintes sur les `Label` du `Node`, ex : rÃ©partition gÃ©ographique, possÃ©dant un GPU, â€¦
- `Pod Affinity` : regrouper des pods pour amÃ©liorer les perfs, par ex d'une mÃªme stack applicative, â€¦
- `Anti-Affinity` : Ã©loigner des pods pour augmenter la robustesse, par ex instances de H/A, â€¦
- `Taints` et `Tolerations` : rÃ©server des `Node` pour des workloads spÃ©cifiques : le `taint` empÃªche de dÃ©ployer des `Pod` sur un `Node` en l'absence de `toleration`) : `NoSchedule` (_Control Plane_, â€¦), `PreferNoSchedule`, `NoExecute` (expulsion)

---

# ğŸ§© Pods multi-conteneurs

---

## ğŸš— Sidecars et autres patterns

- Conteneur(s) classique(s) supplÃ©mentaire(s) dans le Pod
- Points d'accÃ¨s entrÃ©e et/ou sortie Ã  la place du conteneur principal
- Utilise les volumes partagÃ©s ou la couche rÃ©seau pour travailler avec le conteneur principal
- Souvent injectÃ©s par des opÃ©rateurs k8s
- Voir la [documentation](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/)

---

- `sidecar` : Ã©tends les fonctionnalitÃ©s du conteneur principal : logs, monitoring, â€¦
- `adapter` : adapte la donnÃ©e avant de la fournir au conteneur principal (ex: CSV to JSON)
- `ambassador` : authentification, (reverse)proxy, sÃ©curitÃ© (HTTPS), â€¦

---

## ğŸ› ï¸ InitContainer

- Type de conteneur Kubernetes spÃ©cifique : `initContainers`
- LancÃ©s dans l'ordre de spÃ©cification
- Le(s) conteneur(s) classiques dÃ©marrent aprÃ¨s (si succÃ¨s uniquement)
- Usage : chargement de donnÃ©es, migration BDD, gÃ©nÃ©ration de configs, attente dÃ©pendances, â€¦ (tous les prÃ©requis du conteneur)

---

