---
license: ¬© 2025 Tom Avenel under Û∞µ´  BY-SA 4.0
title: Kubernetes
layout: '@layouts/CoursePartLayout.astro'
tags:
- docker
- kubernetes
- devops
---

## Chapitres

---

# Pr√©sentation de Kubernetes

---

`Kubernetes` `k8s` est un orchestrateur de d√©ploiement et de gestion de conteneurs applicatifs dans un cluster de machines virtuelles.

* Ind√©pendant de Docker¬Æ mais m√™me runtime `containerd` => peut tourner les m√™mes images
* Configure et g√®re un cluster applicatif complexe : n≈ìuds du cluster, r√©seau, stockage, ...

---

* De loin l'orchestrateur le plus utilis√© avec Docker¬Æ
* D'autres orchestrateurs existent : `OpenShift`, `Swarm`, `Apache Mesos`, ‚Ä¶

---

* Possibilit√© de g√©rer tout le cluster via API `kubectl`
* Mais configuration recommand√©e via `Yaml` / `Json` pour audit

---

# Recommandations

* `Docker¬Æ` seul / `docker compose` pour CI/CD et outils internes
* `k8s¬Æ` pour gestion applicative de l'environnement de production
* `k8s¬Æ` duplique des fonctionnalit√©s de Docker¬Æ => pr√©f√©rer 100% Docker¬Æ ou k8s¬Æ

---

# Technologies de conteneurs support√©es

1. `containerd` : projet open-source cr√©√© pour Kubernetes (runtime de `Docker` : _Docker sans la CLI_)
2. `Docker Engine` : _Docker avec la CLI_
3. `Podman` : alternative _serverless_ √† Docker
4. `CRI-O` : conteneurs l√©gers
5. `Mirantis Container Runtime (MCR)` (anciennement _Docker Enterprise_)

---

# Plugin r√©seau (CNI)

- Container Networking Interface (_CNI_) :
  - Permet la communication r√©seau au sein du cluster
  - Parfois int√©gr√© √† la distribution, sinon √† installer s√©par√©ment
	- <https://github.com/containernetworking/cni/>

---

## CNI (Kubernetes) vs CNM (Docker)

- Docker :
  - r√©seaux **multiples** et **isol√©s**
	- DNS par **r√©seau**
	- **pas d'interconnexion** des r√©seaux
- Kubernetes :
  - **1 seul** r√©seau _flat_
	- DNS par **`Namespace`**
	- **aucune isolation** des r√©seaux par d√©faut (utiliser des `NetworkPolicies`)

---

## Flannel

- Est un r√©seau de sous-r√©seaux pour Kubernetes
- Utilise des tunnels UDP ou TCP pour la communication 
- Offre une isolation r√©seau par pod
- Est plus simple √† configurer que les autres options
- Est majoritairement utilis√©

---

## Calico

- Utilise BGP (Border Gateway Protocol) pour le routage
- Propose une isolation r√©seau granulaire (par pod)
- Supporte nativement le routage IPv4 et IPv6
- S'int√®gre bien avec l'infrastructure existante

---

## Weave

- Cr√©e un r√©seau virtuel entre tous les conteneurs
- Utilise le DNS int√©gr√© de Docker
- Propose une isolation r√©seau par pod
- Est facile √† configurer mais peut √™tre moins performant que les autres options

---

## Cilium

- Utilise _BPF_ (_Berkeley Packet Filter_)
  - performant, d√©bit √©lev√© et latence r√©duite
- M√©triques d√©taill√©es sur le trafic r√©seau
- Supporte dynamiquement l'ajout et la suppression de n≈ìuds
- Con√ßu pour g√©rer des clusters de grande taille

---
layout: section
---

# Distributions Kubernetes

---

1. Kubeadm
   - outil officiel
	 - installation de chaque composant s√©par√©ment
	 - le plus configurable mais le plus complexe

---

2. Kubespray
   - Utilise `Ansible` pour (re)d√©ployer automatiquement un cluster
	 - compatible _bare-metal_ et _cloud_

---

3. Rancher (RKE) :
   - Plateforme compl√®te pour g√©rer des clusters Kubernetes
   - Propose des fonctionnalit√©s avanc√©es comme la gestion multi-cluster
   - Offre une interface graphique intuitive

---

4. K3s (Rancher Labs) :
   - Version all√©g√©e de Kubernetes con√ßue pour les environnemets embarqu√©s
   - Consomme moins de ressources que Kubernetes standard
   - Id√©al pour les syst√®mes √† faible puissance

---

5. K0s (CNCF) :
   - Autre version all√©g√©e Kubernetes
	 - Tr√®s minimale, aucun composant additionnel
	 - Compatible on-premise, edge, IoT, ‚Ä¶

---

6. OpenShift :
   - Distribution propri√©taire de Red Hat bas√©e sur Kubernetes
   - Inclut des fonctionnalit√©s suppl√©mentaires comme l'orchestration d'applications
   - Forte s√©curit√© et conformit√©

---

7. Docker Kubernetes Service (DKS)
   - Surveillance int√©gr√©e du cluster et des applications.
   - Nombreux drivers storage

---

8. MicroK8s (Ubuntu) :
   - Distribution l√©g√®re et s√©curis√©e de Kubernetes
   - Con√ßue pour les environnemets Ubuntu
   - Propose des fonctionnalit√©s avanc√©es comme l'installation de paquets

---

9. Minikube : 
   - Version l√©g√®re pour le d√©veloppement et le test
   - Fonctionne sur un seul ordinateur
   - Id√©al pour d√©butants et environnement de d√©veloppement

---

10. Docker Desktop :
   - Int√®gre Kubernetes nativement
   - Offre une exp√©rience utilisateur simplifi√©e
   - Adapt√© aux d√©veloppeurs utilisant Docker

---

11. Kind (Kubernetes IN Docker) :
   - D√©ploie Kubernetes dans un conteneur pour le d√©veloppement et le test
   - Cr√©e rapidement un ou plusieurs clusters localement
   - Utile pour tester plusieurs clusters : upgrade, changements d'infrastructure, ‚Ä¶

---

12. Talos Linux :
   - Distribution Linux d√©di√©e
	 - OS immuable : pas de SSH, shell, ‚Ä¶

---

# Plateformes manag√©es

- Amazon Elastic Kubernetes Service (EKS)
- Google Kubernetes Engine (GKE)
- Azure Kubernetes Services (AKS) 
- Oracle Kubernetes Engine (OKE)
- IBMCloud K8s
- OVHCloud K8s

---
layout: section
---

# Architecture

---

# Installation

- `kubeadm` : l'outil officiel (installation de chaque composant s√©par√©ment)
- Int√©gr√© dans la distribution : `k3s`, `minikube`, `microk8s`, ‚Ä¶
- Versions manag√©es : outils d√©di√©s au fournisseur de Cloud

---

# Mod√®le

* Un cluster k8s est compos√© de plusieurs `Node`
* Chaque `Node` fait tourner des `Pod` (ensemble de conteneurs - c'est l'unit√© atomique de k8s !)
* Un `Deployment` g√®re _d√©clarativement_ des ressources √† d√©ployer (pods, replicas, mise √† jour, ‚Ä¶ )
* Un `Service` permet d'exposer les ports d'un pod (interne ou externe)
  - _Aucun lien avec un `service` de `docker-compose` !_

---

## Types de Nodes

* Node de r√¥le `master` : le `control pane`, g√®re le cluster (orchestration, API server, ‚Ä¶)
* Node de type `worker` (sans r√¥le) : ex√©cute les pods et leur fournit les ressources

---

# Limites

- k8s est fait pour g√©rer de gros clusters :
- limitations Kubernetes v1.31 :
  - < 5,000 Node
  - < 110 Pod / Node
  - < 150,000 Pod (total)
  - < 300,000 Containers (total)

---

![Architecture d'un cluster Kubernetes](https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg)

<div class="caption">Architecture d'un cluster Kubernetes (source: kubernetes.io)</div>

---

```plantuml
@startditaa
+-------------------------------------------------+
|                                                 |
| +--------------------------------------------+  |
| |              Pod web : 1 adresse IP        |  |
| |  +-----------+    +-----------+            |  |
| |  | conteneur |    | conteneur |            |  |
| |  | docker    |    | docker    |            |  |
| |  | logger    |    | nginx     |            |  |
| |  +-----------+    +-----------+            |  |
| |                                            |  |
| +--------------------------------------------+  |
|                      Node                       |
+-------------------------------------------------+

@endditaa
```

<div class="caption">Architecture d'un Pod</div> 

---

# Composants

* `APIServer` : API de gestion du cluster
* `etcd` : stockage de la configuration du cluster
* `Controller Manager` : g√®re les `WorkerNode` depuis le `MasterNode`
* `Kubelet` : ex√©cute et g√®re les conteneurs sur les `Node`
* `Kube-proxy` : √©quilibre le trafic sur chaque `Node`
* `Scheduler` : assigne les `Pod` √† un `Node`

---

# etcd

- Backend k8s : √©tat du cluster (le reste est stateless)
  - store cl√©=valeur
- Dans ou en dehors du cluster
- 1 leader (par consensus)
  - d√©ployer un nombre impair d'instances
  - supporte N/3 instances d√©faillantes
- Jamais utilis√© directement (`APIServer`)
- Critique !
  - machine d√©di√©e ou environnement isol√©
  - bonnes performances r√©seau / disque

---

# ControllerManager

- Compare l'√©tat d√©sir√© (d√©claratif) √† l'√©tat actuel
- En d√©duit (et applique) les actions n√©cessaires (`APIServer`)
- Beaucoup de contr√¥leurs diff√©rents
  - possibilit√© d'installer des contr√¥leurs externes pour g√©rer de nouvelles ressources (`Custom Resource Definition`)
  - ex: Load Balancer AWS, ‚Ä¶
- Boucles de r√©conciliation :
  - reconstruit des ressources si besoin pendant le cycle de vie du cluster
  - sans besoin d'intervention
- Contient toute l'intelligence de Kubernetes

---

# Scheduler

- Assigne les `Pod` (en state: `Pending`) aux `Node`
  - techniquement : cr√©e un `Binding` et change le `nodeName` du `Pod`
- Calcule de score par _filtrage_ puis _score_ :
    1. _filtrage_ : capacit√©, tol√©rance, affinit√©, s√©lecteurs, ‚Ä¶
    2. _score_ : load-balancing, ‚Ä¶
- Possibilit√© d'installer un `Scheduler` customis√©

---

# Kubelet

- 1 `Kubelet` par `Node`
  - Un `kubelet` est souvent install√© sur le `MasterNode` pour y g√©rer ses composants dans des pods (optionnel)
  - En g√©n√©ral, on y ajoute le `Taint` : `node-role.kubernetes.io/master:NoSchedule` pour ne pas utiliser le `Master` comme un `Worker`.
- Connexion permanente √† l'`APIServer`
- D√©ploie le `Pod` s'il a le `nodeName` du `Node` courant :
    1. R√©cup√©ration de l'image (format `OCI`)
    2. Cr√©ation des ressources : `Volumes`, `Networks`, `Containers`
    3. √âtats du `Pod` : `pending` -> `running` / `failed` -> `succeeded` (termin√©)
    4. Remonte l'information √† l'`APIServer`

---

# Kube-proxy

- g√®re le r√©seau sur chaque `Node` (entre Pods et vers ext√©rieur)
- plusieurs modes :
  - tout traffic par `iptables`, r√®gles `DNAT` ( ‚ö†Ô∏è CPU si beaucoup de r√®gles)
    - load-balancer : _round-robbin_
  - `ipvs` : module noyau g√©rant un ensemble de r√®gles d'un coup (plus performant)
    - load-balancer avanc√©
  - Si CNI `Cilium` : r√®gles `eBPF` dans le noyau, plus besoin de `Kube-proxy`
    - voir section sur les CNI
- Connexion entre `Pods` : niveau 3 (_IP_)
- Connexion par `Services` : niveau 4 (_TCP_, _UDP_)
- Connexion par `Ingress` : niveau 7 (_HTTP_)

---

Voir : <https://2021-05-enix.container.training/5.yml.html#50> pour un exemple de fonctionnement du _Control Plane_ suite √† la cr√©ation d'un `Deployment`

---

# Gestion du cluster

* Fichiers de configuration `yml` (√† privil√©gier autant que possible !)
* Interface en ligne de commande `kubectl` (surtout pour lancer les fichiers de config)
* Interface web (peu utilis√©e)

---
layout: section
---

# Ressources basiques du cluster

---

## Interactions entre ressources

- Les `Pod` ex√©cutent les microservices.
- Les `Service` exposent ces pods pour permettre leur communication et leur acc√®s.
- Les `ConfigMap` et `Secret` injectent les configurations et les donn√©es sensibles.
- Le/Les `Ingress` g√®rent le trafic externe (routage par _URI_ ou header _host_) et les certificats SSL/TLS.
- Les `PersistentVolume` et `StatefulSet` supportent les applications avec √©tat.
- Les `DaemonSet` assurent le fonctionnement des outils d‚Äôadministration sur chaque noeud.

---

## Gestion des applications

- `Deployment` : g√®re le d√©ploiement d'un `ReplicaSet`
  - et la mise √† jour des applications (rolling update, rollback, scaling)
- `ReplicaSet` : cr√©e et g√®re le suivi (r√©plicas) d'un pod
  - ne pas utiliser de `ReplicaSet` directement mais passer par un `Deployment` (plus puissant)
- `Pod` : g√®re un ensemble de conteneurs partageant la m√™me isolation : stack r√©seau, stockage, ‚Ä¶
  - d√©marr√© directement ou (mieux) par un `deployment` cr√©ant un `ReplicaSet`
  - √©ph√©m√®re : pas de donn√©es critiques dans le pod
  - 1 IP par pod partag√©e entre tous les conteneurs (mais l'IP peut changer)
    - acc√®s par `localhost` aux autres conteneurs et **partage des ports ouverts**

---

```plantuml
@startditaa
+-------------------------------------------------+
|                 Deployment : replicas=2         |
| +--------------------------------------------+  |
| |              ReplicaSet : 2 Pods           |  |
| |  +-----------+    +-----------+            |  |
| |  | Pod 1     |    | Pod 2     |            |  |
| |  | nginx     |    | nginx     |            |  |
| |  +-----------+    +-----------+            |  |
| |                                            |  |
| +--------------------------------------------+  |
|                                                 |
+-------------------------------------------------+

@endditaa
```

<div class="caption">Un Deployment g√©rant un ReplicaSet g√©rant un Pod</div> 

---

## Labels

- attributs cl√©=valeur des objets du cluster
- utilis√© par kubernetes
- `NodeSelector` : lance un pod sur un `Node` ayant ce label
- `NodeAffinity` : d√©crit des affinit√©s entre un `Pod` et un `Node`
- `podAffinity`, `podAntiAffinity` : (anti)affinit√© entre `Pod`
- il existe aussi des `annotations` : idem mais NON utilis√© par k8s ensuite

---

### Labels et debug

- Beaucoup de ressources utilisent les labels pour s√©lectionner les ressources (`Pod`, ‚Ä¶) √† manager
- Pour debugger un `Pod` fautif, on peut changer son `Label` :
  - le Pod fautif sera retir√© du Service (plus de Load balancing)
  - un nouveau Pod est cr√©√© par le `ReplicaSet` ou le `DaemonSet`
  - le Pod fautif est toujours actif pour du debug

---

## Service

- Service DNS permettant d'acc√©der √† 1 (ou plusieurs) Pods
  - Nom DNS court (dans le namespace) : `<service_name>.<namespace>` (ou `<service_name>` si `namespace==default`)
  - Nom DNS complet : `<service_name>.<namespace>.svc.<cluster-domain>`
  - exemple : `mon_service.mon_namespace.svc.mon_cluster`
- Association `Service` <-> `Pod`(s) gr√¢ce aux _labels_
  - **avec gestion des r√©plicas**
- Au moins 2 CIDR (plages r√©seau) : CIDR Pod et CIDR Services

---

### Service: ClusterIP

- Expose √† l'int√©rieur du cluster uniquement
- Cr√©e une Virtual IP
- Acc√®s via le nom du service
- Load balancer interne sur les Pods

---

### Service: NodePort

- Extension du `ClusterIP`
- Expose √† l'ext√©rieur du cluster
- Acc√®s via des ports sur les Nodes du cluster
- Load balancer interne sur les Pods

---

### Service: LoadBalancer

- LoadBalancer pour l'acc√®s au Pod depuis l'ext√©rieur
  - id√©alement directement, sinon par un `NodePort`
- Permet d'avoir un acc√®s unique √† plusieurs conteneurs d'un Pod tournant sur plusieurs Nodes.
- Load balancer externe : li√© au service de load balancing du Cloud Provider.
  - on-premise, installer `MetalLB`

---

### Service: ExternalName

- R√©f√©rence un DNS interne ou externe (alias)
- exemple : BDD externe au cluster
- pas de Load balancer

---

### Endpoint

- Lien `Service` <-> `Pod`

---

## Ingress

- Point d'acc√®s publique HTTP/HTTPS unique pour l'acc√®s aux diff√©rentes Pods (diff√©rent d'un Service)
- Agit comme un _Reverse-proxy_ qui redirige la requ√™te vers le `Service`
- R√®gles de routage avanc√©es
- En principe, cr√©e un service `LoadBalancer` (point d'entr√©e de l'Ingress).
- Recquiert une impl√©mentation d'`Ingress Controller` √† installer :
  - `Nginx Ingress Controller` : standard, stable, supporte HTTPS et annotations avanc√©es.
  - `HAProxy Ingress` : performant
  - `Traefik` : l√©ger, dynamique (cloud, microservices)
  - `Consul Ingress / Istio Gateway` : int√©gration avec les _service mesh_ Consul / Istio

---

```mermaid
---
title: Schema d'un Ingress bas√© path.
---

graph LR;
  client([client])-. Ingress-managed <br> load balancer .->ingress[Ingress, 178.91.123.132];
  ingress-->|/foo|service1[Service service1:4200];
  ingress-->|/bar|service2[Service service2:8080];
  subgraph cluster
  ingress;
  service1-->pod1[Pod];
  service1-->pod2[Pod];
  service2-->pod3[Pod];
  service2-->pod4[Pod];
  end
  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
  class ingress,service1,service2,pod1,pod2,pod3,pod4 k8s;
  class client plain;
  class cluster cluster;
```

```mermaid
---
title: Schema d'un Ingress bas√© hostname.
---

graph LR;
  client([client])-. Ingress-managed <br> load balancer .->ingress[Ingress, 178.91.123.132];
  ingress-->|Host: foo.bar.com|service1[Service service1:80];
  ingress-->|Host: bar.foo.com|service2[Service service2:80];
  subgraph cluster
  ingress;
  service1-->pod1[Pod];
  service1-->pod2[Pod];
  service2-->pod3[Pod];
  service2-->pod4[Pod];
  end
  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
  class ingress,service1,service2,pod1,pod2,pod3,pod4 k8s;
  class client plain;
  class cluster cluster;

```

<div class="caption">Source: <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></div>

---

## cert-manager (TLS)

- CRD √† ajouter au Cluster pour g√©n√©rer et signer des `Certificat`
- Stocke la `key` et le `crt` dans un `Secret`
  - R√©utilisables dans `Ingress`, ‚Ä¶
- Utilise des `Issuer` (namespace-limited) ou des `ClusterIssuer` (cluster-wide)

---

## Service Mesh

- Ajoute les services d'infrastructure communs
  - authentification
  - s√©curit√©
  - logs
- G√®re la communication s√©curis√©e entre conteneurs sur des architectures micro-services
- √Ä installer : `Istio`, `linkerd`, `consul`, ‚Ä¶
  - Voir la [page des outils Devops](https://www.avenel.pro/tools#-kubernetes-specific)

---

## Gateway API

- Nouvelle API Kubernetes (successeur Ingress)
  - Orient√© r√¥les, portable, extensible
  - Routage multi-namespace
  - D√©cor√©l√© de l'installation de kubernetes
- `GatewayClass` : Ensemble de `Gateway` avec configuration commune et g√©r√© par un contr√¥leur
- `Gateway` : D√©finit une instance d'infrastructure de gestion du trafic : Cloud load-balancing, ‚Ä¶
- `HTTPRoute` : R√®gles pour mapper le trafic d'une `Gateway` √† un endpoint r√©seau (`Service`)

```mermaid
---
title: Gateway API
---

graph LR;
  client([client])-. requ√™te <br> HTTP .->gateway[Gateway];
  gateway-->httpRoute[HTTPRoute];
  httpRoute-->|R√®gle de routage|service[Service];
  service-->pod1[Pod];
  service-->pod2[Pod];
  classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
  classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
  classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
  class gateway,httpRoute,service,pod1,pod2, k8s;
  class client plain;
  class cluster cluster;
```

---

## Configuration des applications

* `ConfigMap` pour modifier la configuration des applications
  - d√©cor√©l√© du code de l'application
* `Secret` (mots de passe, ‚Ä¶) : assez similaire
  - [Differents types de Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#secret-types)
  - ‚ö†Ô∏è par d√©faut, **simple encodage** : voir les [bonnes pratiques de s√©curit√©](https://kubernetes.io/docs/concepts/security/secrets-good-practices/)
  - [chiffrement possible](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/) des acc√®s _REST_ mais l'_API Server_ ne peut plus d√©marrer automatiquement (si tr√®s fort besoin de s√©curit√© uniquement)
* `ConfigMap` et `Secret` peuvent √™tre _immuable_ 

---

## Stockage

---

### Volume

- `Volume` : **points de montage** d'un Pod
- pas de ressource dans l'_API Server_ (~`kubectl get volumes`~)
- tr√®s similaire √† _Docker_
- pour acc√®s aux configs, persistence, filesystem temporaire, ‚Ä¶
- accessible √† tous les _Conteneurs_ du _Pod_
- d√©truit (ou d√©tach√© si _remote_) √† la destruction du Pod (persiste au red√©marrage)

---

### PersistentVolume

- `PersitentVolume` (PV) : vision _storage_ du cluster Kubernetes
- **stockage ext√©rieur** √† la vision _conteneur/pod_
- Repr√©sente un disque concret : _EBS_, _SAN_, ‚Ä¶
  - existe dans l'_API Server_ : `kubectl get persistentvolumes`
  - dur√©e de vie ind√©pendante du pod
  - ~ne peut **pas √™tre associ√© directement**~ √† un _Pod_

- `PersistentVolumeClaim` : r√©quisition d'un `PV`
  - permet l'association d'un disque √† un _Pod_

---

En r√©sum√© :

- `Volume` => vision _container_ : un point de montage pour configs, persistence, filesystem temporaire, ‚Ä¶
- `PersitentVolume` (`PV`) => vision _storage_ du cluster Kubernetes, un espace de stockage
- `PersistenVolumeClaim` (`PVC`) => un type de _Volume_ permettant de r√©quisitionner et d'utiliser un `PV`

---

### Quelques types de Volumes

- `emptyDir` : volume vide, supprim√© avec le Pod (mais partage entre conteneurs du pod) 
- `hostPath` : monte un r√©pertoire du Host vers le Pod
- `configMap` : monte des fichiers de configuration
- `PersistentVolume` : `iscsi`, `nfs`, `cephfs`
- beaucoup de types support√©s : <https://kubernetes.io/docs/concepts/storage/volumes/>

---

### Quelques solutions de stockage

Solution | Type | Mode d'acc√®s | Cas d'usage
---------|------|--------------|-------------
_AWS EBS CSI_ | Stockage en bloc | `RWO` (noeud unique) | Stockage haute performance sur AWS
_Google Persistent Disk CSI_ | Stockage en bloc | `RWO` (noeud unique) | Applications cloud-native sur GCP
_Ceph RBD CSI_ | Stockage distribu√© | `RWO`, `RWX` | Bases de donn√©es distribu√©es
_Longhorn CSI_ | Stockage local | `RWO`, `RWX` | Stockage persistant natif Kubernetes

---

### Volumes statiques - Odre des op√©rations

- Cr√©ation du volume `PV` par l'utilisateur : taille, type de stockage, ‚Ä¶
- Cr√©ation du `PVC` par l'utilisateur : taille et type de stockage requis (correspond √† un PV existant qui r√©pond √† ces crit√®res).
- Association entre `PVC` et `PV` par Kubernetes
- Utilisation du `Volume` par un `Pod`

---

### Volumes dynamiques - Odre des op√©rations

- `PVC` : l'utilisateur demande un volume persistant et sp√©cifie une `StorageClass`
- _Provisionnement_ du `Volume` via le driver `CSI` (_Container Storage Interface_) associ√© √† la `StorageClass`
- _Attachement du volume_ au _Node_ par le `CSI`
- _Montage du volume_ dans le _conteneur_ depuis le _Node_

---

# Ressources avanc√©es

---

## DaemonSet

- Assure que des pods tournent sur tous les noeuds du cluster
- Utile pour monitoring & logs

---

## StatefulSet

- D√©ploie des applications avec √©tat : BDD, ‚Ä¶
- Ressources **ordonn√©es** (ordre de lancement)
- Un volume persistant par _Pod_ (vs. _ReplicaSet_ o√π les volumes sont partag√©s)
- Un m√™me volume mont√© dans un pod (`PVC`) le reste pour toujours (m√™me apr√®s recr√©ation)
- Un DNS d√©di√© (_service headless_) :
  - load-balancing sur tous les pods du set
  - s√©lection d'un pod en particulier

---

## Job et CronJob

- Pour travaux "longs" (> minutes / heures)
- `Job` : D√©marre un `Pod`, en cas d'√©chec, relance jusqu'au _backoff limit_ (default=6)
  - param√®tres : `completions` (default=1) => nombre d'ex√©cutions, `parallelism` (default=1)
- `CronJob` : N√©cessite un `schedule` (idem _Cron_ sur _UNIX_)

---

## Configuration du cluster

* Metadata
* `Namespace` : espaces de noms isolant des ressources
  - cloisonne une partie du cluster
  - idem namespace Linux
* R√¥les

---

## Role-Based Access Control (RBAC)

- `ClusterRole` : profil permettant des acc√®s / actions / ressources
- `ServiceAccount` : user applicatif
  - g√©n√®re des token (secrets) : √† monter par exemple dans un `Pod` pour permettre l'acc√®s
- `Cluster Role Binding` : association `ServiceAccount` <-> `ClusterRole`

---

## NetworkPolicies

- Par d√©faut :
- un `Pod` peut communiquer avec tout autre `Pod`, y compris d'autres `Namespace`
- un `Service` est accessible partout, y compris depuis d'autres `Namespace`
- Une `NetworkPolicy` permet d'**ajouter** de l'isolation :
  - si un `Pod` n'est _s√©lectionn√©_ par **aucune `NetworkPolicy`** : **aucune isolation**
  - si un `Pod` **est _s√©lectionn√©_** par au moins une `NetworkPolicy` : **isolation totale par d√©faut** (sauf r√®gles accept√©es par la `NetworkPolicy`)
  - **stateful** : isolation √† la **connexion**, et ~non par paquet~
  - Pod A -> Pod B : accepter A vers B (`egress`) **et** B depuis A (`ingress`)

:::warn
Certains CNI ne supportent pas (totalement) les _NetworkPolicies_ : la ressource est appliqu√©e mais sans effet !
:::

---
layout: section
---

# Configuration avanc√©e des Pods

---

## Sondes Healthcheck

- `ReadinessProbe`
  - remplacement du Pod si d√©fectueux
  - exemple : d√©pendance service externe
  - laisser de la marge : ne pas tuer en boucle un conteneur qui d√©marre !
- `LivenessProbe`
  - monitoring du Pod
  - kill du conteneur si √©chec
	- et donc (souvent) red√©marrage automatique du Pod
  - jamais de d√©pendance vers l'ext√©rieur du Pod
- `StartupProbe`
  - doit renvoyer un √©chec tant que l'application n'est pas initialis√©e
- 3 modes : `exec` (commande), `httpGet`, `tcpSocket`
- si v√©rification > 1 seconde, pr√©f√©rer pr√©calculer (asynchrone) et retourner un cache

Voir aussi : <https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/probes/>

---

### ‚ö†Ô∏è Healthcheck exec : processus orphelins 

- En Linux, quand un processus se termine : 
  - son parent g√®re son _exit status_ (`wait()`/`waitpid()`) => √©tat _zombie_
  - si le processus a √©t√© tu√©, ses enfants sont rattach√©s au `PID=1` (responsable de tuer les zombies)
  - OK sur syst√®me "standard" (`/sbin/init`, ‚Ä¶) mais ici `PID=1` est le processus principal du conteneur
- Besoin d'un tueur de zombies üßü en cas d'`exec`
  - <https://github.com/krallin/tini> : utiliser un mini `init`
  - Ou [partager le namespace PID entre tous les conteneurs du Pod](https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/) : `gcr.io/pause` tuera les zombies

---

## Limiter les ressources d'un Pod

- Ressources :
  - `ResourceRequirements` : limite le CPU et/ou la m√©moire
    - dans les fichiers `Deployment`, `StatefulSet`, `DaemonSet`
    - block `resource:`
  - `LimitRange` : limites par d√©faut du cluster
- 2 types de limites :
  - `requests` : minimum requis par conteneur
    - pour la r√©partition sur les Node (`Scheduler`)
  - `limits` : maximum par `Pod`
    - pour la sant√© des Pod (`Kubelet`)
- Requirement: installer un `MetricsServer` dans le cluster
- Utilise les fonctionnalit√©s de Docker : voir le [cours Docker sur le site](https://www.avenel.pro/cours/docker)

---

## Scaling

- Scaling horizontal : plusieurs instances de l'application
  - commande `kubectl`
  - ou automatiquement `HorizontalPodAutoscaler` (`HPA`) : natif k8s mais requiert un [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)
- Scaling vertical : redimensionner les ressources de l'application (m√©moire, CPU)
  - par mise √† jour du d√©ploiement et cr√©ation d'un nouveau Pod
  - ou automatiquement : `VerticalPodAutoscaler` [extension √† installer](https://github.com/kubernetes/autoscaler/tree/9f87b78df0f1d6e142234bb32e8acbd71295585a/vertical-pod-autoscaler)

---

## S√©curit√©

- Appliquer un `SecurityContext` : 
  - changer le `UID`, `GID`
	- drop de _capabilities_
	- filesystem _R/O_
	- ‚Ä¶

---

## Strat√©gies de d√©ploiements

- k8s propose 2 strat√©gies de d√©ploiements :
  - **rolling update** :
    1. cr√©ation pod v2 en coexistance avec v1
    2. int√©gration v2
    3. suppression v1
  - **recreate** (sans coexistance) :
    1. suppression v1
    2. cr√©ation v2

---

- autres strat√©gies manuelles ou en ajoutant d'autres outils :
  - **blue/green** : coexistance des 2 versions (dont la nouvelle pour test)
  - **canary deployment** : coexistance avec migration progressive des requ√™tes vers v2 : avec Ingress [Nginx](https://kubernetes.github.io/ingress-nginx/examples/canary/) ou [Traefik](https://2021-05-enix.container.training/2.yml.html#658)
- Utiliser un outil comme <https://github.com/weaveworks/flagger> pour des d√©ploiements plus pouss√©s

---

## Assigner un Pod √† un Node sp√©cifique

- _NodeName_ : assigner un _Pod_ √† un _Node_ (test uniquement)
- _Node Selector_ : s√©lectionner un _Node_ par _Label_, ex : CI/CD, Node sp√©cialis√© stockage pour Pod BDD, ‚Ä¶
- _Node Affinity : contraintes sur les _Label_ du _Node_, ex : r√©partition g√©ographique, poss√©dant un GPU, ‚Ä¶
- _Pod Affinity_ : regrouper des pods pour am√©liorer les perfs, par ex d'une m√™me stack applicative, ‚Ä¶
- _Anti-Affinity_ : √©loigner des pods pour augmenter la robustesse, par ex instances de H/A, ‚Ä¶
- _Taints_ et _Tolerations_ : r√©server des _Node_ pour des workloads sp√©cifiques : le _taint_ emp√™che de d√©ployer des _Pod_ sur un _Node_ en l'absence de _toleration_) : `NoSchedule` (_Control Plane_, ‚Ä¶), `PreferNoSchedule`, `NoExecute` (expulsion)

---

# Pods multi-conteneurs

---

## Sidecars et autres patterns

- Conteneur(s) classiques suppl√©mentaire(s) dans le Pod
- Points d'acc√®s entr√©e et/ou sortie √† la place du conteneur principal
- Utilise les volumes partag√©s ou la couche r√©seau pour travailler avec le conteneur principal
- Souvent inject√©s par des op√©rateurs k8s
- Abstraction th√©orique : pas de mod√®le kubernetes [mais une impl√©mentation est en beta](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/)

---

- `sidecar` : √©tends les fonctionnalit√©s du conteneur principal : logs, monitoring, ‚Ä¶
- `adapter` : adapte la donn√©e avant de la fournir au conteneur principal (ex: CSV to JSON)
- `ambassador` : authentification, (reverse)proxy, s√©curit√© (HTTPS), ‚Ä¶

---

## initcontainer

- Type de conteneur Kubernetes sp√©cifique : `initContainers`
- Lanc√©s dans l'ordre de sp√©cification
- Le(s) conteneur(s) classiques d√©marrent apr√®s (si succ√®s uniquement)
- usage : chargement de donn√©es, migration BDD, g√©n√©ration de configs, attente d√©pendances, ‚Ä¶ (tous les pr√©-requis du conteneur)

---

# Commandes de base de Kubernetes¬Æ 

Voir la [cheatsheet sur Kubernetes¬Æ](https://www.avenel.pro/cours/docker/kubernetes-cheatsheet)

---

# Structure d'un fichier k8s

```yaml
apiVersion: v1 # Version de l'APIServer k8s
kind: ‚Ä¶ # Le type de ressource √† g√©rer : Pod, Deployment, Service, ‚Ä¶
metadata: # M√©tadatas de la ressource
  name: ‚Ä¶ # nom (interne) de la ressource √† cr√©er et/ou monitorer
  namespace: mon-namespace # Namespace sp√©cial (optionnel - sinon default)
  labels: # ajout de labels (optionnel)
    ma-cle: ma-valeur 
  [‚Ä¶]
spec: # Les sp√©cifications de la ressource. Diff√©rent pour chaque type de ressource
  [‚Ä¶]
```

---
layout: section
---

# Administration du cluster

- Condens√© de [ces slides sur la _disruption_ dans un Cluster](https://github.com/jpetazzo/container.training/blob/main/slides/k8s/disruptions.md)

---

## Panne de _Node_

- Un _Node_ devient `unreachable`
- application automatique de _taints_ `NoSchedule` et `NoExecute`
- _Pods_ resch√©dul√©s sur un autre _Node_ (sauf `StatefulSet`)
- Anticipation : > 2 r√©plicas de _Pod_ & anti-affinit√© pour ex√©cution sur diff√©rents _Node_

---

## Pression m√©moire/disque

- Plus assez de m√©moire sur le _Node_ ou disque plein (`errno = ENOSPC` & `No space left on device`)
- _√©viction_ de _Pod_ sur d'autres _Node_ (candidats tri√©s par `priorityClassName` puis par consommation)
- _graceful shutdown_ pendant `terminationGracePeriodSeconds` puis _kill_
- Att√©nuation : `resource limits`, `resource requests`

---

## Arr√™t manuel d'un _Node_ et PDB

- Attention si trop de r√©plicas sur ce _Node_ ou qui ne devraient pas √™tre interrompues
- `PodDisruptionBudget` : contrat entre "sysops" (admin des _Node_ cluster) et "devops" (d√©ployant les applis)
- ex : *dans cet ensemble de pods, ne pas "perturber" plus de X √† la fois*
- Arr√™ter un _Node_ uniquement lorsqu'il n'y a plus de _Pod_ ex√©cut√© dessus (sauf _Pod_ syst√®me d'un `DaemonSets`).
- `kubectl drain` :
  - _cordon_ (boucle) le _Node_ : _taint_ `NoSchedule`
  - _eviction API_ pour supprimer les _Pod_ (respecte les `PDB`).
  - n'expulsera pas les _Pod_ utilisant des volumes `emptyDir` (sauf `--delete-emptydir-data`)

---

## Upgrade d'un _Node_

- Exemple¬†: upgrade `Kubelet` ou noyau `Linux`
- Si upgrade "safe" : upgrade noyau, patch `Kubelet` (1.X.Y ‚Üí 1.X.Z) : _normalement_ OK, **√† tester en staging!**
- Sinon¬†: migrer d'abord hors du _Node_ (idem arr√™t du _Node_).

---

## Rescheduling manuel

- Exemple¬†: le pod X effectue beaucoup d'E/S disque (_starvation_)
- Cons√©quence¬†: les _Pod_ d√©plac√©es sont temporairement interrompus
- Att√©nuation¬†: d√©finir un nombre appropri√© de r√©plicas, d√©clarer des PDB, utiliser l'[eviction API](https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/)

---
layout: section
---

# √âtendre Kubernetes

Voir aussi : [ces slides de formation](https://2021-05-enix.container.training/4.yml.html#103)

---

# Controller

> Controllers are control loops that watch the state of your cluster, then make or request changes where needed.
> Each controller tries to move the current cluster state closer to the desired state

<https://kubernetes.io/docs/concepts/architecture/controller/>

- surveilles des ressources
- applique des changements : API uniquement (`Deployment`, `ReplicaSet`), configure des ressources (`kube-proxy`), provisionne des ressources (_Load Balancer_)
- `kube-scheduler` est un _Controller_ !
- [Admission Controller](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) et [extension](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) : peut examiner ou transformer les requ√™tes √† l'`API Server` avant la cr√©ation des ressources : `ServiceAccount`, `LimitRanger`, `Kyverno`, `Open Policy Agent`, ‚Ä¶

---

![Kubernetes API request lifecycle (source: Banzai Cloud)](@assets/k8s/api-request-lifecycle.png)

---

# CRD

- Kubernetes permet d'ajouter des types de `Ressource` personnalis√©s
- nouveau `kind: ‚Ä¶`
- appel√©es _Custom Ressource Definition (CRD)_
- `kubectl api-resources` (toutes les ressources)
- `kubectl get crds` (CRD)
- <https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/>
- <https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/>

---

# Aggregation Layer

- Ressources [APIServer](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) : lien `kind: ‚Ä¶` √† un service externe
- D√©l√®gue des parties de l'`API Server` √† des API externes
- ex: `metrics-server`
- `kubectl get apiservices`
- (autre option: [Service catalog](https://kubernetes.io/docs/concepts/extend-kubernetes/service-catalog/) utilisant l'_Open service broker API_)

---

# Operators

> An operator represents human operational knowledge in software, to reliably manage an application. - [CoreOS](https://coreos.com/blog/introducing-operators.html)

- Un _Operator_ permet d'ajouter au cluster des _CRDs_ et un _Controller_ pour les g√©rer
- liste : <https://operatorhub.io/>
- exemples :
  - BDD r√©pliqu√©es primaire / secondaire : `MariaDB`, `MySQL`, `PostgreSQL`, `Redis`
  - _Node_ qui n'ont pas le m√™me r√¥le : `ElasticSearch`, `MongoDB`
  - D√©pendances complexes : `Flink` & `Kafka` avec d√©pendance `Zookeeper`
  - Ressources externes : `AWS S3`
  - Add-ons complexes : `Istio`, `Consul`
  - G√©rer ses propres applications
- Kubernetes a pour projet de tout d√©l√©guer dans le futur √† des _Operator_ : `Service`, `Deployment`, ‚Ä¶
- [Exemple de cr√©ation d'Operator par Ansible](https://blog.stephane-robert.info/post/ansible-kubernetes-operator/)

---

# Kustomize

- Permet d'ajouter / modifier des ressources Kubernetes par `Kustomization` (fichier YAML)
- Int√©gr√© dans `kubectl` : `apply -k ‚Ä¶`
- Utile pour :
  - config dev vs prod, ‚Ä¶
  - scaling : `replicas: ‚Ä¶`
  - mettre √† jour l'image d'un conteneur

![Exemple d'utilisation de Kustomize](https://kustomize.io/images/header_templates.png)

<div class="caption">Exemple d'usage de Kustomize. Credits: kustomize.io</div>

---

## Vocabulaire Kustomize

- **kustomization** : une **base** ou un **overlay**
- **base** : _kustomization_ **r√©f√©renc√©e par** d'autres _kustomization_
- **overlay** : _kustomization_ **qui r√©f√©rence** d'autres _kustomization_ (r√©cursif)
- **patch** : une **modification** d'une ressource Kubernetes existante
- **variant** : **version finale** de la ressource Kubernetes apr√®s application des _bases_ et _overlay_

---
layout: section
---

# Outils externes

---

# Helm

- Gestionnaire de "paquets" k8s
  - en fait des fichiers Yaml
  - ajout du versionning
- `chart` : ensemble de fichiers manifests
- Stock√©s dans des `repositories`
  - `hub` officiel : <https://hub.helm.sh/>

---

# FluxCD

- Outil Gitops pour k8s
    - scrute un d√©p√¥t Git distant
    - mise √† jour automatique des ressources k8s
    - plus de CLI `kubectl`
    - Utilise des `Kustomizations` de `Kustomize` (outil int√©gr√© √† k8s)
- int√©grations Helm et Terraform
- CLI `fluxctl` (pas de GUI)
    - outils plus avanc√©s : `argoscd`, `jenkins X`

---

![Architecture de FluxCD](https://raw.githubusercontent.com/fluxcd/flux2/main/docs/diagrams/fluxcd-controllers.png)

<div class="caption">Architecture de FluxCD (source: documentation FluxCD)</div>

---

# Kyverno

- Moteur de politiques pour k8s
- G√®re des r√®gles de s√©curit√©, de conformit√© et de gestion (fichiers Yaml)
- DevSecOps
- Voir [une introduction √† Kyverno](https://2021-05-enix.container.training/4.yml.html#399)

---

# Kubeseal

- Transforme un `Secret` Kubernetes en `SealedSecret` chiffr√© (cl√© priv√©e dans le cluster, cl√© publique pour g√©n√©rer les secrets, contr√¥leur `SealedSecrets`).
- Seul le cluster peut d√©chiffrer un `SealedSecret`
- Il devient possible de laisser les _Secret_ chiffr√©s dans _Git_.

---

# Certifications

- Kubernetes and Cloud Native Associate (KCNA)
- Certified Kubernetes Application Developer (CKAD)
- Certified Kubernetes Administrator (CKA)
- Certified Kubernetes Security Specialist (CKS)

Pour plus d'information, voir [une explication des diff√©rentes formations](https://gist.github.com/bakavets/05681473ca617579156de033ba40ee7a)

---

# Bonnes pratiques

---

## S√©curit√© des Secrets

- ~**Ne pas stocker de `Secret` en clair**~ dans des fichiers YAML : Utiliser `Kubeseal` pour les chiffrer.
- Limiter l'acc√®s aux `Secret` avec **Role-Based Access Control (RBAC)** : Par d√©faut, acc√®s √† tous les Secrets du Namespace.
- Activer le **chiffrement des Secrets dans `etcd`** : par d√©faut les Secrets sont en clair. Attention √† l'impact sur les performances.
- Utiliser des **solutions de gestion externe des Secrets** : _HashiCorp Vault_, _AWS Secrets Manager_, _Azure Key Vault_, ‚Ä¶

---

## Limitation des ressources

- Dans chaque `Namespace`, cr√©er un `LimitRange` :
  - `defaultRequest.cpu` et `defaultLimit.cpu` petits (`100m`, ‚Ä¶)
  - `defaultRequest.memory` et `defaultLimit.memory` en lien avec le workflow standard :
    - `Java`, `Ruby` : `1G`
    - `Go`, `Python`, `PHP`, `Node` : `250M`
- Dans chaque `Namespace`, cr√©er un `ResourceQuota` :
  - `limits.cpu` et `limits.memory` √©lev√©s (1/2 cluster, ‚Ä¶)
  - limite √©lev√©e d'objets (seulement si un contr√¥leur s'emballe)
- Voir [ces slides sur les limitations de ressources](https://2021-05-enix.container.training/4.yml.html#190)

---

## D√©coupage en sous-clusters :

- Un cluster par application, des `Namespace` diff√©rents pour les environnements¬†?
- Un cluster par environnement, des `Namespace` diff√©rents pour les applications¬†?
- Tout sur un seul cluster¬†? Un cluster par combinaison¬†?
- Un cluster interm√©diaire¬†:
  - Cluster de production, cluster de base de donn√©es, cluster de d√©veloppement/pr√©production/etc.
  - Cluster de production et de base de donn√©es par application, cluster de d√©veloppement/pr√©production/etc. partag√©

---
layout: two-cols
---

<!-- class: liens -->

# Liens

- [Site web Kubernetes](https://kubernetes.io/)
- Bacs √† sable pour tester k8s : [killercoda](https://killercoda.com/playgrounds/scenario/kubernetes) et <https://labs.play-with-k8s.com/>
- Mini-distributions : <https://blog.palark.com/small-local-kubernetes-comparison/>
- [Introduction √† k8s](https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/introduction/)
- Cours sur kubernetes :
  - [uptime-formation](https://supports.uptime-formation.fr/05-kubernetes/01_cours_presentation_k8s/)
  - [stephane-robert](https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/kubernetes/introduction/)
  - [vid√©os xavki](https://www.youtube.com/watch?v=37VLg7mlHu8&list=PLn6POgpklwWqfzaosSgX2XEKpse5VY2v5)
	- <https://container.training/> : Les **excellentes formations (compl√®tes) de J√©r√¥me Petazzo**, notamment :
	  - [Fondamentaux Kubernetes](https://2021-05-enix.container.training/2.yml.html) : cours complet, de l'installation aux usages de kubernetes
		- [Packaging d'applications et CI/CD pour Kubernetes](https://2021-05-enix.container.training/3.yml.html)
		- [Kubernetes Avanc√©](https://2021-05-enix.container.training/4.yml.html)
		- [Op√©rer Kubernetes](https://2021-05-enix.container.training/5.yml.html)
		- <https://github.com/jpetazzo/container.training>
		- [Video - Deep Dive into Kubernetes Internals for Builders and Operators](https://www.youtube.com/watch?v=3KtEAa7_duA)
- [Dear Friend, you have built a Kubernetes](https://www.macchaffee.com/blog/2024/you-have-built-a-kubernetes/)

- [Introduction √† kubectl](https://blog.stephane-robert.info/docs/conteneurs/orchestrateurs/outils/kubectl/)
- <https://roadmap.sh/kubernetes>
- [Helm: package manager pour d√©ployer dans k8s](https://helm.sh/)
    - [Introduction √† Helm](https://www.aukfood.fr/helm-le-meilleur-ami-de-votre-kubernetes/)
- [Livre : Bootstrapping Microservices with Docker, Kubernetes, and Terraform](https://www.manning.com/books/bootstrapping-microservices-with-docker-kubernetes-and-terraform)
- <https://www.cortex.io/post/understanding-kubernetes-services-ingress-networking>

---
layout: two-cols
---

- Autoscaling : [doc](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) et [pratique](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)
- [HPA Autoscaling depuis des m√©triques custom dans Prometheus](https://blog.zwindler.fr/2024/10/11/optimisation-ressources-kubernetes-autoscaling-horizontal-custom-metrics-prometheus-adapter/)
- [Blue-Green deployment in k8s](https://developer.harness.io/docs/continuous-delivery/deploy-srv-diff-platforms/kubernetes/kubernetes-executions/create-a-kubernetes-blue-green-deployment/)
- [Canary deployment in k8s](https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/kubernetes/canary-demo?view=azure-devops&tabs=yaml)
- <https://blog.wescale.fr/comment-rendre-une-application-haute-disponibilit%C3%A9-avec-kubernetes>

- Tutoriels sur la communication entre pods :
  - [Utiliser un service](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)
  - [Tutoriel complet](https://medium.com/@extio/mastering-kubernetes-pod-to-pod-communication-a-comprehensive-guide-46832b30556b)
	- [Youtube Xavki : Kubernetes 021 - Services : NodePort, LoadBalancer, ExternalName et notions de Endpoints](https://www.youtube.com/watch?v=tF28iwTco9A)
- [Exemple de monitoring Prometheus - Grafana dans un cluster Kubernetes](https://blog.octo.com/exemple-dutilisation-de-prometheus-et-grafana-pour-le-monitoring-dun-cluster-kubernetes)
- [Article tr√®s complet sur le service mesh Istio](https://une-tasse-de.cafe/blog/istio/)
- <https://spacelift.io/blog/kubernetes-secrets>
- [Learning Kubernetes, Pods & Deployments with Doom](https://www.youtube.com/watch?v=j9DOWkw9-pc)
- [Administration de cluster via etcd](https://blog.stephane-robert.info/post/kubernetes-etcd/)
- [Un cluster de production en un √©clair avec Talos](https://kdrive.infomaniak.com/app/share/834488/21e24b60-ece5-4445-ba1d-c5adc3c170cc)
- [Installer Kubernetes via kubeadm](https://dev.to/abhay_yt_52a8e72b213be229/how-to-set-up-and-install-a-kubernetes-cluster-a-step-by-step-guide-375j)
- [Kubernetes HA : what if kubernetes internal components go down](https://medium.com/@s.atmaramani/what-if-kubernetes-internal-components-goes-down-6f6372ce0838)
- [Fichiers perso de configuration des lignes de commandes : kubectl, helm, ‚Ä¶](https://git.sr.ht/~toma/dotfiles/tree/main/item/.config/zsh/k8s.sh)
- [10 Ways to Shoot Yourself in the Foot with Kubernetes, #9 Will Surprise You (Youtube)](https://www.youtube.com/watch?v=QKI-JRs2RIE)
- [Scheduler Kubernetes (pour d√©monstration)](https://github.com/kelseyhightower/scheduler)
- Exemples de projets : voir la [page des liens](/cours/liens#kubernetes)
- <https://learnk8s.io/production-best-practices/>
- [Blog: comparaison des types de r√©seau et de CNI dans Kubernetes (publi√© par le CNI Calico)](https://docs.tigera.io/calico/latest/networking/determine-best-networking)
- Awesome Kubernetes: <https://github.com/tomhuang12/awesome-k8s-resources>
- Exemple de d√©ploiement de [Wordpress avec MySQL](https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/)
- [Slides sur cert-manager](https://2021-05-enix.container.training/3.yml.html#205)
- Livre "Kubernetes 101" de Jeff Geerling et [playlist Youtube](https://www.youtube.com/watch?v=IcslsH7OoYo&list=PL2_OBreMn7FoYmfx27iSwocotjiikS5BD) et [d√©p√¥t Github](https://github.com/geerlingguy/kubernetes-101)
- [Interconnecting Clusters](https://2021-05-enix.container.training/5.yml.html#186)

---

# Legal

- Docker¬Æ, Docker Swarm and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries. Docker, Inc. and other parties may also have trademark rights in other terms used herein.
- Kubernetes¬Æ is a registered trademark of The Linux Foundation in the United States and/or other countries
- Other names may be trademarks of their respective owners

