---
license: ¬© 2025 Tom Avenel under Û∞µ´  BY-SA 4.0
title: Clusters de haute disponibilit√© (HA) et Corosync
layout: '@layouts/CoursePartLayout.astro'
---

## üéØ Objectifs p√©dagogiques

- Comprendre les concepts fondamentaux des clusters de haute disponibilit√© (objectifs, m√©triques, architectures).
- Expliquer le r√¥le pr√©cis de Corosync dans une pile HA et d√©tailler son fonctionnement interne (Totem, quorum, membership, CPG).
- Installer, configurer et s√©curiser Corosync (fichiers de config, authkey, transport multicast vs udpu, multi-ring).
- Int√©grer Corosync avec un gestionnaire de ressources (ex. Pacemaker) pour orchestrer des ressources HA.
- Diagnostiquer les incidents courants (split-brain, perte de quorum, r√©seau malade) et appliquer des mesures correctives.

---

## üìö Glossaire rapide

- **HA (High Availability)** : Capacit√© d'un service √† rester accessible malgr√© des pannes mat√©rielles ou logicielles.
- **RTO / RPO** : Recovery Time Objective (temps de r√©tablissement cible) / Recovery Point Objective (point de reprise des donn√©es).
- **Quorum** : R√®gle de d√©cision majoritaire qui permet d'√©viter des √©tats divergents (split-brain).
- **STONITH** : Shoot The Other Node In The Head : m√©canisme de fencing/power-off pour garantir l'exclusion d'un n≈ìud fautif.
- **Totem** : Protocole de Corosync assurant la diffusion fiable et l'ordre des messages dans le cluster.
- **CPG** : Closed Process Group : API de messagerie fournissant un canal de communication ordonn√©.

---

## Th√©orie des clusters HA

### Pourquoi la HA ? m√©triques et exigences

- **Disponibilit√©** : ex. 99.9% ‚Üí ~8.76h downtime/an. Les exigences d√©terminent l'architecture.
- **RTO (Recovery Time Objective)** : combien de temps maximal pour r√©tablir le service.
- **RPO (Recovery Point Objective)** : perte de donn√©es tol√©rable.

Ces contraintes fondent les choix : redondance active/passive ou active/active, r√©plication synchrone vs asynchrone, etc.

---

### Mod√®les d'architectures

- **Active‚ÄìPassive (Primary/Standby)** : un n≈ìud sert, un autre prend la rel√®ve (failover). Simple et s√ªr.
- **Active‚ÄìActive** : plusieurs n≈ìuds servent simultan√©ment (requiert coordination et ressources partag√©es ou sharding).
- **Split-brain risk** : augmente quand plusieurs n≈ìuds pensent √™tre primaires : pr√©vention par quorum et fencing.

---

### Composants d‚Äôun cluster HA

- **Layer de communication et membership** (ex. Corosync) : maintien de la vue des n≈ìuds et diffusion fiable.
- **Resource manager** (ex. Pacemaker) : d√©finition, d√©marrage/arr√™t et placement des ressources.
- **Agents de ressources** (OCF, LSB) : scripts qui d√©marrent, arr√™tent, monitorent les services.
- **Fencing / STONITH** : isolation irr√©futable d'un n≈ìud d√©faillant (poweroff/reboot via IPMI, PDU, virtualisation).
- **Stockage partag√© ou r√©pliqu√©** : iSCSI, DRBD, NFS, ceph : choisir selon RPO/RTO.

---

### Quorum et d√©cisions distribut√©es

- **Quorum majoritaire** : la majorit√© des votes doit √™tre pr√©sente pour prendre des d√©cisions (pr√©vention du split-brain).
- **Quorum device / qdevice** : entit√© externe servant d'arbitre lorsqu'on a un nombre pair/insuffisant de n≈ìuds.
- **Two-node clusters** : cas particulier : g√©n√©ralement on active un tie-breaker (Qdevice) ou des r√®gles sp√©cifiques.

---

### Fencing / STONITH

- Sans fencing, on risque la corruption des donn√©es : deux n≈ìuds √©crivant la m√™me ressource.
- M√©thodes : IPMI, iLO, iDRAC, PDU controllable, fence_virt via hyperviseur, fence_xvm, fence_vmware, etc.

:::warn
Toujours configurer un m√©canisme STONITH op√©rationnel avant tout essai en prod.
:::

---

## Vue d'ensemble de Corosync

### R√¥le et position dans la pile HA

- **Corosync** fournit la couche de communication fiable, le membership et la gestion du quorum. Il ~ne g√®re pas directement~ les ressources applicatives.
- Souvent utilis√© conjointement avec **Pacemaker** (gestionnaire de ressources) : _Corosync_ g√®re le cluster membership et transport, _Pacemaker_ orchestre.

### Principaux modules / services

- **Totem** : protocole de diffusion ordonn√©e (ring/token) qui garantit livraison fiable et ordering.
- **CPG (Closed Process Group)** : API de messagerie pour applications qui ont besoin d‚Äôun groupe et d‚Äôun ordre (Pacemaker utilise ces API).
- **Quorum** : module qui calcule si le cluster a le droit de voter (majority, expected_votes...).
- **Configuration manager (cmap)** : permet de partager de petites donn√©es de configuration/cl√©-valeur entre n≈ìuds.

### Garanties offertes par Corosync

- **Fiabilit√© de diffusion** : messages d√©livr√©s √† tous les n≈ìuds survivants dans le bon ordre.
- **D√©tection de d√©faillance et membership** : chaque n≈ìud a une "vue" coh√©rente du cluster.
- **Quorum** : d√©cision majoritaire pour √©viter d√©cisions divergentes.

---

## Architecture et protocole Totem

:::tip
Le Totem protocol fournit la logique sous-jacente de transport et d'ordre. Il utilise un m√©canisme de passage de token (token passing) et peut fonctionner via multicast (`mcast`) ou unicast (`UDPU`).
:::

### Token passing

- Un token circule entre n≈ìuds suivant l‚Äôordre de la `nodelist`.
- Le n≈ìud qui poss√®de le token peut √©mettre des messages ; le token garantit un ordre global.
- Si un n≈ìud ne renvoie pas le token, les autres d√©tectent sa d√©faillance et mettent √† jour la vue.

### Multicast vs UDPU (unicast)

- **Multicast** : efficace pour LAN, n√©cessite que le r√©seau autorise le multicast.
- **UDPU (User Datagram Protocol Unicast)** : option si multicast indisponible - chaque pair communique en unicast.
- Choix conditionn√© par l‚Äôinfrastructure (clouds/h√©bergeurs qui bloquent multicast ‚Üí UDPU).

### Param√®tres Totem importants

- `token` (ms) : intervalle cible pour la rotation du token - influences latence de d√©tection ; augmenter si r√©seau lent.
- `token_retransmits_before_loss_const` : nombre de retransmissions avant qu‚Äôun n≈ìud soit consid√©r√© perdu.
- `join` : d√©lai maximal pour joindre le cluster.
- `consensus` : param√®tres internes pour parvenir √† un accord.
- `max_messages` : nombre maximal de messages envoy√©s quand on d√©tient le token.

:::tip
Ces param√®tres ont un impact direct sur la sensibilit√© aux pannes et la r√©activit√©. Les valeurs par d√©faut conviennent en g√©n√©ral, mais doivent √™tre adapt√©es pour r√©seaux √† latence √©lev√©e.
:::

---

## Int√©gration avec Pacemaker

### R√¥le de Pacemaker (orchestration des ressources)

- Pacemaker agit comme **Resource Manager** : placement, d√©pendances, contraintes et monitors.
- Pacemaker s'appuie sur Corosync pour la messagerie et la vue du cluster.

### Types de ressources

- **Primitive** : ressource simple (IP flottante, service Apache, script OCF).
- **Group** : collection de primitives ex√©cut√©es sur le m√™me n≈ìud (ex. VIP + daemon).
- **Clone** : ressource r√©pliqu√©e sur plusieurs n≈ìuds (active/active selon agent).

### Exemple

Cr√©ation d'un IP flottant et d'un service

:::tip
Exemple via **pcs** (RHEL/CentOS/Fedora). Les commandes √©quivalentes existent pour `crm`/`crmsh`.
:::

```bash
# authentifier les n≈ìuds
sudo pcs cluster auth node1 node2 node3 -u hacluster -p Secr3t
# cr√©ation et d√©marrage du cluster
sudo pcs cluster setup --name mycluster node1 node2 node3
sudo pcs cluster start --all

# cr√©ation d'une ressource IP
sudo pcs resource create VirtualIP ocf:heartbeat:IPaddr2 ip=192.168.100.200 cidr_netmask=24 op monitor interval=30s

# cr√©ation d'une ressource apache (exemple)
sudo pcs resource create WebServer ocf:heartbeat:apache configfile=/etc/apache2/sites-enabled/000-default.conf op monitor interval=30s

# contrainte pour que WebServer suive VirtualIP
sudo pcs constraint colocation add WebServer VirtualIP INFINITY
sudo pcs constraint order VirtualIP then WebServer
```

:::warn
Avant toute cr√©ation de ressources en production, configurer le fencing STONITH.
:::

---

## S√©curit√© et bonnes pratiques

**R√©seau** :

- **S√©parer** le plan de donn√©es et le plan de gestion/heartbeat (interface d√©di√©e pour Corosync).
- **R√©server** des VLANs ou un r√©seau priv√© pour la communication Corosync.
- **MTU** identique sur toutes les interfaces utilis√©es (√©viter fragmentation).

**Synchronisation temporelle** :

- NTP/chrony sur tous les n≈ìuds - des horloges tr√®s d√©cal√©es perturbent les protocoles distribu√©s.

**Authentification & chiffrement** :

- `secauth: on` dans `corosync.conf` pour activer l'authentification.
- Prot√©ger `/etc/corosync/authkey` (0400) et utiliser canaux s√©curis√©s pour sa distribution.

**Fencing (obligatoire)** :

- Configurer un dispositif STONITH d√®s la mise en production (IPMI, PDU, etc.).
- Tester r√©guli√®rement les sc√©narios de fencing dans un environnement contr√¥l√©.

**Tests et monitoring** :

- Sc√©narios de test : arr√™t brutal d‚Äôun n≈ìud, perte d‚Äôune interface r√©seau, surcharge CPU/m√©moire.
- Metriques √† surveiller : latence r√©seau, perte de paquets, taux de token loss, logs corosync/pacemaker, √©tat STONITH.

**Valeurs recommand√©es** :

- **Nombre de n≈ìuds** : id√©alement impair (3,5,7) pour faciliter quorum majoritaire.
- **Token** : conserver la valeur par d√©faut sauf si latence r√©seau √©lev√©e - augmentez prudemment.
- **Multi-ring** : 2 r√©seaux physiques s√©par√©s pour la redondance heartbeat.

---

## Troubleshooting et diagnostics

- **Perte de quorum** : ex√©cuter `corosync-quorumtool -l` et v√©rifier la configuration `expected_votes`.
- **Split-brain** : deux groupes ind√©pendants - v√©rifier les logs et la pr√©sence d‚Äôun m√©canisme STONITH.
- **R√©seau intermittent** : regarder `dmesg`, `ethtool`, `ip -s link` pour collisions, erreurs.

### Exemple de proc√©dure en cas de split-brain

1. V√©rifier l‚Äô√©tat du r√©seau et des n≈ìuds (ping, ssh, journalctl).
2. Ne prenez pas de d√©cision h√¢tive - identifier la tranche de n≈ìuds majoritaire.
3. Si un n≈ìud est isol√©, utiliser STONITH pour l'√©teindre proprement.
4. Rattacher le n≈ìud √©teint et laisser Pacemaker re-r√©pliquer/relancer les ressources.
5. Si impossible de fencing automatique, envisager remise √† z√©ro manuelle sous contr√¥le (op√©ration risqu√©e).

---

## QCM / questions de r√©flexion

1. Pourquoi le fencing est-il indispensable dans un cluster HA ?
2. Expliquez le fonctionnement du protocole Totem en 3 phrases.
3. Quels sont les risques d‚Äôutiliser multicast sur un r√©seau non isol√© ?
4. Dans un cluster 2 n≈ìuds, quelles options permettent d‚Äô√©viter le split-brain ?
5. D√©crivez une proc√©dure s√ªre pour remettre en production un n≈ìud isol√©.

---

